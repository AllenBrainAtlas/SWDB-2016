{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# please make sure your drive_path is set, so that the notebook can find the data files on the hard drive\n",
    "\n",
    "# OS X\n",
    "drive_path = '/Volumes/Brain2016'\n",
    "\n",
    "# Windows (a good guess)\n",
    "# drive_path = 'e:/'\n",
    "\n",
    "# Linux (will vary; the following is possibly what Ubuntu will do)\n",
    "# drive_path = '/media/Brain2016/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports go here\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "\n",
    "import tensorflow.contrib.learn.python.learn as learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare cache for Brain Observatory Data (from hard drive)\n",
    "from allensdk.core.brain_observatory_cache import BrainObservatoryCache\n",
    "\n",
    "manifest_path = os.path.join(drive_path,'BrainObservatory','manifest.json')\n",
    "boc = BrainObservatoryCache(manifest_file=manifest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# choose a session (here I've chosen a session with the \"Session A\" stimulus, for drifting gratings)\n",
    "session_id = 501132496\n",
    "\n",
    "data_set = boc.get_ophys_experiment_data(ophys_experiment_id = session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bilalbari/anaconda/lib/python2.7/site-packages/allensdk/brain_observatory/stimulus_analysis.py:327: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  temp = self.celltraces[nc, start:end]\n",
      "/Users/bilalbari/anaconda/lib/python2.7/site-packages/allensdk/brain_observatory/stimulus_analysis.py:330: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  sweep_response['dx'][index] = self.dxcm[start:end]\n"
     ]
    }
   ],
   "source": [
    "# get the stimulus response info for drifting gratings\n",
    "from allensdk.brain_observatory.drifting_gratings import DriftingGratings\n",
    "\n",
    "dg = DriftingGratings(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>dx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.924288</td>\n",
       "      <td>139.039246</td>\n",
       "      <td>3.320146</td>\n",
       "      <td>4.998099</td>\n",
       "      <td>1.681830</td>\n",
       "      <td>5.663136</td>\n",
       "      <td>2.919455</td>\n",
       "      <td>0.918880</td>\n",
       "      <td>-3.759009</td>\n",
       "      <td>3.555691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.645828</td>\n",
       "      <td>-0.875782</td>\n",
       "      <td>-3.214458</td>\n",
       "      <td>0.474641</td>\n",
       "      <td>-2.173970</td>\n",
       "      <td>1.003021</td>\n",
       "      <td>0.414215</td>\n",
       "      <td>1.177390</td>\n",
       "      <td>-1.611366</td>\n",
       "      <td>-0.066844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.187183</td>\n",
       "      <td>-24.438265</td>\n",
       "      <td>2.673344</td>\n",
       "      <td>-0.293247</td>\n",
       "      <td>3.444310</td>\n",
       "      <td>1.934456</td>\n",
       "      <td>0.668622</td>\n",
       "      <td>6.963308</td>\n",
       "      <td>-3.042664</td>\n",
       "      <td>5.318217</td>\n",
       "      <td>...</td>\n",
       "      <td>2.709987</td>\n",
       "      <td>55.290691</td>\n",
       "      <td>9.364721</td>\n",
       "      <td>1.938032</td>\n",
       "      <td>0.385931</td>\n",
       "      <td>2.058041</td>\n",
       "      <td>1.408415</td>\n",
       "      <td>-0.849697</td>\n",
       "      <td>8.414009</td>\n",
       "      <td>-0.003886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.389491</td>\n",
       "      <td>-3.373666</td>\n",
       "      <td>-1.105948</td>\n",
       "      <td>-3.351060</td>\n",
       "      <td>-6.802246</td>\n",
       "      <td>-8.136486</td>\n",
       "      <td>-2.200871</td>\n",
       "      <td>-4.114766</td>\n",
       "      <td>1.099198</td>\n",
       "      <td>0.643019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.382965</td>\n",
       "      <td>-23.700367</td>\n",
       "      <td>2.227257</td>\n",
       "      <td>-2.507078</td>\n",
       "      <td>6.320639</td>\n",
       "      <td>0.508618</td>\n",
       "      <td>-0.111236</td>\n",
       "      <td>3.562952</td>\n",
       "      <td>-7.060203</td>\n",
       "      <td>-0.017735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.648354</td>\n",
       "      <td>-0.201499</td>\n",
       "      <td>-1.394195</td>\n",
       "      <td>2.023925</td>\n",
       "      <td>0.865427</td>\n",
       "      <td>-1.500949</td>\n",
       "      <td>-0.049483</td>\n",
       "      <td>-0.148969</td>\n",
       "      <td>2.382510</td>\n",
       "      <td>-0.147107</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.366849</td>\n",
       "      <td>67.413918</td>\n",
       "      <td>-0.671766</td>\n",
       "      <td>-0.474635</td>\n",
       "      <td>0.823868</td>\n",
       "      <td>0.926811</td>\n",
       "      <td>-1.400171</td>\n",
       "      <td>-1.315529</td>\n",
       "      <td>0.547427</td>\n",
       "      <td>0.044397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.695783</td>\n",
       "      <td>-6.672735</td>\n",
       "      <td>1.123534</td>\n",
       "      <td>-4.644054</td>\n",
       "      <td>1.922332</td>\n",
       "      <td>0.822137</td>\n",
       "      <td>2.217314</td>\n",
       "      <td>-1.290334</td>\n",
       "      <td>-3.215374</td>\n",
       "      <td>-6.528415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.808711</td>\n",
       "      <td>-17.756134</td>\n",
       "      <td>-3.919241</td>\n",
       "      <td>0.325657</td>\n",
       "      <td>-2.346906</td>\n",
       "      <td>-0.070429</td>\n",
       "      <td>0.938984</td>\n",
       "      <td>-2.778193</td>\n",
       "      <td>-4.067456</td>\n",
       "      <td>0.796255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.105288</td>\n",
       "      <td>0.270228</td>\n",
       "      <td>-1.131875</td>\n",
       "      <td>-3.345745</td>\n",
       "      <td>-2.742775</td>\n",
       "      <td>-2.914334</td>\n",
       "      <td>-2.938337</td>\n",
       "      <td>-1.104345</td>\n",
       "      <td>-2.640578</td>\n",
       "      <td>-0.969058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045305</td>\n",
       "      <td>40.664608</td>\n",
       "      <td>1.221713</td>\n",
       "      <td>-2.125975</td>\n",
       "      <td>-44.827511</td>\n",
       "      <td>-1.314613</td>\n",
       "      <td>-4.855375</td>\n",
       "      <td>-7.632878</td>\n",
       "      <td>-5.547353</td>\n",
       "      <td>14.656958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.913040</td>\n",
       "      <td>-5.720100</td>\n",
       "      <td>-2.191884</td>\n",
       "      <td>-2.941567</td>\n",
       "      <td>5.448968</td>\n",
       "      <td>-0.039351</td>\n",
       "      <td>-1.169098</td>\n",
       "      <td>0.411887</td>\n",
       "      <td>-3.637960</td>\n",
       "      <td>0.140610</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147944</td>\n",
       "      <td>-24.849033</td>\n",
       "      <td>-0.323507</td>\n",
       "      <td>-0.082678</td>\n",
       "      <td>-5.189839</td>\n",
       "      <td>-3.281437</td>\n",
       "      <td>-1.677512</td>\n",
       "      <td>-1.574794</td>\n",
       "      <td>0.294556</td>\n",
       "      <td>5.851471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-3.638568</td>\n",
       "      <td>-5.278098</td>\n",
       "      <td>-3.767323</td>\n",
       "      <td>-3.667766</td>\n",
       "      <td>-3.329220</td>\n",
       "      <td>-4.234958</td>\n",
       "      <td>-3.651684</td>\n",
       "      <td>-6.762294</td>\n",
       "      <td>6.556122</td>\n",
       "      <td>-1.140049</td>\n",
       "      <td>...</td>\n",
       "      <td>2.653934</td>\n",
       "      <td>4.629211</td>\n",
       "      <td>7.787938</td>\n",
       "      <td>-3.595284</td>\n",
       "      <td>3.150859</td>\n",
       "      <td>6.502433</td>\n",
       "      <td>3.341332</td>\n",
       "      <td>2.514690</td>\n",
       "      <td>-6.887974</td>\n",
       "      <td>-0.112774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.965379</td>\n",
       "      <td>-1.220630</td>\n",
       "      <td>-2.411900</td>\n",
       "      <td>-0.384438</td>\n",
       "      <td>1.123843</td>\n",
       "      <td>1.443959</td>\n",
       "      <td>-2.941186</td>\n",
       "      <td>0.521265</td>\n",
       "      <td>2.170068</td>\n",
       "      <td>-0.329759</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.178237</td>\n",
       "      <td>-0.663309</td>\n",
       "      <td>0.334645</td>\n",
       "      <td>-1.287442</td>\n",
       "      <td>0.334880</td>\n",
       "      <td>1.449013</td>\n",
       "      <td>0.798468</td>\n",
       "      <td>-0.794909</td>\n",
       "      <td>-3.788896</td>\n",
       "      <td>-0.016343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.523103</td>\n",
       "      <td>-1.307847</td>\n",
       "      <td>-3.954524</td>\n",
       "      <td>-1.976030</td>\n",
       "      <td>-1.326262</td>\n",
       "      <td>1.303590</td>\n",
       "      <td>-1.997757</td>\n",
       "      <td>0.526156</td>\n",
       "      <td>-0.404156</td>\n",
       "      <td>-0.034587</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.469388</td>\n",
       "      <td>0.630100</td>\n",
       "      <td>0.566566</td>\n",
       "      <td>-0.006475</td>\n",
       "      <td>-0.055857</td>\n",
       "      <td>-1.762099</td>\n",
       "      <td>-1.806312</td>\n",
       "      <td>0.888357</td>\n",
       "      <td>-1.280843</td>\n",
       "      <td>-0.022777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.400976</td>\n",
       "      <td>4.411369</td>\n",
       "      <td>1.965216</td>\n",
       "      <td>10.501359</td>\n",
       "      <td>1.827985</td>\n",
       "      <td>3.555163</td>\n",
       "      <td>0.076538</td>\n",
       "      <td>3.242754</td>\n",
       "      <td>-3.623305</td>\n",
       "      <td>1.484903</td>\n",
       "      <td>...</td>\n",
       "      <td>1.684771</td>\n",
       "      <td>-3.939123</td>\n",
       "      <td>-4.117356</td>\n",
       "      <td>2.621072</td>\n",
       "      <td>-4.040996</td>\n",
       "      <td>-0.534386</td>\n",
       "      <td>-0.841724</td>\n",
       "      <td>0.688198</td>\n",
       "      <td>1.575741</td>\n",
       "      <td>0.008953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.393918</td>\n",
       "      <td>-1.399357</td>\n",
       "      <td>-1.921929</td>\n",
       "      <td>1.789759</td>\n",
       "      <td>1.321348</td>\n",
       "      <td>-2.434721</td>\n",
       "      <td>-0.046752</td>\n",
       "      <td>-1.835363</td>\n",
       "      <td>4.207152</td>\n",
       "      <td>1.588088</td>\n",
       "      <td>...</td>\n",
       "      <td>4.514926</td>\n",
       "      <td>3.061454</td>\n",
       "      <td>7.603007</td>\n",
       "      <td>-1.146731</td>\n",
       "      <td>3.469285</td>\n",
       "      <td>-0.573925</td>\n",
       "      <td>3.130630</td>\n",
       "      <td>0.723385</td>\n",
       "      <td>-0.126531</td>\n",
       "      <td>-0.024256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.285526</td>\n",
       "      <td>0.113567</td>\n",
       "      <td>-1.892128</td>\n",
       "      <td>0.312889</td>\n",
       "      <td>-0.364932</td>\n",
       "      <td>0.233449</td>\n",
       "      <td>-2.923228</td>\n",
       "      <td>-1.210942</td>\n",
       "      <td>-0.230948</td>\n",
       "      <td>-1.347720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.393256</td>\n",
       "      <td>24.941034</td>\n",
       "      <td>-2.037556</td>\n",
       "      <td>-1.491583</td>\n",
       "      <td>1.375948</td>\n",
       "      <td>1.206669</td>\n",
       "      <td>1.090515</td>\n",
       "      <td>0.051682</td>\n",
       "      <td>0.170378</td>\n",
       "      <td>0.029808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.850880</td>\n",
       "      <td>1.894634</td>\n",
       "      <td>-2.189308</td>\n",
       "      <td>-5.097061</td>\n",
       "      <td>-1.219078</td>\n",
       "      <td>-0.370510</td>\n",
       "      <td>-1.251640</td>\n",
       "      <td>0.340566</td>\n",
       "      <td>2.044925</td>\n",
       "      <td>-1.756741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485236</td>\n",
       "      <td>-7.768829</td>\n",
       "      <td>0.909420</td>\n",
       "      <td>-0.151698</td>\n",
       "      <td>-0.990010</td>\n",
       "      <td>-2.915681</td>\n",
       "      <td>-3.186737</td>\n",
       "      <td>-1.257592</td>\n",
       "      <td>-1.205321</td>\n",
       "      <td>-0.022356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-3.324741</td>\n",
       "      <td>-3.586802</td>\n",
       "      <td>-0.957122</td>\n",
       "      <td>0.585849</td>\n",
       "      <td>2.805957</td>\n",
       "      <td>-1.245859</td>\n",
       "      <td>-1.390436</td>\n",
       "      <td>-0.383192</td>\n",
       "      <td>2.204726</td>\n",
       "      <td>-2.437202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.511496</td>\n",
       "      <td>0.637543</td>\n",
       "      <td>-1.480356</td>\n",
       "      <td>-4.103607</td>\n",
       "      <td>1.150615</td>\n",
       "      <td>13.334332</td>\n",
       "      <td>1.684322</td>\n",
       "      <td>-0.606622</td>\n",
       "      <td>1.614188</td>\n",
       "      <td>0.002045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.735496</td>\n",
       "      <td>-0.789406</td>\n",
       "      <td>-1.744491</td>\n",
       "      <td>3.837182</td>\n",
       "      <td>-1.904578</td>\n",
       "      <td>0.268651</td>\n",
       "      <td>0.144801</td>\n",
       "      <td>-3.003933</td>\n",
       "      <td>3.220147</td>\n",
       "      <td>-1.739945</td>\n",
       "      <td>...</td>\n",
       "      <td>2.011066</td>\n",
       "      <td>2.584674</td>\n",
       "      <td>-0.458997</td>\n",
       "      <td>1.830270</td>\n",
       "      <td>-0.465859</td>\n",
       "      <td>-7.366899</td>\n",
       "      <td>4.407147</td>\n",
       "      <td>0.332467</td>\n",
       "      <td>-2.723969</td>\n",
       "      <td>0.015643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-3.219146</td>\n",
       "      <td>18.205942</td>\n",
       "      <td>-0.127314</td>\n",
       "      <td>1.333165</td>\n",
       "      <td>-3.439427</td>\n",
       "      <td>-0.786580</td>\n",
       "      <td>-0.410455</td>\n",
       "      <td>-3.107592</td>\n",
       "      <td>2.505305</td>\n",
       "      <td>-0.786437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.746147</td>\n",
       "      <td>1.031157</td>\n",
       "      <td>-0.960495</td>\n",
       "      <td>-0.490246</td>\n",
       "      <td>1.545993</td>\n",
       "      <td>0.438766</td>\n",
       "      <td>1.592210</td>\n",
       "      <td>-0.312319</td>\n",
       "      <td>-1.011388</td>\n",
       "      <td>0.015250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.681372</td>\n",
       "      <td>-4.473771</td>\n",
       "      <td>-1.233306</td>\n",
       "      <td>-0.862452</td>\n",
       "      <td>-0.430186</td>\n",
       "      <td>-0.408731</td>\n",
       "      <td>0.960102</td>\n",
       "      <td>-2.092379</td>\n",
       "      <td>3.165249</td>\n",
       "      <td>0.991119</td>\n",
       "      <td>...</td>\n",
       "      <td>1.493793</td>\n",
       "      <td>2.686297</td>\n",
       "      <td>3.968291</td>\n",
       "      <td>1.003199</td>\n",
       "      <td>3.024117</td>\n",
       "      <td>-0.083849</td>\n",
       "      <td>1.402323</td>\n",
       "      <td>1.563622</td>\n",
       "      <td>2.221023</td>\n",
       "      <td>-0.028533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.525200</td>\n",
       "      <td>3.261483</td>\n",
       "      <td>-1.259961</td>\n",
       "      <td>-0.421897</td>\n",
       "      <td>-0.319500</td>\n",
       "      <td>-0.913823</td>\n",
       "      <td>-3.678387</td>\n",
       "      <td>-1.413226</td>\n",
       "      <td>-0.884650</td>\n",
       "      <td>-1.291778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.899681</td>\n",
       "      <td>5.070135</td>\n",
       "      <td>-1.152652</td>\n",
       "      <td>-0.965855</td>\n",
       "      <td>-1.193007</td>\n",
       "      <td>-1.613603</td>\n",
       "      <td>-0.092547</td>\n",
       "      <td>-0.316049</td>\n",
       "      <td>1.177489</td>\n",
       "      <td>0.001302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4.989819</td>\n",
       "      <td>-0.694928</td>\n",
       "      <td>0.238136</td>\n",
       "      <td>-14.499375</td>\n",
       "      <td>2.697859</td>\n",
       "      <td>0.720924</td>\n",
       "      <td>-3.167151</td>\n",
       "      <td>3.864067</td>\n",
       "      <td>-7.119856</td>\n",
       "      <td>1.568457</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.835459</td>\n",
       "      <td>-4.882126</td>\n",
       "      <td>-3.638778</td>\n",
       "      <td>1.222103</td>\n",
       "      <td>-2.521146</td>\n",
       "      <td>0.258955</td>\n",
       "      <td>-1.638875</td>\n",
       "      <td>1.095805</td>\n",
       "      <td>3.590905</td>\n",
       "      <td>-0.000466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-3.607743</td>\n",
       "      <td>10.097647</td>\n",
       "      <td>0.275561</td>\n",
       "      <td>1.155236</td>\n",
       "      <td>-1.186157</td>\n",
       "      <td>-0.386108</td>\n",
       "      <td>-0.727734</td>\n",
       "      <td>-0.721562</td>\n",
       "      <td>-2.088788</td>\n",
       "      <td>1.572580</td>\n",
       "      <td>...</td>\n",
       "      <td>1.373488</td>\n",
       "      <td>3.028760</td>\n",
       "      <td>3.165435</td>\n",
       "      <td>-1.326111</td>\n",
       "      <td>2.987508</td>\n",
       "      <td>2.007899</td>\n",
       "      <td>1.204484</td>\n",
       "      <td>1.595665</td>\n",
       "      <td>0.203116</td>\n",
       "      <td>0.022403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-13.067767</td>\n",
       "      <td>-0.439002</td>\n",
       "      <td>1.266569</td>\n",
       "      <td>-1.081039</td>\n",
       "      <td>3.111125</td>\n",
       "      <td>-3.952534</td>\n",
       "      <td>-1.754139</td>\n",
       "      <td>0.327421</td>\n",
       "      <td>-3.033976</td>\n",
       "      <td>0.340488</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.563610</td>\n",
       "      <td>0.585880</td>\n",
       "      <td>-3.849229</td>\n",
       "      <td>-5.955876</td>\n",
       "      <td>-1.693727</td>\n",
       "      <td>6.205564</td>\n",
       "      <td>-0.899744</td>\n",
       "      <td>-4.475671</td>\n",
       "      <td>2.789082</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-4.818124</td>\n",
       "      <td>1.386886</td>\n",
       "      <td>-2.035119</td>\n",
       "      <td>-2.180003</td>\n",
       "      <td>-2.218489</td>\n",
       "      <td>-1.244888</td>\n",
       "      <td>-2.725014</td>\n",
       "      <td>0.467789</td>\n",
       "      <td>2.022690</td>\n",
       "      <td>6.794149</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.197180</td>\n",
       "      <td>-0.028454</td>\n",
       "      <td>5.085932</td>\n",
       "      <td>-0.039118</td>\n",
       "      <td>0.501265</td>\n",
       "      <td>-0.924362</td>\n",
       "      <td>-0.694872</td>\n",
       "      <td>-2.246082</td>\n",
       "      <td>-2.702517</td>\n",
       "      <td>-0.034864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.210771</td>\n",
       "      <td>6.206063</td>\n",
       "      <td>-2.026463</td>\n",
       "      <td>-7.255296</td>\n",
       "      <td>2.979352</td>\n",
       "      <td>1.081062</td>\n",
       "      <td>-0.185011</td>\n",
       "      <td>7.383189</td>\n",
       "      <td>-8.566527</td>\n",
       "      <td>3.485308</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.090871</td>\n",
       "      <td>-4.176533</td>\n",
       "      <td>-4.380309</td>\n",
       "      <td>-0.705243</td>\n",
       "      <td>-4.812479</td>\n",
       "      <td>-0.922223</td>\n",
       "      <td>-5.494411</td>\n",
       "      <td>-4.303218</td>\n",
       "      <td>5.708227</td>\n",
       "      <td>-0.019084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.230911</td>\n",
       "      <td>-0.968029</td>\n",
       "      <td>1.320367</td>\n",
       "      <td>-0.121107</td>\n",
       "      <td>-2.505485</td>\n",
       "      <td>-0.726112</td>\n",
       "      <td>-1.945240</td>\n",
       "      <td>-0.112899</td>\n",
       "      <td>13.317016</td>\n",
       "      <td>-1.316139</td>\n",
       "      <td>...</td>\n",
       "      <td>1.194288</td>\n",
       "      <td>20.965084</td>\n",
       "      <td>12.176484</td>\n",
       "      <td>-0.119798</td>\n",
       "      <td>3.661775</td>\n",
       "      <td>-2.230568</td>\n",
       "      <td>0.909810</td>\n",
       "      <td>-2.714396</td>\n",
       "      <td>0.321294</td>\n",
       "      <td>0.012813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.080237</td>\n",
       "      <td>1.352651</td>\n",
       "      <td>-0.272965</td>\n",
       "      <td>-2.851300</td>\n",
       "      <td>-2.706688</td>\n",
       "      <td>1.817893</td>\n",
       "      <td>-1.817830</td>\n",
       "      <td>2.120890</td>\n",
       "      <td>-38.106678</td>\n",
       "      <td>0.561321</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.338845</td>\n",
       "      <td>13.563090</td>\n",
       "      <td>-4.135433</td>\n",
       "      <td>-0.582149</td>\n",
       "      <td>-11.262362</td>\n",
       "      <td>-3.332664</td>\n",
       "      <td>-2.441708</td>\n",
       "      <td>0.067574</td>\n",
       "      <td>0.427803</td>\n",
       "      <td>0.028335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.182860</td>\n",
       "      <td>1.476600</td>\n",
       "      <td>-3.185627</td>\n",
       "      <td>-2.864347</td>\n",
       "      <td>0.287083</td>\n",
       "      <td>2.411208</td>\n",
       "      <td>3.113635</td>\n",
       "      <td>-0.633001</td>\n",
       "      <td>-5.065260</td>\n",
       "      <td>10.398504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.446611</td>\n",
       "      <td>-31.850929</td>\n",
       "      <td>-1.499517</td>\n",
       "      <td>0.867418</td>\n",
       "      <td>-4.049119</td>\n",
       "      <td>0.298537</td>\n",
       "      <td>-2.283825</td>\n",
       "      <td>2.242084</td>\n",
       "      <td>-4.678553</td>\n",
       "      <td>0.020688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6.478179</td>\n",
       "      <td>3.833503</td>\n",
       "      <td>3.088685</td>\n",
       "      <td>-2.911348</td>\n",
       "      <td>1.311059</td>\n",
       "      <td>0.509329</td>\n",
       "      <td>-0.923945</td>\n",
       "      <td>7.231589</td>\n",
       "      <td>-3.022735</td>\n",
       "      <td>1.836617</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.536381</td>\n",
       "      <td>10.260139</td>\n",
       "      <td>1.954950</td>\n",
       "      <td>0.991135</td>\n",
       "      <td>-2.676598</td>\n",
       "      <td>-1.950536</td>\n",
       "      <td>-3.008706</td>\n",
       "      <td>-3.620806</td>\n",
       "      <td>4.380615</td>\n",
       "      <td>0.002521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.341517</td>\n",
       "      <td>0.569213</td>\n",
       "      <td>-3.909366</td>\n",
       "      <td>-2.231922</td>\n",
       "      <td>0.521498</td>\n",
       "      <td>0.608407</td>\n",
       "      <td>0.470003</td>\n",
       "      <td>-0.680742</td>\n",
       "      <td>-2.450145</td>\n",
       "      <td>1.575964</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570397</td>\n",
       "      <td>43.633217</td>\n",
       "      <td>-0.054178</td>\n",
       "      <td>5.200435</td>\n",
       "      <td>-3.243068</td>\n",
       "      <td>-0.408317</td>\n",
       "      <td>-0.972884</td>\n",
       "      <td>0.771487</td>\n",
       "      <td>1.060251</td>\n",
       "      <td>17.396075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-4.969223</td>\n",
       "      <td>-3.979612</td>\n",
       "      <td>-2.713813</td>\n",
       "      <td>-0.319336</td>\n",
       "      <td>-2.828335</td>\n",
       "      <td>0.945298</td>\n",
       "      <td>-1.734352</td>\n",
       "      <td>-4.605103</td>\n",
       "      <td>5.901812</td>\n",
       "      <td>-0.401664</td>\n",
       "      <td>...</td>\n",
       "      <td>0.204377</td>\n",
       "      <td>-20.178316</td>\n",
       "      <td>4.171975</td>\n",
       "      <td>-2.712280</td>\n",
       "      <td>18.980419</td>\n",
       "      <td>1.227097</td>\n",
       "      <td>2.244862</td>\n",
       "      <td>2.200610</td>\n",
       "      <td>-0.688745</td>\n",
       "      <td>15.822747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>-3.095855</td>\n",
       "      <td>0.991994</td>\n",
       "      <td>0.812682</td>\n",
       "      <td>-1.240612</td>\n",
       "      <td>0.385388</td>\n",
       "      <td>-1.167224</td>\n",
       "      <td>-0.312679</td>\n",
       "      <td>-0.372990</td>\n",
       "      <td>-1.944096</td>\n",
       "      <td>-1.472425</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.495978</td>\n",
       "      <td>-6.093196</td>\n",
       "      <td>-0.561485</td>\n",
       "      <td>-0.041038</td>\n",
       "      <td>-0.537590</td>\n",
       "      <td>-1.045390</td>\n",
       "      <td>-3.881995</td>\n",
       "      <td>-1.075246</td>\n",
       "      <td>1.987032</td>\n",
       "      <td>-0.007507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>-5.079594</td>\n",
       "      <td>-2.542379</td>\n",
       "      <td>-2.523017</td>\n",
       "      <td>-1.303433</td>\n",
       "      <td>-2.250457</td>\n",
       "      <td>-2.963351</td>\n",
       "      <td>-2.488337</td>\n",
       "      <td>-4.027700</td>\n",
       "      <td>3.040204</td>\n",
       "      <td>-0.088869</td>\n",
       "      <td>...</td>\n",
       "      <td>1.819617</td>\n",
       "      <td>2.897612</td>\n",
       "      <td>6.364987</td>\n",
       "      <td>-1.695480</td>\n",
       "      <td>1.964303</td>\n",
       "      <td>2.949125</td>\n",
       "      <td>-1.475724</td>\n",
       "      <td>-0.154514</td>\n",
       "      <td>-11.096727</td>\n",
       "      <td>0.018817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>35.580811</td>\n",
       "      <td>17.301605</td>\n",
       "      <td>15.678452</td>\n",
       "      <td>-14.485700</td>\n",
       "      <td>10.525707</td>\n",
       "      <td>6.448646</td>\n",
       "      <td>4.637093</td>\n",
       "      <td>14.009764</td>\n",
       "      <td>-21.141970</td>\n",
       "      <td>3.372103</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.356482</td>\n",
       "      <td>-12.065597</td>\n",
       "      <td>-21.718700</td>\n",
       "      <td>17.236118</td>\n",
       "      <td>-12.762092</td>\n",
       "      <td>-10.977227</td>\n",
       "      <td>-10.579799</td>\n",
       "      <td>-4.348370</td>\n",
       "      <td>3.090791</td>\n",
       "      <td>1.406927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>2.913547</td>\n",
       "      <td>1.408100</td>\n",
       "      <td>-3.433574</td>\n",
       "      <td>-1.862719</td>\n",
       "      <td>-1.000050</td>\n",
       "      <td>1.720196</td>\n",
       "      <td>0.344291</td>\n",
       "      <td>-3.112572</td>\n",
       "      <td>-1.152676</td>\n",
       "      <td>-1.714013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693874</td>\n",
       "      <td>0.215904</td>\n",
       "      <td>0.566783</td>\n",
       "      <td>0.008836</td>\n",
       "      <td>-7.992690</td>\n",
       "      <td>-2.880335</td>\n",
       "      <td>-3.514330</td>\n",
       "      <td>1.489497</td>\n",
       "      <td>1.087507</td>\n",
       "      <td>-0.002101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>-3.685522</td>\n",
       "      <td>-2.522456</td>\n",
       "      <td>-7.160974</td>\n",
       "      <td>2.798734</td>\n",
       "      <td>-7.482585</td>\n",
       "      <td>-2.429771</td>\n",
       "      <td>-6.390221</td>\n",
       "      <td>0.574009</td>\n",
       "      <td>1.262386</td>\n",
       "      <td>-1.509065</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976345</td>\n",
       "      <td>1.101496</td>\n",
       "      <td>5.434181</td>\n",
       "      <td>-6.742166</td>\n",
       "      <td>-4.616714</td>\n",
       "      <td>-1.204841</td>\n",
       "      <td>-3.492124</td>\n",
       "      <td>-1.843675</td>\n",
       "      <td>4.666268</td>\n",
       "      <td>0.012245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>-0.579709</td>\n",
       "      <td>-1.691277</td>\n",
       "      <td>-0.282200</td>\n",
       "      <td>0.730972</td>\n",
       "      <td>-2.642205</td>\n",
       "      <td>1.683703</td>\n",
       "      <td>-1.942820</td>\n",
       "      <td>0.922937</td>\n",
       "      <td>1.755798</td>\n",
       "      <td>-1.980164</td>\n",
       "      <td>...</td>\n",
       "      <td>2.128494</td>\n",
       "      <td>-0.053250</td>\n",
       "      <td>1.369120</td>\n",
       "      <td>-1.675158</td>\n",
       "      <td>-0.790355</td>\n",
       "      <td>0.726847</td>\n",
       "      <td>0.750982</td>\n",
       "      <td>0.281385</td>\n",
       "      <td>-5.799805</td>\n",
       "      <td>0.032375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>-2.688773</td>\n",
       "      <td>-0.825927</td>\n",
       "      <td>-3.941991</td>\n",
       "      <td>2.194184</td>\n",
       "      <td>-3.262952</td>\n",
       "      <td>-1.648626</td>\n",
       "      <td>-2.189327</td>\n",
       "      <td>0.578751</td>\n",
       "      <td>-0.756484</td>\n",
       "      <td>-1.008421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.624323</td>\n",
       "      <td>2.018784</td>\n",
       "      <td>2.378461</td>\n",
       "      <td>-4.932682</td>\n",
       "      <td>-0.140446</td>\n",
       "      <td>-1.505649</td>\n",
       "      <td>-1.451096</td>\n",
       "      <td>-5.139336</td>\n",
       "      <td>2.316603</td>\n",
       "      <td>0.026021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>-0.238653</td>\n",
       "      <td>11.495327</td>\n",
       "      <td>-2.984108</td>\n",
       "      <td>1.184136</td>\n",
       "      <td>-1.919456</td>\n",
       "      <td>-1.007196</td>\n",
       "      <td>-0.665336</td>\n",
       "      <td>-1.809111</td>\n",
       "      <td>1.851484</td>\n",
       "      <td>-0.209474</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314021</td>\n",
       "      <td>0.907928</td>\n",
       "      <td>5.613289</td>\n",
       "      <td>-0.214375</td>\n",
       "      <td>-1.685567</td>\n",
       "      <td>-1.564008</td>\n",
       "      <td>-0.486497</td>\n",
       "      <td>-0.231671</td>\n",
       "      <td>-3.163579</td>\n",
       "      <td>-0.002383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>-1.646259</td>\n",
       "      <td>-8.456416</td>\n",
       "      <td>0.555801</td>\n",
       "      <td>-1.153914</td>\n",
       "      <td>0.633539</td>\n",
       "      <td>-1.990719</td>\n",
       "      <td>-2.551492</td>\n",
       "      <td>-1.304962</td>\n",
       "      <td>-1.644436</td>\n",
       "      <td>-0.869988</td>\n",
       "      <td>...</td>\n",
       "      <td>2.457619</td>\n",
       "      <td>-4.600485</td>\n",
       "      <td>1.047696</td>\n",
       "      <td>-2.067935</td>\n",
       "      <td>0.430039</td>\n",
       "      <td>-1.066545</td>\n",
       "      <td>0.513414</td>\n",
       "      <td>1.122020</td>\n",
       "      <td>-1.186654</td>\n",
       "      <td>0.007292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>-5.856678</td>\n",
       "      <td>-1.771817</td>\n",
       "      <td>-2.271776</td>\n",
       "      <td>4.573662</td>\n",
       "      <td>-3.600124</td>\n",
       "      <td>-2.956415</td>\n",
       "      <td>-3.056065</td>\n",
       "      <td>0.496437</td>\n",
       "      <td>4.721088</td>\n",
       "      <td>-0.894636</td>\n",
       "      <td>...</td>\n",
       "      <td>3.268982</td>\n",
       "      <td>3.305658</td>\n",
       "      <td>7.942459</td>\n",
       "      <td>-3.346364</td>\n",
       "      <td>8.020095</td>\n",
       "      <td>1.698529</td>\n",
       "      <td>0.259287</td>\n",
       "      <td>0.830056</td>\n",
       "      <td>-1.161154</td>\n",
       "      <td>0.002062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>-1.675692</td>\n",
       "      <td>15.600994</td>\n",
       "      <td>1.014739</td>\n",
       "      <td>0.787642</td>\n",
       "      <td>-4.461729</td>\n",
       "      <td>-1.379094</td>\n",
       "      <td>-1.530338</td>\n",
       "      <td>1.233768</td>\n",
       "      <td>0.880724</td>\n",
       "      <td>2.986063</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067477</td>\n",
       "      <td>2.572978</td>\n",
       "      <td>3.772719</td>\n",
       "      <td>-0.831652</td>\n",
       "      <td>-4.126413</td>\n",
       "      <td>-2.438422</td>\n",
       "      <td>0.069485</td>\n",
       "      <td>-0.943622</td>\n",
       "      <td>0.844767</td>\n",
       "      <td>0.009998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>-15.092143</td>\n",
       "      <td>-13.224869</td>\n",
       "      <td>-10.390069</td>\n",
       "      <td>7.707072</td>\n",
       "      <td>-5.274088</td>\n",
       "      <td>-7.292727</td>\n",
       "      <td>-4.619399</td>\n",
       "      <td>-6.617990</td>\n",
       "      <td>10.483824</td>\n",
       "      <td>-4.509278</td>\n",
       "      <td>...</td>\n",
       "      <td>9.257380</td>\n",
       "      <td>7.887979</td>\n",
       "      <td>10.601987</td>\n",
       "      <td>-11.395568</td>\n",
       "      <td>2.658276</td>\n",
       "      <td>3.975852</td>\n",
       "      <td>2.753469</td>\n",
       "      <td>1.104719</td>\n",
       "      <td>-2.289798</td>\n",
       "      <td>0.060876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>-4.668159</td>\n",
       "      <td>-1.995103</td>\n",
       "      <td>-2.556474</td>\n",
       "      <td>5.171669</td>\n",
       "      <td>-0.030953</td>\n",
       "      <td>-0.826772</td>\n",
       "      <td>2.212581</td>\n",
       "      <td>-0.599354</td>\n",
       "      <td>4.375148</td>\n",
       "      <td>1.752225</td>\n",
       "      <td>...</td>\n",
       "      <td>4.437995</td>\n",
       "      <td>2.019757</td>\n",
       "      <td>7.051458</td>\n",
       "      <td>-3.107524</td>\n",
       "      <td>1.686700</td>\n",
       "      <td>1.801682</td>\n",
       "      <td>3.195222</td>\n",
       "      <td>0.178156</td>\n",
       "      <td>0.163410</td>\n",
       "      <td>-0.003642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>0.825049</td>\n",
       "      <td>-0.423898</td>\n",
       "      <td>-2.328767</td>\n",
       "      <td>-0.687410</td>\n",
       "      <td>-0.571657</td>\n",
       "      <td>-0.759289</td>\n",
       "      <td>-0.841022</td>\n",
       "      <td>-0.475937</td>\n",
       "      <td>-1.416927</td>\n",
       "      <td>-1.077004</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.982034</td>\n",
       "      <td>-0.861519</td>\n",
       "      <td>-5.159665</td>\n",
       "      <td>0.896521</td>\n",
       "      <td>-3.923237</td>\n",
       "      <td>-2.272877</td>\n",
       "      <td>-5.484826</td>\n",
       "      <td>-0.755417</td>\n",
       "      <td>-1.173620</td>\n",
       "      <td>-0.003347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>0.257904</td>\n",
       "      <td>-1.788263</td>\n",
       "      <td>-0.684800</td>\n",
       "      <td>2.525361</td>\n",
       "      <td>-0.140105</td>\n",
       "      <td>-0.177543</td>\n",
       "      <td>-2.007057</td>\n",
       "      <td>-1.793865</td>\n",
       "      <td>3.542266</td>\n",
       "      <td>-0.960114</td>\n",
       "      <td>...</td>\n",
       "      <td>1.256450</td>\n",
       "      <td>3.510507</td>\n",
       "      <td>-19.192471</td>\n",
       "      <td>-2.420871</td>\n",
       "      <td>-2.767613</td>\n",
       "      <td>4.519663</td>\n",
       "      <td>0.601086</td>\n",
       "      <td>-0.406587</td>\n",
       "      <td>0.405376</td>\n",
       "      <td>0.011442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>6.503065</td>\n",
       "      <td>1.846151</td>\n",
       "      <td>0.849472</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>3.150829</td>\n",
       "      <td>1.664264</td>\n",
       "      <td>-1.095912</td>\n",
       "      <td>-0.163156</td>\n",
       "      <td>-2.536456</td>\n",
       "      <td>-0.181062</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.918024</td>\n",
       "      <td>-0.619166</td>\n",
       "      <td>-6.065208</td>\n",
       "      <td>0.582867</td>\n",
       "      <td>-0.754068</td>\n",
       "      <td>-2.566118</td>\n",
       "      <td>0.110497</td>\n",
       "      <td>-0.323783</td>\n",
       "      <td>1.109098</td>\n",
       "      <td>0.144319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>-10.711489</td>\n",
       "      <td>-2.863409</td>\n",
       "      <td>-7.512796</td>\n",
       "      <td>5.731974</td>\n",
       "      <td>-6.589447</td>\n",
       "      <td>-3.046241</td>\n",
       "      <td>-2.138485</td>\n",
       "      <td>-0.685418</td>\n",
       "      <td>5.017659</td>\n",
       "      <td>0.164722</td>\n",
       "      <td>...</td>\n",
       "      <td>5.043692</td>\n",
       "      <td>3.736551</td>\n",
       "      <td>5.826368</td>\n",
       "      <td>-8.646785</td>\n",
       "      <td>4.322601</td>\n",
       "      <td>1.610111</td>\n",
       "      <td>1.640785</td>\n",
       "      <td>1.816753</td>\n",
       "      <td>-4.059197</td>\n",
       "      <td>7.355069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>-4.557356</td>\n",
       "      <td>-3.429137</td>\n",
       "      <td>-0.621815</td>\n",
       "      <td>5.803518</td>\n",
       "      <td>-3.719575</td>\n",
       "      <td>-1.644781</td>\n",
       "      <td>-4.222875</td>\n",
       "      <td>-2.723591</td>\n",
       "      <td>6.396855</td>\n",
       "      <td>0.701718</td>\n",
       "      <td>...</td>\n",
       "      <td>7.427451</td>\n",
       "      <td>5.570056</td>\n",
       "      <td>13.505412</td>\n",
       "      <td>-4.047778</td>\n",
       "      <td>6.317860</td>\n",
       "      <td>4.406146</td>\n",
       "      <td>4.061604</td>\n",
       "      <td>4.565135</td>\n",
       "      <td>0.989831</td>\n",
       "      <td>30.859750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>-2.804446</td>\n",
       "      <td>-4.160568</td>\n",
       "      <td>-1.553250</td>\n",
       "      <td>-1.124255</td>\n",
       "      <td>-4.906032</td>\n",
       "      <td>-2.968473</td>\n",
       "      <td>-2.435140</td>\n",
       "      <td>-0.321671</td>\n",
       "      <td>3.400070</td>\n",
       "      <td>-0.784654</td>\n",
       "      <td>...</td>\n",
       "      <td>1.612394</td>\n",
       "      <td>1.694732</td>\n",
       "      <td>4.513949</td>\n",
       "      <td>-1.265485</td>\n",
       "      <td>0.496015</td>\n",
       "      <td>-0.445881</td>\n",
       "      <td>-2.226226</td>\n",
       "      <td>-1.791826</td>\n",
       "      <td>1.875984</td>\n",
       "      <td>10.514241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>-1.234604</td>\n",
       "      <td>-1.655766</td>\n",
       "      <td>0.029978</td>\n",
       "      <td>-2.655091</td>\n",
       "      <td>-3.477989</td>\n",
       "      <td>-2.926247</td>\n",
       "      <td>-2.124537</td>\n",
       "      <td>-0.571466</td>\n",
       "      <td>-4.170223</td>\n",
       "      <td>-1.419657</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.521714</td>\n",
       "      <td>-1.353740</td>\n",
       "      <td>-0.615689</td>\n",
       "      <td>-2.293883</td>\n",
       "      <td>-4.232930</td>\n",
       "      <td>-3.744883</td>\n",
       "      <td>-2.443779</td>\n",
       "      <td>-2.660396</td>\n",
       "      <td>-2.767665</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.023824</td>\n",
       "      <td>0.567420</td>\n",
       "      <td>1.365275</td>\n",
       "      <td>-1.625191</td>\n",
       "      <td>-1.393048</td>\n",
       "      <td>-3.839812</td>\n",
       "      <td>-1.288355</td>\n",
       "      <td>-0.375941</td>\n",
       "      <td>-1.387748</td>\n",
       "      <td>0.191423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168408</td>\n",
       "      <td>-2.008973</td>\n",
       "      <td>-1.807862</td>\n",
       "      <td>-0.785448</td>\n",
       "      <td>-0.528595</td>\n",
       "      <td>-0.396561</td>\n",
       "      <td>-0.084867</td>\n",
       "      <td>-2.236903</td>\n",
       "      <td>-0.481307</td>\n",
       "      <td>-0.031492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>2.128158</td>\n",
       "      <td>-0.486009</td>\n",
       "      <td>0.508009</td>\n",
       "      <td>-0.079232</td>\n",
       "      <td>-0.815487</td>\n",
       "      <td>0.049513</td>\n",
       "      <td>-0.551244</td>\n",
       "      <td>-0.551181</td>\n",
       "      <td>0.511607</td>\n",
       "      <td>-0.142069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>-1.512793</td>\n",
       "      <td>1.006040</td>\n",
       "      <td>-0.650967</td>\n",
       "      <td>-2.224532</td>\n",
       "      <td>-1.577847</td>\n",
       "      <td>-3.266154</td>\n",
       "      <td>-0.255357</td>\n",
       "      <td>1.959205</td>\n",
       "      <td>-0.053206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>0.292793</td>\n",
       "      <td>-1.613258</td>\n",
       "      <td>-0.145686</td>\n",
       "      <td>-0.632246</td>\n",
       "      <td>0.665717</td>\n",
       "      <td>-1.543291</td>\n",
       "      <td>-1.437172</td>\n",
       "      <td>0.992205</td>\n",
       "      <td>1.445921</td>\n",
       "      <td>-0.306570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.084733</td>\n",
       "      <td>0.010575</td>\n",
       "      <td>1.269570</td>\n",
       "      <td>-1.591422</td>\n",
       "      <td>0.010723</td>\n",
       "      <td>1.355564</td>\n",
       "      <td>1.306133</td>\n",
       "      <td>-0.465286</td>\n",
       "      <td>0.064943</td>\n",
       "      <td>0.030816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>2.573601</td>\n",
       "      <td>1.789498</td>\n",
       "      <td>0.922984</td>\n",
       "      <td>-0.590569</td>\n",
       "      <td>1.251474</td>\n",
       "      <td>1.367609</td>\n",
       "      <td>0.420144</td>\n",
       "      <td>1.051161</td>\n",
       "      <td>-4.435774</td>\n",
       "      <td>-0.633543</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.608680</td>\n",
       "      <td>-1.832741</td>\n",
       "      <td>-15.100786</td>\n",
       "      <td>-1.264741</td>\n",
       "      <td>-1.476287</td>\n",
       "      <td>-0.395858</td>\n",
       "      <td>-0.937087</td>\n",
       "      <td>-3.084548</td>\n",
       "      <td>1.598522</td>\n",
       "      <td>0.411372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>3.882016</td>\n",
       "      <td>-0.315156</td>\n",
       "      <td>-0.336708</td>\n",
       "      <td>1.705815</td>\n",
       "      <td>1.723967</td>\n",
       "      <td>-0.880714</td>\n",
       "      <td>-2.537192</td>\n",
       "      <td>-0.378752</td>\n",
       "      <td>-1.561155</td>\n",
       "      <td>-0.321335</td>\n",
       "      <td>...</td>\n",
       "      <td>1.095855</td>\n",
       "      <td>-0.795792</td>\n",
       "      <td>0.810154</td>\n",
       "      <td>-0.201014</td>\n",
       "      <td>-1.366597</td>\n",
       "      <td>-0.034735</td>\n",
       "      <td>-0.383351</td>\n",
       "      <td>0.628719</td>\n",
       "      <td>0.889104</td>\n",
       "      <td>4.846269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>-11.324406</td>\n",
       "      <td>-6.725654</td>\n",
       "      <td>-6.096860</td>\n",
       "      <td>6.631592</td>\n",
       "      <td>-7.429407</td>\n",
       "      <td>-0.322463</td>\n",
       "      <td>-1.146922</td>\n",
       "      <td>-2.513149</td>\n",
       "      <td>7.161876</td>\n",
       "      <td>-0.283677</td>\n",
       "      <td>...</td>\n",
       "      <td>6.652052</td>\n",
       "      <td>5.703064</td>\n",
       "      <td>17.698229</td>\n",
       "      <td>-7.765018</td>\n",
       "      <td>8.004341</td>\n",
       "      <td>6.559282</td>\n",
       "      <td>3.643175</td>\n",
       "      <td>4.353058</td>\n",
       "      <td>-3.282046</td>\n",
       "      <td>32.062282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>3.532280</td>\n",
       "      <td>1.836630</td>\n",
       "      <td>-1.710806</td>\n",
       "      <td>0.434794</td>\n",
       "      <td>0.648746</td>\n",
       "      <td>-0.385857</td>\n",
       "      <td>-3.113389</td>\n",
       "      <td>1.272408</td>\n",
       "      <td>-1.913318</td>\n",
       "      <td>-0.202274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.267747</td>\n",
       "      <td>0.288795</td>\n",
       "      <td>-1.713692</td>\n",
       "      <td>0.763267</td>\n",
       "      <td>-1.300305</td>\n",
       "      <td>-2.256498</td>\n",
       "      <td>-1.042759</td>\n",
       "      <td>-1.107738</td>\n",
       "      <td>0.200589</td>\n",
       "      <td>24.916980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>4.709373</td>\n",
       "      <td>1.725870</td>\n",
       "      <td>1.914142</td>\n",
       "      <td>-1.141430</td>\n",
       "      <td>-0.030399</td>\n",
       "      <td>-0.003636</td>\n",
       "      <td>-0.429561</td>\n",
       "      <td>2.167270</td>\n",
       "      <td>1.369164</td>\n",
       "      <td>4.724129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195199</td>\n",
       "      <td>-2.004667</td>\n",
       "      <td>2.687394</td>\n",
       "      <td>0.528962</td>\n",
       "      <td>-1.189968</td>\n",
       "      <td>-0.297502</td>\n",
       "      <td>-2.775496</td>\n",
       "      <td>-0.923652</td>\n",
       "      <td>5.996053</td>\n",
       "      <td>13.268416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>-1.018611</td>\n",
       "      <td>-0.969609</td>\n",
       "      <td>-0.761612</td>\n",
       "      <td>-0.525985</td>\n",
       "      <td>-1.054427</td>\n",
       "      <td>-2.064385</td>\n",
       "      <td>-2.721828</td>\n",
       "      <td>-0.650995</td>\n",
       "      <td>4.186494</td>\n",
       "      <td>-0.934572</td>\n",
       "      <td>...</td>\n",
       "      <td>2.389631</td>\n",
       "      <td>2.734637</td>\n",
       "      <td>5.324659</td>\n",
       "      <td>-3.433262</td>\n",
       "      <td>1.761693</td>\n",
       "      <td>1.528042</td>\n",
       "      <td>0.895441</td>\n",
       "      <td>-1.121529</td>\n",
       "      <td>-6.384573</td>\n",
       "      <td>0.009884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>-1.711694</td>\n",
       "      <td>0.290882</td>\n",
       "      <td>-1.851421</td>\n",
       "      <td>-2.129434</td>\n",
       "      <td>0.808698</td>\n",
       "      <td>-0.237613</td>\n",
       "      <td>-2.461541</td>\n",
       "      <td>-0.995184</td>\n",
       "      <td>0.306736</td>\n",
       "      <td>-2.027390</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048599</td>\n",
       "      <td>-2.295673</td>\n",
       "      <td>-2.718639</td>\n",
       "      <td>-3.761733</td>\n",
       "      <td>-3.928424</td>\n",
       "      <td>-2.806128</td>\n",
       "      <td>-0.591695</td>\n",
       "      <td>-2.400742</td>\n",
       "      <td>2.625391</td>\n",
       "      <td>0.002648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0           1          2          3          4         5  \\\n",
       "0     3.924288  139.039246   3.320146   4.998099   1.681830  5.663136   \n",
       "1    11.187183  -24.438265   2.673344  -0.293247   3.444310  1.934456   \n",
       "2    -0.389491   -3.373666  -1.105948  -3.351060  -6.802246 -8.136486   \n",
       "3    -3.648354   -0.201499  -1.394195   2.023925   0.865427 -1.500949   \n",
       "4    -5.695783   -6.672735   1.123534  -4.644054   1.922332  0.822137   \n",
       "5     3.105288    0.270228  -1.131875  -3.345745  -2.742775 -2.914334   \n",
       "6     1.913040   -5.720100  -2.191884  -2.941567   5.448968 -0.039351   \n",
       "7    -3.638568   -5.278098  -3.767323  -3.667766  -3.329220 -4.234958   \n",
       "8     1.965379   -1.220630  -2.411900  -0.384438   1.123843  1.443959   \n",
       "9    -0.523103   -1.307847  -3.954524  -1.976030  -1.326262  1.303590   \n",
       "10    5.400976    4.411369   1.965216  10.501359   1.827985  3.555163   \n",
       "11    3.393918   -1.399357  -1.921929   1.789759   1.321348 -2.434721   \n",
       "12   -0.285526    0.113567  -1.892128   0.312889  -0.364932  0.233449   \n",
       "13    1.850880    1.894634  -2.189308  -5.097061  -1.219078 -0.370510   \n",
       "14   -3.324741   -3.586802  -0.957122   0.585849   2.805957 -1.245859   \n",
       "15   -2.735496   -0.789406  -1.744491   3.837182  -1.904578  0.268651   \n",
       "16   -3.219146   18.205942  -0.127314   1.333165  -3.439427 -0.786580   \n",
       "17    5.681372   -4.473771  -1.233306  -0.862452  -0.430186 -0.408731   \n",
       "18   -0.525200    3.261483  -1.259961  -0.421897  -0.319500 -0.913823   \n",
       "19    4.989819   -0.694928   0.238136 -14.499375   2.697859  0.720924   \n",
       "20   -3.607743   10.097647   0.275561   1.155236  -1.186157 -0.386108   \n",
       "21  -13.067767   -0.439002   1.266569  -1.081039   3.111125 -3.952534   \n",
       "22   -4.818124    1.386886  -2.035119  -2.180003  -2.218489 -1.244888   \n",
       "23    3.210771    6.206063  -2.026463  -7.255296   2.979352  1.081062   \n",
       "24   -1.230911   -0.968029   1.320367  -0.121107  -2.505485 -0.726112   \n",
       "25    2.080237    1.352651  -0.272965  -2.851300  -2.706688  1.817893   \n",
       "26   -0.182860    1.476600  -3.185627  -2.864347   0.287083  2.411208   \n",
       "27    6.478179    3.833503   3.088685  -2.911348   1.311059  0.509329   \n",
       "28    2.341517    0.569213  -3.909366  -2.231922   0.521498  0.608407   \n",
       "29   -4.969223   -3.979612  -2.713813  -0.319336  -2.828335  0.945298   \n",
       "..         ...         ...        ...        ...        ...       ...   \n",
       "598  -3.095855    0.991994   0.812682  -1.240612   0.385388 -1.167224   \n",
       "599  -5.079594   -2.542379  -2.523017  -1.303433  -2.250457 -2.963351   \n",
       "600  35.580811   17.301605  15.678452 -14.485700  10.525707  6.448646   \n",
       "601   2.913547    1.408100  -3.433574  -1.862719  -1.000050  1.720196   \n",
       "602  -3.685522   -2.522456  -7.160974   2.798734  -7.482585 -2.429771   \n",
       "603  -0.579709   -1.691277  -0.282200   0.730972  -2.642205  1.683703   \n",
       "604  -2.688773   -0.825927  -3.941991   2.194184  -3.262952 -1.648626   \n",
       "605  -0.238653   11.495327  -2.984108   1.184136  -1.919456 -1.007196   \n",
       "606  -1.646259   -8.456416   0.555801  -1.153914   0.633539 -1.990719   \n",
       "607  -5.856678   -1.771817  -2.271776   4.573662  -3.600124 -2.956415   \n",
       "608  -1.675692   15.600994   1.014739   0.787642  -4.461729 -1.379094   \n",
       "609 -15.092143  -13.224869 -10.390069   7.707072  -5.274088 -7.292727   \n",
       "610  -4.668159   -1.995103  -2.556474   5.171669  -0.030953 -0.826772   \n",
       "611   0.825049   -0.423898  -2.328767  -0.687410  -0.571657 -0.759289   \n",
       "612   0.257904   -1.788263  -0.684800   2.525361  -0.140105 -0.177543   \n",
       "613   6.503065    1.846151   0.849472  -1.756361   3.150829  1.664264   \n",
       "614 -10.711489   -2.863409  -7.512796   5.731974  -6.589447 -3.046241   \n",
       "615  -4.557356   -3.429137  -0.621815   5.803518  -3.719575 -1.644781   \n",
       "616  -2.804446   -4.160568  -1.553250  -1.124255  -4.906032 -2.968473   \n",
       "617  -1.234604   -1.655766   0.029978  -2.655091  -3.477989 -2.926247   \n",
       "618   0.023824    0.567420   1.365275  -1.625191  -1.393048 -3.839812   \n",
       "619   2.128158   -0.486009   0.508009  -0.079232  -0.815487  0.049513   \n",
       "620   0.292793   -1.613258  -0.145686  -0.632246   0.665717 -1.543291   \n",
       "621   2.573601    1.789498   0.922984  -0.590569   1.251474  1.367609   \n",
       "622   3.882016   -0.315156  -0.336708   1.705815   1.723967 -0.880714   \n",
       "623 -11.324406   -6.725654  -6.096860   6.631592  -7.429407 -0.322463   \n",
       "624   3.532280    1.836630  -1.710806   0.434794   0.648746 -0.385857   \n",
       "625   4.709373    1.725870   1.914142  -1.141430  -0.030399 -0.003636   \n",
       "626  -1.018611   -0.969609  -0.761612  -0.525985  -1.054427 -2.064385   \n",
       "627  -1.711694    0.290882  -1.851421  -2.129434   0.808698 -0.237613   \n",
       "\n",
       "            6          7          8          9    ...            108  \\\n",
       "0    2.919455   0.918880  -3.759009   3.555691    ...       0.645828   \n",
       "1    0.668622   6.963308  -3.042664   5.318217    ...       2.709987   \n",
       "2   -2.200871  -4.114766   1.099198   0.643019    ...      -0.382965   \n",
       "3   -0.049483  -0.148969   2.382510  -0.147107    ...      -1.366849   \n",
       "4    2.217314  -1.290334  -3.215374  -6.528415    ...      -0.808711   \n",
       "5   -2.938337  -1.104345  -2.640578  -0.969058    ...       0.045305   \n",
       "6   -1.169098   0.411887  -3.637960   0.140610    ...       0.147944   \n",
       "7   -3.651684  -6.762294   6.556122  -1.140049    ...       2.653934   \n",
       "8   -2.941186   0.521265   2.170068  -0.329759    ...      -0.178237   \n",
       "9   -1.997757   0.526156  -0.404156  -0.034587    ...      -0.469388   \n",
       "10   0.076538   3.242754  -3.623305   1.484903    ...       1.684771   \n",
       "11  -0.046752  -1.835363   4.207152   1.588088    ...       4.514926   \n",
       "12  -2.923228  -1.210942  -0.230948  -1.347720    ...       0.393256   \n",
       "13  -1.251640   0.340566   2.044925  -1.756741    ...       0.485236   \n",
       "14  -1.390436  -0.383192   2.204726  -2.437202    ...       0.511496   \n",
       "15   0.144801  -3.003933   3.220147  -1.739945    ...       2.011066   \n",
       "16  -0.410455  -3.107592   2.505305  -0.786437    ...       0.746147   \n",
       "17   0.960102  -2.092379   3.165249   0.991119    ...       1.493793   \n",
       "18  -3.678387  -1.413226  -0.884650  -1.291778    ...      -0.899681   \n",
       "19  -3.167151   3.864067  -7.119856   1.568457    ...      -0.835459   \n",
       "20  -0.727734  -0.721562  -2.088788   1.572580    ...       1.373488   \n",
       "21  -1.754139   0.327421  -3.033976   0.340488    ...      -0.563610   \n",
       "22  -2.725014   0.467789   2.022690   6.794149    ...      -0.197180   \n",
       "23  -0.185011   7.383189  -8.566527   3.485308    ...      -2.090871   \n",
       "24  -1.945240  -0.112899  13.317016  -1.316139    ...       1.194288   \n",
       "25  -1.817830   2.120890 -38.106678   0.561321    ...      -0.338845   \n",
       "26   3.113635  -0.633001  -5.065260  10.398504    ...      -0.446611   \n",
       "27  -0.923945   7.231589  -3.022735   1.836617    ...      -1.536381   \n",
       "28   0.470003  -0.680742  -2.450145   1.575964    ...       0.570397   \n",
       "29  -1.734352  -4.605103   5.901812  -0.401664    ...       0.204377   \n",
       "..        ...        ...        ...        ...    ...            ...   \n",
       "598 -0.312679  -0.372990  -1.944096  -1.472425    ...      -2.495978   \n",
       "599 -2.488337  -4.027700   3.040204  -0.088869    ...       1.819617   \n",
       "600  4.637093  14.009764 -21.141970   3.372103    ...     -13.356482   \n",
       "601  0.344291  -3.112572  -1.152676  -1.714013    ...       0.693874   \n",
       "602 -6.390221   0.574009   1.262386  -1.509065    ...       0.976345   \n",
       "603 -1.942820   0.922937   1.755798  -1.980164    ...       2.128494   \n",
       "604 -2.189327   0.578751  -0.756484  -1.008421    ...       0.624323   \n",
       "605 -0.665336  -1.809111   1.851484  -0.209474    ...       0.314021   \n",
       "606 -2.551492  -1.304962  -1.644436  -0.869988    ...       2.457619   \n",
       "607 -3.056065   0.496437   4.721088  -0.894636    ...       3.268982   \n",
       "608 -1.530338   1.233768   0.880724   2.986063    ...      -0.067477   \n",
       "609 -4.619399  -6.617990  10.483824  -4.509278    ...       9.257380   \n",
       "610  2.212581  -0.599354   4.375148   1.752225    ...       4.437995   \n",
       "611 -0.841022  -0.475937  -1.416927  -1.077004    ...      -1.982034   \n",
       "612 -2.007057  -1.793865   3.542266  -0.960114    ...       1.256450   \n",
       "613 -1.095912  -0.163156  -2.536456  -0.181062    ...      -0.918024   \n",
       "614 -2.138485  -0.685418   5.017659   0.164722    ...       5.043692   \n",
       "615 -4.222875  -2.723591   6.396855   0.701718    ...       7.427451   \n",
       "616 -2.435140  -0.321671   3.400070  -0.784654    ...       1.612394   \n",
       "617 -2.124537  -0.571466  -4.170223  -1.419657    ...      -1.521714   \n",
       "618 -1.288355  -0.375941  -1.387748   0.191423    ...       0.168408   \n",
       "619 -0.551244  -0.551181   0.511607  -0.142069    ...       0.001168   \n",
       "620 -1.437172   0.992205   1.445921  -0.306570    ...      -0.084733   \n",
       "621  0.420144   1.051161  -4.435774  -0.633543    ...      -2.608680   \n",
       "622 -2.537192  -0.378752  -1.561155  -0.321335    ...       1.095855   \n",
       "623 -1.146922  -2.513149   7.161876  -0.283677    ...       6.652052   \n",
       "624 -3.113389   1.272408  -1.913318  -0.202274    ...      -0.267747   \n",
       "625 -0.429561   2.167270   1.369164   4.724129    ...       0.195199   \n",
       "626 -2.721828  -0.650995   4.186494  -0.934572    ...       2.389631   \n",
       "627 -2.461541  -0.995184   0.306736  -2.027390    ...       0.048599   \n",
       "\n",
       "           109        110        111        112        113        114  \\\n",
       "0    -0.875782  -3.214458   0.474641  -2.173970   1.003021   0.414215   \n",
       "1    55.290691   9.364721   1.938032   0.385931   2.058041   1.408415   \n",
       "2   -23.700367   2.227257  -2.507078   6.320639   0.508618  -0.111236   \n",
       "3    67.413918  -0.671766  -0.474635   0.823868   0.926811  -1.400171   \n",
       "4   -17.756134  -3.919241   0.325657  -2.346906  -0.070429   0.938984   \n",
       "5    40.664608   1.221713  -2.125975 -44.827511  -1.314613  -4.855375   \n",
       "6   -24.849033  -0.323507  -0.082678  -5.189839  -3.281437  -1.677512   \n",
       "7     4.629211   7.787938  -3.595284   3.150859   6.502433   3.341332   \n",
       "8    -0.663309   0.334645  -1.287442   0.334880   1.449013   0.798468   \n",
       "9     0.630100   0.566566  -0.006475  -0.055857  -1.762099  -1.806312   \n",
       "10   -3.939123  -4.117356   2.621072  -4.040996  -0.534386  -0.841724   \n",
       "11    3.061454   7.603007  -1.146731   3.469285  -0.573925   3.130630   \n",
       "12   24.941034  -2.037556  -1.491583   1.375948   1.206669   1.090515   \n",
       "13   -7.768829   0.909420  -0.151698  -0.990010  -2.915681  -3.186737   \n",
       "14    0.637543  -1.480356  -4.103607   1.150615  13.334332   1.684322   \n",
       "15    2.584674  -0.458997   1.830270  -0.465859  -7.366899   4.407147   \n",
       "16    1.031157  -0.960495  -0.490246   1.545993   0.438766   1.592210   \n",
       "17    2.686297   3.968291   1.003199   3.024117  -0.083849   1.402323   \n",
       "18    5.070135  -1.152652  -0.965855  -1.193007  -1.613603  -0.092547   \n",
       "19   -4.882126  -3.638778   1.222103  -2.521146   0.258955  -1.638875   \n",
       "20    3.028760   3.165435  -1.326111   2.987508   2.007899   1.204484   \n",
       "21    0.585880  -3.849229  -5.955876  -1.693727   6.205564  -0.899744   \n",
       "22   -0.028454   5.085932  -0.039118   0.501265  -0.924362  -0.694872   \n",
       "23   -4.176533  -4.380309  -0.705243  -4.812479  -0.922223  -5.494411   \n",
       "24   20.965084  12.176484  -0.119798   3.661775  -2.230568   0.909810   \n",
       "25   13.563090  -4.135433  -0.582149 -11.262362  -3.332664  -2.441708   \n",
       "26  -31.850929  -1.499517   0.867418  -4.049119   0.298537  -2.283825   \n",
       "27   10.260139   1.954950   0.991135  -2.676598  -1.950536  -3.008706   \n",
       "28   43.633217  -0.054178   5.200435  -3.243068  -0.408317  -0.972884   \n",
       "29  -20.178316   4.171975  -2.712280  18.980419   1.227097   2.244862   \n",
       "..         ...        ...        ...        ...        ...        ...   \n",
       "598  -6.093196  -0.561485  -0.041038  -0.537590  -1.045390  -3.881995   \n",
       "599   2.897612   6.364987  -1.695480   1.964303   2.949125  -1.475724   \n",
       "600 -12.065597 -21.718700  17.236118 -12.762092 -10.977227 -10.579799   \n",
       "601   0.215904   0.566783   0.008836  -7.992690  -2.880335  -3.514330   \n",
       "602   1.101496   5.434181  -6.742166  -4.616714  -1.204841  -3.492124   \n",
       "603  -0.053250   1.369120  -1.675158  -0.790355   0.726847   0.750982   \n",
       "604   2.018784   2.378461  -4.932682  -0.140446  -1.505649  -1.451096   \n",
       "605   0.907928   5.613289  -0.214375  -1.685567  -1.564008  -0.486497   \n",
       "606  -4.600485   1.047696  -2.067935   0.430039  -1.066545   0.513414   \n",
       "607   3.305658   7.942459  -3.346364   8.020095   1.698529   0.259287   \n",
       "608   2.572978   3.772719  -0.831652  -4.126413  -2.438422   0.069485   \n",
       "609   7.887979  10.601987 -11.395568   2.658276   3.975852   2.753469   \n",
       "610   2.019757   7.051458  -3.107524   1.686700   1.801682   3.195222   \n",
       "611  -0.861519  -5.159665   0.896521  -3.923237  -2.272877  -5.484826   \n",
       "612   3.510507 -19.192471  -2.420871  -2.767613   4.519663   0.601086   \n",
       "613  -0.619166  -6.065208   0.582867  -0.754068  -2.566118   0.110497   \n",
       "614   3.736551   5.826368  -8.646785   4.322601   1.610111   1.640785   \n",
       "615   5.570056  13.505412  -4.047778   6.317860   4.406146   4.061604   \n",
       "616   1.694732   4.513949  -1.265485   0.496015  -0.445881  -2.226226   \n",
       "617  -1.353740  -0.615689  -2.293883  -4.232930  -3.744883  -2.443779   \n",
       "618  -2.008973  -1.807862  -0.785448  -0.528595  -0.396561  -0.084867   \n",
       "619  -1.512793   1.006040  -0.650967  -2.224532  -1.577847  -3.266154   \n",
       "620   0.010575   1.269570  -1.591422   0.010723   1.355564   1.306133   \n",
       "621  -1.832741 -15.100786  -1.264741  -1.476287  -0.395858  -0.937087   \n",
       "622  -0.795792   0.810154  -0.201014  -1.366597  -0.034735  -0.383351   \n",
       "623   5.703064  17.698229  -7.765018   8.004341   6.559282   3.643175   \n",
       "624   0.288795  -1.713692   0.763267  -1.300305  -2.256498  -1.042759   \n",
       "625  -2.004667   2.687394   0.528962  -1.189968  -0.297502  -2.775496   \n",
       "626   2.734637   5.324659  -3.433262   1.761693   1.528042   0.895441   \n",
       "627  -2.295673  -2.718639  -3.761733  -3.928424  -2.806128  -0.591695   \n",
       "\n",
       "          115        116         dx  \n",
       "0    1.177390  -1.611366  -0.066844  \n",
       "1   -0.849697   8.414009  -0.003886  \n",
       "2    3.562952  -7.060203  -0.017735  \n",
       "3   -1.315529   0.547427   0.044397  \n",
       "4   -2.778193  -4.067456   0.796255  \n",
       "5   -7.632878  -5.547353  14.656958  \n",
       "6   -1.574794   0.294556   5.851471  \n",
       "7    2.514690  -6.887974  -0.112774  \n",
       "8   -0.794909  -3.788896  -0.016343  \n",
       "9    0.888357  -1.280843  -0.022777  \n",
       "10   0.688198   1.575741   0.008953  \n",
       "11   0.723385  -0.126531  -0.024256  \n",
       "12   0.051682   0.170378   0.029808  \n",
       "13  -1.257592  -1.205321  -0.022356  \n",
       "14  -0.606622   1.614188   0.002045  \n",
       "15   0.332467  -2.723969   0.015643  \n",
       "16  -0.312319  -1.011388   0.015250  \n",
       "17   1.563622   2.221023  -0.028533  \n",
       "18  -0.316049   1.177489   0.001302  \n",
       "19   1.095805   3.590905  -0.000466  \n",
       "20   1.595665   0.203116   0.022403  \n",
       "21  -4.475671   2.789082   0.007812  \n",
       "22  -2.246082  -2.702517  -0.034864  \n",
       "23  -4.303218   5.708227  -0.019084  \n",
       "24  -2.714396   0.321294   0.012813  \n",
       "25   0.067574   0.427803   0.028335  \n",
       "26   2.242084  -4.678553   0.020688  \n",
       "27  -3.620806   4.380615   0.002521  \n",
       "28   0.771487   1.060251  17.396075  \n",
       "29   2.200610  -0.688745  15.822747  \n",
       "..        ...        ...        ...  \n",
       "598 -1.075246   1.987032  -0.007507  \n",
       "599 -0.154514 -11.096727   0.018817  \n",
       "600 -4.348370   3.090791   1.406927  \n",
       "601  1.489497   1.087507  -0.002101  \n",
       "602 -1.843675   4.666268   0.012245  \n",
       "603  0.281385  -5.799805   0.032375  \n",
       "604 -5.139336   2.316603   0.026021  \n",
       "605 -0.231671  -3.163579  -0.002383  \n",
       "606  1.122020  -1.186654   0.007292  \n",
       "607  0.830056  -1.161154   0.002062  \n",
       "608 -0.943622   0.844767   0.009998  \n",
       "609  1.104719  -2.289798   0.060876  \n",
       "610  0.178156   0.163410  -0.003642  \n",
       "611 -0.755417  -1.173620  -0.003347  \n",
       "612 -0.406587   0.405376   0.011442  \n",
       "613 -0.323783   1.109098   0.144319  \n",
       "614  1.816753  -4.059197   7.355069  \n",
       "615  4.565135   0.989831  30.859750  \n",
       "616 -1.791826   1.875984  10.514241  \n",
       "617 -2.660396  -2.767665   0.000689  \n",
       "618 -2.236903  -0.481307  -0.031492  \n",
       "619 -0.255357   1.959205  -0.053206  \n",
       "620 -0.465286   0.064943   0.030816  \n",
       "621 -3.084548   1.598522   0.411372  \n",
       "622  0.628719   0.889104   4.846269  \n",
       "623  4.353058  -3.282046  32.062282  \n",
       "624 -1.107738   0.200589  24.916980  \n",
       "625 -0.923652   5.996053  13.268416  \n",
       "626 -1.121529  -6.384573   0.009884  \n",
       "627 -2.400742   2.625391   0.002648  \n",
       "\n",
       "[628 rows x 118 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the table for mean response by trial\n",
    "\n",
    "num_trials,num_traces = dg.mean_sweep_response.shape\n",
    "num_cells = num_traces - 1  # because one trace is the running speed ('dx')\n",
    "\n",
    "\n",
    "dg.mean_sweep_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temporal_frequency</th>\n",
       "      <th>orientation</th>\n",
       "      <th>blank_sweep</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>746</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>836</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>927</td>\n",
       "      <td>986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1017</td>\n",
       "      <td>1077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1107</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1198</td>\n",
       "      <td>1257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1288</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1378</td>\n",
       "      <td>1438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1468</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1559</td>\n",
       "      <td>1618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1649</td>\n",
       "      <td>1709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1739</td>\n",
       "      <td>1799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1830</td>\n",
       "      <td>1889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1920</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2010</td>\n",
       "      <td>2070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2101</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2191</td>\n",
       "      <td>2251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2281</td>\n",
       "      <td>2341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2371</td>\n",
       "      <td>2431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2462</td>\n",
       "      <td>2521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2552</td>\n",
       "      <td>2612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2642</td>\n",
       "      <td>2702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2733</td>\n",
       "      <td>2792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2823</td>\n",
       "      <td>2883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2913</td>\n",
       "      <td>2973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3003</td>\n",
       "      <td>3063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3094</td>\n",
       "      <td>3153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3184</td>\n",
       "      <td>3244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3274</td>\n",
       "      <td>3334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3365</td>\n",
       "      <td>3424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112537</td>\n",
       "      <td>112596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>4.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112627</td>\n",
       "      <td>112687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>1.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112717</td>\n",
       "      <td>112777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>2.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112808</td>\n",
       "      <td>112867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>8.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112898</td>\n",
       "      <td>112958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>112988</td>\n",
       "      <td>113048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>15.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113078</td>\n",
       "      <td>113138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>4.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113169</td>\n",
       "      <td>113228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113259</td>\n",
       "      <td>113319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113349</td>\n",
       "      <td>113409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>2.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113440</td>\n",
       "      <td>113499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>8.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113530</td>\n",
       "      <td>113590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>113620</td>\n",
       "      <td>113680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>1.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113710</td>\n",
       "      <td>113770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612</th>\n",
       "      <td>4.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113801</td>\n",
       "      <td>113860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>613</th>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113891</td>\n",
       "      <td>113951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>2.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113981</td>\n",
       "      <td>114041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>4.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114072</td>\n",
       "      <td>114131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>15.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114162</td>\n",
       "      <td>114222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>8.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114252</td>\n",
       "      <td>114312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>2.0</td>\n",
       "      <td>315.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114342</td>\n",
       "      <td>114402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114433</td>\n",
       "      <td>114492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>8.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114523</td>\n",
       "      <td>114583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114613</td>\n",
       "      <td>114673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>1.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114704</td>\n",
       "      <td>114763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>114794</td>\n",
       "      <td>114854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>624</th>\n",
       "      <td>2.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114884</td>\n",
       "      <td>114944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>2.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>114974</td>\n",
       "      <td>115034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>15.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115065</td>\n",
       "      <td>115124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>627</th>\n",
       "      <td>4.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115155</td>\n",
       "      <td>115215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     temporal_frequency  orientation  blank_sweep   start     end\n",
       "0                   1.0        180.0          0.0     746     806\n",
       "1                   4.0          0.0          0.0     836     896\n",
       "2                   8.0        135.0          0.0     927     986\n",
       "3                   4.0          0.0          0.0    1017    1077\n",
       "4                   4.0        135.0          0.0    1107    1167\n",
       "5                  15.0        180.0          0.0    1198    1257\n",
       "6                   4.0         90.0          0.0    1288    1348\n",
       "7                   8.0         90.0          0.0    1378    1438\n",
       "8                   8.0        270.0          0.0    1468    1528\n",
       "9                  15.0         45.0          0.0    1559    1618\n",
       "10                  1.0         90.0          0.0    1649    1709\n",
       "11                  8.0        315.0          0.0    1739    1799\n",
       "12                  4.0          0.0          0.0    1830    1889\n",
       "13                  4.0        180.0          0.0    1920    1980\n",
       "14                  4.0        270.0          0.0    2010    2070\n",
       "15                  1.0          0.0          0.0    2101    2160\n",
       "16                  1.0        180.0          0.0    2191    2251\n",
       "17                  8.0        315.0          0.0    2281    2341\n",
       "18                  4.0        180.0          0.0    2371    2431\n",
       "19                 15.0         45.0          0.0    2462    2521\n",
       "20                  2.0        225.0          0.0    2552    2612\n",
       "21                  1.0        270.0          0.0    2642    2702\n",
       "22                 15.0        225.0          0.0    2733    2792\n",
       "23                  8.0        225.0          0.0    2823    2883\n",
       "24                 15.0          0.0          0.0    2913    2973\n",
       "25                  4.0          0.0          0.0    3003    3063\n",
       "26                  1.0        135.0          0.0    3094    3153\n",
       "27                  2.0          0.0          0.0    3184    3244\n",
       "28                  4.0          0.0          0.0    3274    3334\n",
       "29                  2.0        135.0          0.0    3365    3424\n",
       "..                  ...          ...          ...     ...     ...\n",
       "598                 1.0         90.0          0.0  112537  112596\n",
       "599                 4.0         45.0          0.0  112627  112687\n",
       "600                 1.0        270.0          0.0  112717  112777\n",
       "601                 2.0        315.0          0.0  112808  112867\n",
       "602                 8.0        270.0          0.0  112898  112958\n",
       "603                 2.0         45.0          0.0  112988  113048\n",
       "604                15.0         45.0          0.0  113078  113138\n",
       "605                 4.0        180.0          0.0  113169  113228\n",
       "606                 2.0        270.0          0.0  113259  113319\n",
       "607                 2.0        135.0          0.0  113349  113409\n",
       "608                 2.0        180.0          0.0  113440  113499\n",
       "609                 8.0         45.0          0.0  113530  113590\n",
       "610                 0.0          0.0          1.0  113620  113680\n",
       "611                 1.0        315.0          0.0  113710  113770\n",
       "612                 4.0        315.0          0.0  113801  113860\n",
       "613                 2.0        135.0          0.0  113891  113951\n",
       "614                 2.0        135.0          0.0  113981  114041\n",
       "615                 4.0        225.0          0.0  114072  114131\n",
       "616                15.0         45.0          0.0  114162  114222\n",
       "617                 8.0        135.0          0.0  114252  114312\n",
       "618                 2.0        315.0          0.0  114342  114402\n",
       "619                 1.0         45.0          0.0  114433  114492\n",
       "620                 8.0        270.0          0.0  114523  114583\n",
       "621                 2.0         45.0          0.0  114613  114673\n",
       "622                 1.0         90.0          0.0  114704  114763\n",
       "623                 0.0          0.0          1.0  114794  114854\n",
       "624                 2.0         45.0          0.0  114884  114944\n",
       "625                 2.0         90.0          0.0  114974  115034\n",
       "626                15.0        225.0          0.0  115065  115124\n",
       "627                 4.0        270.0          0.0  115155  115215\n",
       "\n",
       "[628 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the table for stimulus conditions\n",
    "dg.stim_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow has a simple interface that mimics the SciKit Learn interface.\n",
    "\n",
    "We'll kill two birds with one stone and describe that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For the purposes of this demo, we'll create a \"train-test\" split with \"leave-one-out\".  \n",
    "# Let's write a function to use the ith trial as the \"test\" set and the remainder as the \"train\" set.\n",
    "# We'll use 'orientation' as the variable to decode and the vector of population activity as the input.\n",
    "\n",
    "def leave_one_out(i,stim,response,to_decode='orientation'):\n",
    "    \n",
    "    output = stim[to_decode]\n",
    "    \n",
    "    num_samples = len(output)\n",
    "    #print num_samples\n",
    "    output_test = np.array([output[i]]).reshape((1,-1))\n",
    "    output_train = np.array(output.drop(i)) #.reshape((num_samples-1,-1))\n",
    "    \n",
    "    resp = response.drop('dx',axis=1)\n",
    "    \n",
    "    response_test = np.array(resp.loc[i]).reshape((1,-1))\n",
    "    response_train = np.array(resp.drop(i)) #.reshape((num_samples-1,-1))\n",
    "    \n",
    "    return output_test.astype(np.float32),output_train.astype(np.float32),\\\n",
    "            response_test.astype(np.float32), response_train.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the first trial for the test set\n",
    "\n",
    "output_test, output_train, response_test, response_train = leave_one_out(0,dg.stim_table,dg.mean_sweep_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(627,)\n",
      "(1, 117)\n",
      "(627, 117)\n",
      "[   0.   45.   90.  135.  180.  225.  270.  315.]\n"
     ]
    }
   ],
   "source": [
    "print output_test.shape\n",
    "print output_train.shape\n",
    "print response_test.shape\n",
    "print response_train.shape\n",
    "\n",
    "classes = np.unique(output_train)\n",
    "print classes\n",
    "\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Change warning: `feature_columns` will be required after 2016-08-01.\n",
      "Instructions for updating:\n",
      "Pass `tf.contrib.learn.infer_real_valued_columns_from_input(x)` or `tf.contrib.learn.infer_real_valued_columns_from_input_fn(input_fn)` as `feature_columns`, where `x` or `input_fn` is your argument to `fit`, `evaluate`, or `predict`.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/tz/yjgcdthn1790wqycwv27s4vh0000gn/T/tmp_oZrVO\n",
      "WARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(117)]), is_sparse=False)\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(None)]), is_sparse=False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8d245a900ea6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# I'm guessing this is Logistic Regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, input_fn, steps, batch_size, monitors, max_steps)\u001b[0m\n\u001b[1;32m    238\u001b[0m                              \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                              \u001b[0mmonitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                              max_steps=max_steps)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, steps, feed_fn, init_op, init_feed_fn, init_fn, device_fn, monitors, log_every_steps, fail_on_nan_loss, max_steps)\u001b[0m\n\u001b[1;32m    576\u001b[0m           \u001b[0mfail_on_nan_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfail_on_nan_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m           \u001b[0mmonitors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmonitors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m           max_steps=max_steps)\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_metric_update_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/graph_actions.pyc\u001b[0m in \u001b[0;36m_supervised_train\u001b[0;34m(graph, output_dir, train_op, loss_op, global_step_tensor, init_op, init_feed_dict, init_fn, log_every_steps, supervisor_is_chief, supervisor_master, supervisor_save_model_secs, keep_checkpoint_max, supervisor_save_summaries_steps, feed_fn, steps, fail_on_nan_loss, monitors, max_steps)\u001b[0m\n\u001b[1;32m    278\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuper_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         _, loss = super_sess.run([train_op, loss_op], feed_fn() if feed_fn else\n\u001b[0;32m--> 280\u001b[0;31m                                  None)\n\u001b[0m\u001b[1;32m    281\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/supervised_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \"\"\"\n\u001b[1;32m    269\u001b[0m     return self._sess.run(fetches, feed_dict=feed_dict, options=options,\n\u001b[0;32m--> 270\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/recoverable_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         return self._sess.run(fetches, feed_dict=feed_dict, options=options,\n\u001b[0;32m---> 54\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAbortedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/coordinated_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitored_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m     95\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                                  \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                                  run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_step_tensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/wrapped_session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 382\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    383\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0mmovers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_with_movers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m     results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 655\u001b[0;31m                            feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0;31m# User may have fetched the same tensor multiple times, but we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 723\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    724\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    728\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bilalbari/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    710\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    711\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This is supposed to be the sklearn style interface for TensorFlow\n",
    "# This doesn't actually seem to be ready yet.  This ran for several hours before I just cancelled it last night.\n",
    "\n",
    "ot = output_train/45\n",
    "ot = ot.astype('int32')\n",
    "\n",
    "classifier = learn.LinearClassifier(n_classes=num_classes)  # I'm guessing this is Logistic Regression\n",
    "classifier.fit(response_train,ot,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LDA()  \n",
    "classifier.fit(response_train,output_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893141945774\n",
      "[ 180.] [[ 180.]]\n"
     ]
    }
   ],
   "source": [
    "print np.mean(classifier.predict(response_train)==output_train)\n",
    "print classifier.predict(response_test), output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean train error =  0.106070764636\n",
      "Mean test error =  0.277070063694\n"
     ]
    }
   ],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "for n in range(num_trials):\n",
    "    output_test, output_train, response_test, response_train = leave_one_out(n,dg.stim_table,dg.mean_sweep_response)\n",
    "    classifier = LDA()  \n",
    "    classifier.fit(response_train,output_train)\n",
    "    \n",
    "    train_errors.append(np.mean(classifier.predict(response_train)!=output_train))\n",
    "    test_errors.append((classifier.predict(response_test)!=output_test)[0][0])\n",
    "    #print classifier.predict(response_test), output_test\n",
    "    \n",
    "print \"Mean train error = \", np.mean(train_errors)\n",
    "print \"Mean test error = \", np.mean(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's put that code in a function so that we can use it with multiple classifiers\n",
    "\n",
    "def Decode(stim,response,decoder=LDA):\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "    for n in range(num_trials):\n",
    "        output_test, output_train, response_test, response_train = leave_one_out(n,stim,response)\n",
    "        classifier = decoder()\n",
    "        classifier.fit(response_train,output_train)\n",
    "\n",
    "        train_errors.append(np.mean(classifier.predict(response_train)!=output_train))\n",
    "        test_errors.append((classifier.predict(response_test)!=output_test)[0][0])\n",
    "        #print classifier.predict(response_test), output_test\n",
    "\n",
    "    print \"Decoder = \", decoder\n",
    "    print \"Mean train error = \", np.mean(train_errors)\n",
    "    print \"Mean test error = \", np.mean(test_errors)\n",
    "    \n",
    "    return np.mean(train_errors), np.mean(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder =  <class 'sklearn.discriminant_analysis.LinearDiscriminantAnalysis'>\n",
      "Mean train error =  0.106070764636\n",
      "Mean test error =  0.277070063694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10607076463596746, 0.27707006369426751)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decode(dg.stim_table,dg.mean_sweep_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bilalbari/anaconda/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:688: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder =  <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>\n",
      "Mean train error =  0.0\n",
      "Mean test error =  0.826433121019\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.82643312101910826)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Decode(dg.stim_table,dg.mean_sweep_response,decoder=QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder =  <class 'sklearn.linear_model.logistic.LogisticRegression'>\n",
      "Mean train error =  0.0015923566879\n",
      "Mean test error =  0.25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0015923566878980893, 0.25)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will take awhile\n",
    "\n",
    "Decode(dg.stim_table,dg.mean_sweep_response,decoder=LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder =  <class 'sklearn.svm.classes.SVC'>\n",
      "Mean train error =  0.0\n",
      "Mean test error =  0.834394904459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.83439490445859876)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This will also take awhile\n",
    "\n",
    "Decode(dg.stim_table,dg.mean_sweep_response,decoder=SVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that I'm not using any arguments to the classifier definitions.  For example, we could improve the SVM performance by adjusting regularization or choosing between linear or radial basis functions for the kernel definition.  You'd need to adjust the function definition above to pass those arguments as well.\n",
    "\n",
    "\n",
    "Now let's try building a classifier with TensorFlow.\n",
    "\n",
    "We'll build a simple LogisticRegression classifier.  After that, we'll try adding layers and see what happens.\n",
    "\n",
    "The first thing we need to do is define the Variables we'll use in our computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the variables with randomize initial values\n",
    "\n",
    "W = tf.Variable(0.01*np.random.normal(size=[num_cells,num_classes]),dtype=tf.float32)\n",
    "b = tf.Variable(0.01*np.random.normal(num_classes),dtype=tf.float32)\n",
    "\n",
    "# The \"responses\" from which we'll decode will be \"placeholders\"\n",
    "\n",
    "R = tf.placeholder(tf.float32,shape=(None,num_cells))  # None means that this dimension can have an arbitrary length\n",
    "Y = tf.placeholder(tf.float32,shape=(None,num_classes)) # \"one-hot\" encoding of stimulus condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Since we're going to use \"one-hot\" encoding, we better have a function that will convert our \"output\" variable above \n",
    "# to \"one-hot\".\n",
    "\n",
    "def one_hot(output,classes):\n",
    "    #classes = np.unique(output)\n",
    "    \n",
    "    output_one_hot = np.zeros([output.shape[0],len(classes)],dtype=np.float32)\n",
    "    #print output_one_hot.shape\n",
    "    for i,stim in enumerate(classes):\n",
    "        output_one_hot.T[i][np.where(output==stim)[0]] = 1\n",
    "        \n",
    "    return output_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_train_onehot = one_hot(output_train,np.unique(output_train))\n",
    "output_test_onehot = one_hot(output_test,np.unique(output_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_input = tf.matmul(R,W)+b  # should have shape (None,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_prob = tf.nn.softmax(linear_input)     # should still have shape (None,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy_loss = -tf.reduce_mean(Y*tf.log(Y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.initialize_all_variables()\n",
    "\n",
    "# or we could do\n",
    "# init_W = W.initializer\n",
    "# init_b = b.initializer\n",
    "# etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to define a \"Session\" that will run our computation graph\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss, iteration 0: = 0.301774\n",
      "cross entropy loss, iteration 1: = 0.265089\n",
      "cross entropy loss, iteration 2: = 0.238614\n",
      "cross entropy loss, iteration 3: = 0.218761\n",
      "cross entropy loss, iteration 4: = 0.204752\n",
      "cross entropy loss, iteration 5: = 0.194821\n",
      "cross entropy loss, iteration 6: = 0.187241\n",
      "cross entropy loss, iteration 7: = 0.181061\n",
      "cross entropy loss, iteration 8: = 0.175816\n",
      "cross entropy loss, iteration 9: = 0.171252\n",
      "cross entropy loss, iteration 10: = 0.167212\n",
      "cross entropy loss, iteration 11: = 0.163588\n",
      "cross entropy loss, iteration 12: = 0.160305\n",
      "cross entropy loss, iteration 13: = 0.157306\n",
      "cross entropy loss, iteration 14: = 0.154548\n",
      "cross entropy loss, iteration 15: = 0.151996\n",
      "cross entropy loss, iteration 16: = 0.149623\n",
      "cross entropy loss, iteration 17: = 0.147407\n",
      "cross entropy loss, iteration 18: = 0.145330\n",
      "cross entropy loss, iteration 19: = 0.143375\n",
      "cross entropy loss, iteration 20: = 0.141530\n",
      "cross entropy loss, iteration 21: = 0.139783\n",
      "cross entropy loss, iteration 22: = 0.138125\n",
      "cross entropy loss, iteration 23: = 0.136549\n",
      "cross entropy loss, iteration 24: = 0.135046\n",
      "cross entropy loss, iteration 25: = 0.133611\n",
      "cross entropy loss, iteration 26: = 0.132237\n",
      "cross entropy loss, iteration 27: = 0.130921\n",
      "cross entropy loss, iteration 28: = 0.129658\n",
      "cross entropy loss, iteration 29: = 0.128444\n",
      "cross entropy loss, iteration 30: = 0.127276\n",
      "cross entropy loss, iteration 31: = 0.126150\n",
      "cross entropy loss, iteration 32: = 0.125063\n",
      "cross entropy loss, iteration 33: = 0.124014\n",
      "cross entropy loss, iteration 34: = 0.123000\n",
      "cross entropy loss, iteration 35: = 0.122019\n",
      "cross entropy loss, iteration 36: = 0.121069\n",
      "cross entropy loss, iteration 37: = 0.120148\n",
      "cross entropy loss, iteration 38: = 0.119254\n",
      "cross entropy loss, iteration 39: = 0.118387\n",
      "cross entropy loss, iteration 40: = 0.117545\n",
      "cross entropy loss, iteration 41: = 0.116726\n",
      "cross entropy loss, iteration 42: = 0.115929\n",
      "cross entropy loss, iteration 43: = 0.115154\n",
      "cross entropy loss, iteration 44: = 0.114399\n",
      "cross entropy loss, iteration 45: = 0.113663\n",
      "cross entropy loss, iteration 46: = 0.112946\n",
      "cross entropy loss, iteration 47: = 0.112246\n",
      "cross entropy loss, iteration 48: = 0.111563\n",
      "cross entropy loss, iteration 49: = 0.110896\n",
      "cross entropy loss, iteration 50: = 0.110245\n",
      "cross entropy loss, iteration 51: = 0.109608\n",
      "cross entropy loss, iteration 52: = 0.108986\n",
      "cross entropy loss, iteration 53: = 0.108377\n",
      "cross entropy loss, iteration 54: = 0.107782\n",
      "cross entropy loss, iteration 55: = 0.107199\n",
      "cross entropy loss, iteration 56: = 0.106628\n",
      "cross entropy loss, iteration 57: = 0.106069\n",
      "cross entropy loss, iteration 58: = 0.105521\n",
      "cross entropy loss, iteration 59: = 0.104983\n",
      "cross entropy loss, iteration 60: = 0.104457\n",
      "cross entropy loss, iteration 61: = 0.103940\n",
      "cross entropy loss, iteration 62: = 0.103433\n",
      "cross entropy loss, iteration 63: = 0.102936\n",
      "cross entropy loss, iteration 64: = 0.102447\n",
      "cross entropy loss, iteration 65: = 0.101967\n",
      "cross entropy loss, iteration 66: = 0.101496\n",
      "cross entropy loss, iteration 67: = 0.101033\n",
      "cross entropy loss, iteration 68: = 0.100578\n",
      "cross entropy loss, iteration 69: = 0.100131\n",
      "cross entropy loss, iteration 70: = 0.099691\n",
      "cross entropy loss, iteration 71: = 0.099258\n",
      "cross entropy loss, iteration 72: = 0.098833\n",
      "cross entropy loss, iteration 73: = 0.098414\n",
      "cross entropy loss, iteration 74: = 0.098002\n",
      "cross entropy loss, iteration 75: = 0.097597\n",
      "cross entropy loss, iteration 76: = 0.097197\n",
      "cross entropy loss, iteration 77: = 0.096804\n",
      "cross entropy loss, iteration 78: = 0.096417\n",
      "cross entropy loss, iteration 79: = 0.096035\n",
      "cross entropy loss, iteration 80: = 0.095659\n",
      "cross entropy loss, iteration 81: = 0.095289\n",
      "cross entropy loss, iteration 82: = 0.094923\n",
      "cross entropy loss, iteration 83: = 0.094563\n",
      "cross entropy loss, iteration 84: = 0.094208\n",
      "cross entropy loss, iteration 85: = 0.093858\n",
      "cross entropy loss, iteration 86: = 0.093513\n",
      "cross entropy loss, iteration 87: = 0.093173\n",
      "cross entropy loss, iteration 88: = 0.092837\n",
      "cross entropy loss, iteration 89: = 0.092505\n",
      "cross entropy loss, iteration 90: = 0.092178\n",
      "cross entropy loss, iteration 91: = 0.091855\n",
      "cross entropy loss, iteration 92: = 0.091536\n",
      "cross entropy loss, iteration 93: = 0.091222\n",
      "cross entropy loss, iteration 94: = 0.090911\n",
      "cross entropy loss, iteration 95: = 0.090604\n",
      "cross entropy loss, iteration 96: = 0.090301\n",
      "cross entropy loss, iteration 97: = 0.090002\n",
      "cross entropy loss, iteration 98: = 0.089706\n",
      "cross entropy loss, iteration 99: = 0.089414\n",
      "cross entropy loss, iteration 100: = 0.089125\n",
      "cross entropy loss, iteration 101: = 0.088840\n",
      "cross entropy loss, iteration 102: = 0.088558\n",
      "cross entropy loss, iteration 103: = 0.088279\n",
      "cross entropy loss, iteration 104: = 0.088004\n",
      "cross entropy loss, iteration 105: = 0.087731\n",
      "cross entropy loss, iteration 106: = 0.087462\n",
      "cross entropy loss, iteration 107: = 0.087196\n",
      "cross entropy loss, iteration 108: = 0.086932\n",
      "cross entropy loss, iteration 109: = 0.086672\n",
      "cross entropy loss, iteration 110: = 0.086414\n",
      "cross entropy loss, iteration 111: = 0.086159\n",
      "cross entropy loss, iteration 112: = 0.085907\n",
      "cross entropy loss, iteration 113: = 0.085658\n",
      "cross entropy loss, iteration 114: = 0.085411\n",
      "cross entropy loss, iteration 115: = 0.085167\n",
      "cross entropy loss, iteration 116: = 0.084925\n",
      "cross entropy loss, iteration 117: = 0.084686\n",
      "cross entropy loss, iteration 118: = 0.084449\n",
      "cross entropy loss, iteration 119: = 0.084214\n",
      "cross entropy loss, iteration 120: = 0.083982\n",
      "cross entropy loss, iteration 121: = 0.083752\n",
      "cross entropy loss, iteration 122: = 0.083525\n",
      "cross entropy loss, iteration 123: = 0.083300\n",
      "cross entropy loss, iteration 124: = 0.083076\n",
      "cross entropy loss, iteration 125: = 0.082856\n",
      "cross entropy loss, iteration 126: = 0.082637\n",
      "cross entropy loss, iteration 127: = 0.082420\n",
      "cross entropy loss, iteration 128: = 0.082205\n",
      "cross entropy loss, iteration 129: = 0.081992\n",
      "cross entropy loss, iteration 130: = 0.081782\n",
      "cross entropy loss, iteration 131: = 0.081573\n",
      "cross entropy loss, iteration 132: = 0.081366\n",
      "cross entropy loss, iteration 133: = 0.081161\n",
      "cross entropy loss, iteration 134: = 0.080958\n",
      "cross entropy loss, iteration 135: = 0.080756\n",
      "cross entropy loss, iteration 136: = 0.080557\n",
      "cross entropy loss, iteration 137: = 0.080359\n",
      "cross entropy loss, iteration 138: = 0.080163\n",
      "cross entropy loss, iteration 139: = 0.079969\n",
      "cross entropy loss, iteration 140: = 0.079776\n",
      "cross entropy loss, iteration 141: = 0.079585\n",
      "cross entropy loss, iteration 142: = 0.079396\n",
      "cross entropy loss, iteration 143: = 0.079208\n",
      "cross entropy loss, iteration 144: = 0.079022\n",
      "cross entropy loss, iteration 145: = 0.078837\n",
      "cross entropy loss, iteration 146: = 0.078654\n",
      "cross entropy loss, iteration 147: = 0.078472\n",
      "cross entropy loss, iteration 148: = 0.078292\n",
      "cross entropy loss, iteration 149: = 0.078114\n",
      "cross entropy loss, iteration 150: = 0.077936\n",
      "cross entropy loss, iteration 151: = 0.077761\n",
      "cross entropy loss, iteration 152: = 0.077586\n",
      "cross entropy loss, iteration 153: = 0.077413\n",
      "cross entropy loss, iteration 154: = 0.077242\n",
      "cross entropy loss, iteration 155: = 0.077072\n",
      "cross entropy loss, iteration 156: = 0.076903\n",
      "cross entropy loss, iteration 157: = 0.076735\n",
      "cross entropy loss, iteration 158: = 0.076569\n",
      "cross entropy loss, iteration 159: = 0.076404\n",
      "cross entropy loss, iteration 160: = 0.076240\n",
      "cross entropy loss, iteration 161: = 0.076078\n",
      "cross entropy loss, iteration 162: = 0.075916\n",
      "cross entropy loss, iteration 163: = 0.075756\n",
      "cross entropy loss, iteration 164: = 0.075597\n",
      "cross entropy loss, iteration 165: = 0.075440\n",
      "cross entropy loss, iteration 166: = 0.075283\n",
      "cross entropy loss, iteration 167: = 0.075128\n",
      "cross entropy loss, iteration 168: = 0.074974\n",
      "cross entropy loss, iteration 169: = 0.074820\n",
      "cross entropy loss, iteration 170: = 0.074668\n",
      "cross entropy loss, iteration 171: = 0.074517\n",
      "cross entropy loss, iteration 172: = 0.074368\n",
      "cross entropy loss, iteration 173: = 0.074219\n",
      "cross entropy loss, iteration 174: = 0.074071\n",
      "cross entropy loss, iteration 175: = 0.073924\n",
      "cross entropy loss, iteration 176: = 0.073779\n",
      "cross entropy loss, iteration 177: = 0.073634\n",
      "cross entropy loss, iteration 178: = 0.073490\n",
      "cross entropy loss, iteration 179: = 0.073348\n",
      "cross entropy loss, iteration 180: = 0.073206\n",
      "cross entropy loss, iteration 181: = 0.073065\n",
      "cross entropy loss, iteration 182: = 0.072925\n",
      "cross entropy loss, iteration 183: = 0.072786\n",
      "cross entropy loss, iteration 184: = 0.072648\n",
      "cross entropy loss, iteration 185: = 0.072511\n",
      "cross entropy loss, iteration 186: = 0.072375\n",
      "cross entropy loss, iteration 187: = 0.072240\n",
      "cross entropy loss, iteration 188: = 0.072106\n",
      "cross entropy loss, iteration 189: = 0.071972\n",
      "cross entropy loss, iteration 190: = 0.071840\n",
      "cross entropy loss, iteration 191: = 0.071708\n",
      "cross entropy loss, iteration 192: = 0.071577\n",
      "cross entropy loss, iteration 193: = 0.071447\n",
      "cross entropy loss, iteration 194: = 0.071318\n",
      "cross entropy loss, iteration 195: = 0.071189\n",
      "cross entropy loss, iteration 196: = 0.071062\n",
      "cross entropy loss, iteration 197: = 0.070935\n",
      "cross entropy loss, iteration 198: = 0.070809\n",
      "cross entropy loss, iteration 199: = 0.070684\n",
      "cross entropy loss, iteration 200: = 0.070559\n",
      "cross entropy loss, iteration 201: = 0.070436\n",
      "cross entropy loss, iteration 202: = 0.070313\n",
      "cross entropy loss, iteration 203: = 0.070191\n",
      "cross entropy loss, iteration 204: = 0.070069\n",
      "cross entropy loss, iteration 205: = 0.069948\n",
      "cross entropy loss, iteration 206: = 0.069828\n",
      "cross entropy loss, iteration 207: = 0.069709\n",
      "cross entropy loss, iteration 208: = 0.069591\n",
      "cross entropy loss, iteration 209: = 0.069473\n",
      "cross entropy loss, iteration 210: = 0.069356\n",
      "cross entropy loss, iteration 211: = 0.069239\n",
      "cross entropy loss, iteration 212: = 0.069123\n",
      "cross entropy loss, iteration 213: = 0.069008\n",
      "cross entropy loss, iteration 214: = 0.068894\n",
      "cross entropy loss, iteration 215: = 0.068780\n",
      "cross entropy loss, iteration 216: = 0.068667\n",
      "cross entropy loss, iteration 217: = 0.068554\n",
      "cross entropy loss, iteration 218: = 0.068443\n",
      "cross entropy loss, iteration 219: = 0.068331\n",
      "cross entropy loss, iteration 220: = 0.068221\n",
      "cross entropy loss, iteration 221: = 0.068111\n",
      "cross entropy loss, iteration 222: = 0.068001\n",
      "cross entropy loss, iteration 223: = 0.067893\n",
      "cross entropy loss, iteration 224: = 0.067785\n",
      "cross entropy loss, iteration 225: = 0.067677\n",
      "cross entropy loss, iteration 226: = 0.067570\n",
      "cross entropy loss, iteration 227: = 0.067464\n",
      "cross entropy loss, iteration 228: = 0.067358\n",
      "cross entropy loss, iteration 229: = 0.067253\n",
      "cross entropy loss, iteration 230: = 0.067148\n",
      "cross entropy loss, iteration 231: = 0.067044\n",
      "cross entropy loss, iteration 232: = 0.066941\n",
      "cross entropy loss, iteration 233: = 0.066838\n",
      "cross entropy loss, iteration 234: = 0.066735\n",
      "cross entropy loss, iteration 235: = 0.066633\n",
      "cross entropy loss, iteration 236: = 0.066532\n",
      "cross entropy loss, iteration 237: = 0.066431\n",
      "cross entropy loss, iteration 238: = 0.066331\n",
      "cross entropy loss, iteration 239: = 0.066231\n",
      "cross entropy loss, iteration 240: = 0.066132\n",
      "cross entropy loss, iteration 241: = 0.066033\n",
      "cross entropy loss, iteration 242: = 0.065935\n",
      "cross entropy loss, iteration 243: = 0.065837\n",
      "cross entropy loss, iteration 244: = 0.065740\n",
      "cross entropy loss, iteration 245: = 0.065643\n",
      "cross entropy loss, iteration 246: = 0.065547\n",
      "cross entropy loss, iteration 247: = 0.065452\n",
      "cross entropy loss, iteration 248: = 0.065356\n",
      "cross entropy loss, iteration 249: = 0.065262\n",
      "cross entropy loss, iteration 250: = 0.065167\n",
      "cross entropy loss, iteration 251: = 0.065073\n",
      "cross entropy loss, iteration 252: = 0.064980\n",
      "cross entropy loss, iteration 253: = 0.064887\n",
      "cross entropy loss, iteration 254: = 0.064795\n",
      "cross entropy loss, iteration 255: = 0.064703\n",
      "cross entropy loss, iteration 256: = 0.064611\n",
      "cross entropy loss, iteration 257: = 0.064520\n",
      "cross entropy loss, iteration 258: = 0.064429\n",
      "cross entropy loss, iteration 259: = 0.064339\n",
      "cross entropy loss, iteration 260: = 0.064249\n",
      "cross entropy loss, iteration 261: = 0.064160\n",
      "cross entropy loss, iteration 262: = 0.064071\n",
      "cross entropy loss, iteration 263: = 0.063983\n",
      "cross entropy loss, iteration 264: = 0.063895\n",
      "cross entropy loss, iteration 265: = 0.063807\n",
      "cross entropy loss, iteration 266: = 0.063720\n",
      "cross entropy loss, iteration 267: = 0.063633\n",
      "cross entropy loss, iteration 268: = 0.063546\n",
      "cross entropy loss, iteration 269: = 0.063460\n",
      "cross entropy loss, iteration 270: = 0.063375\n",
      "cross entropy loss, iteration 271: = 0.063290\n",
      "cross entropy loss, iteration 272: = 0.063205\n",
      "cross entropy loss, iteration 273: = 0.063120\n",
      "cross entropy loss, iteration 274: = 0.063036\n",
      "cross entropy loss, iteration 275: = 0.062953\n",
      "cross entropy loss, iteration 276: = 0.062869\n",
      "cross entropy loss, iteration 277: = 0.062786\n",
      "cross entropy loss, iteration 278: = 0.062704\n",
      "cross entropy loss, iteration 279: = 0.062622\n",
      "cross entropy loss, iteration 280: = 0.062540\n",
      "cross entropy loss, iteration 281: = 0.062458\n",
      "cross entropy loss, iteration 282: = 0.062377\n",
      "cross entropy loss, iteration 283: = 0.062297\n",
      "cross entropy loss, iteration 284: = 0.062216\n",
      "cross entropy loss, iteration 285: = 0.062136\n",
      "cross entropy loss, iteration 286: = 0.062056\n",
      "cross entropy loss, iteration 287: = 0.061977\n",
      "cross entropy loss, iteration 288: = 0.061898\n",
      "cross entropy loss, iteration 289: = 0.061820\n",
      "cross entropy loss, iteration 290: = 0.061741\n",
      "cross entropy loss, iteration 291: = 0.061663\n",
      "cross entropy loss, iteration 292: = 0.061586\n",
      "cross entropy loss, iteration 293: = 0.061508\n",
      "cross entropy loss, iteration 294: = 0.061432\n",
      "cross entropy loss, iteration 295: = 0.061355\n",
      "cross entropy loss, iteration 296: = 0.061279\n",
      "cross entropy loss, iteration 297: = 0.061203\n",
      "cross entropy loss, iteration 298: = 0.061127\n",
      "cross entropy loss, iteration 299: = 0.061052\n",
      "cross entropy loss, iteration 300: = 0.060977\n",
      "cross entropy loss, iteration 301: = 0.060902\n",
      "cross entropy loss, iteration 302: = 0.060828\n",
      "cross entropy loss, iteration 303: = 0.060754\n",
      "cross entropy loss, iteration 304: = 0.060680\n",
      "cross entropy loss, iteration 305: = 0.060606\n",
      "cross entropy loss, iteration 306: = 0.060533\n",
      "cross entropy loss, iteration 307: = 0.060460\n",
      "cross entropy loss, iteration 308: = 0.060388\n",
      "cross entropy loss, iteration 309: = 0.060316\n",
      "cross entropy loss, iteration 310: = 0.060244\n",
      "cross entropy loss, iteration 311: = 0.060172\n",
      "cross entropy loss, iteration 312: = 0.060101\n",
      "cross entropy loss, iteration 313: = 0.060029\n",
      "cross entropy loss, iteration 314: = 0.059959\n",
      "cross entropy loss, iteration 315: = 0.059888\n",
      "cross entropy loss, iteration 316: = 0.059818\n",
      "cross entropy loss, iteration 317: = 0.059748\n",
      "cross entropy loss, iteration 318: = 0.059678\n",
      "cross entropy loss, iteration 319: = 0.059609\n",
      "cross entropy loss, iteration 320: = 0.059540\n",
      "cross entropy loss, iteration 321: = 0.059471\n",
      "cross entropy loss, iteration 322: = 0.059402\n",
      "cross entropy loss, iteration 323: = 0.059334\n",
      "cross entropy loss, iteration 324: = 0.059266\n",
      "cross entropy loss, iteration 325: = 0.059198\n",
      "cross entropy loss, iteration 326: = 0.059130\n",
      "cross entropy loss, iteration 327: = 0.059063\n",
      "cross entropy loss, iteration 328: = 0.058996\n",
      "cross entropy loss, iteration 329: = 0.058929\n",
      "cross entropy loss, iteration 330: = 0.058863\n",
      "cross entropy loss, iteration 331: = 0.058797\n",
      "cross entropy loss, iteration 332: = 0.058731\n",
      "cross entropy loss, iteration 333: = 0.058665\n",
      "cross entropy loss, iteration 334: = 0.058599\n",
      "cross entropy loss, iteration 335: = 0.058534\n",
      "cross entropy loss, iteration 336: = 0.058469\n",
      "cross entropy loss, iteration 337: = 0.058404\n",
      "cross entropy loss, iteration 338: = 0.058340\n",
      "cross entropy loss, iteration 339: = 0.058276\n",
      "cross entropy loss, iteration 340: = 0.058212\n",
      "cross entropy loss, iteration 341: = 0.058148\n",
      "cross entropy loss, iteration 342: = 0.058084\n",
      "cross entropy loss, iteration 343: = 0.058021\n",
      "cross entropy loss, iteration 344: = 0.057958\n",
      "cross entropy loss, iteration 345: = 0.057895\n",
      "cross entropy loss, iteration 346: = 0.057832\n",
      "cross entropy loss, iteration 347: = 0.057770\n",
      "cross entropy loss, iteration 348: = 0.057708\n",
      "cross entropy loss, iteration 349: = 0.057646\n",
      "cross entropy loss, iteration 350: = 0.057584\n",
      "cross entropy loss, iteration 351: = 0.057523\n",
      "cross entropy loss, iteration 352: = 0.057461\n",
      "cross entropy loss, iteration 353: = 0.057400\n",
      "cross entropy loss, iteration 354: = 0.057339\n",
      "cross entropy loss, iteration 355: = 0.057279\n",
      "cross entropy loss, iteration 356: = 0.057218\n",
      "cross entropy loss, iteration 357: = 0.057158\n",
      "cross entropy loss, iteration 358: = 0.057098\n",
      "cross entropy loss, iteration 359: = 0.057038\n",
      "cross entropy loss, iteration 360: = 0.056979\n",
      "cross entropy loss, iteration 361: = 0.056919\n",
      "cross entropy loss, iteration 362: = 0.056860\n",
      "cross entropy loss, iteration 363: = 0.056801\n",
      "cross entropy loss, iteration 364: = 0.056743\n",
      "cross entropy loss, iteration 365: = 0.056684\n",
      "cross entropy loss, iteration 366: = 0.056626\n",
      "cross entropy loss, iteration 367: = 0.056568\n",
      "cross entropy loss, iteration 368: = 0.056510\n",
      "cross entropy loss, iteration 369: = 0.056452\n",
      "cross entropy loss, iteration 370: = 0.056394\n",
      "cross entropy loss, iteration 371: = 0.056337\n",
      "cross entropy loss, iteration 372: = 0.056280\n",
      "cross entropy loss, iteration 373: = 0.056223\n",
      "cross entropy loss, iteration 374: = 0.056166\n",
      "cross entropy loss, iteration 375: = 0.056110\n",
      "cross entropy loss, iteration 376: = 0.056053\n",
      "cross entropy loss, iteration 377: = 0.055997\n",
      "cross entropy loss, iteration 378: = 0.055941\n",
      "cross entropy loss, iteration 379: = 0.055885\n",
      "cross entropy loss, iteration 380: = 0.055830\n",
      "cross entropy loss, iteration 381: = 0.055774\n",
      "cross entropy loss, iteration 382: = 0.055719\n",
      "cross entropy loss, iteration 383: = 0.055664\n",
      "cross entropy loss, iteration 384: = 0.055609\n",
      "cross entropy loss, iteration 385: = 0.055555\n",
      "cross entropy loss, iteration 386: = 0.055500\n",
      "cross entropy loss, iteration 387: = 0.055446\n",
      "cross entropy loss, iteration 388: = 0.055392\n",
      "cross entropy loss, iteration 389: = 0.055338\n",
      "cross entropy loss, iteration 390: = 0.055284\n",
      "cross entropy loss, iteration 391: = 0.055230\n",
      "cross entropy loss, iteration 392: = 0.055177\n",
      "cross entropy loss, iteration 393: = 0.055123\n",
      "cross entropy loss, iteration 394: = 0.055070\n",
      "cross entropy loss, iteration 395: = 0.055017\n",
      "cross entropy loss, iteration 396: = 0.054965\n",
      "cross entropy loss, iteration 397: = 0.054912\n",
      "cross entropy loss, iteration 398: = 0.054860\n",
      "cross entropy loss, iteration 399: = 0.054807\n",
      "cross entropy loss, iteration 400: = 0.054755\n",
      "cross entropy loss, iteration 401: = 0.054703\n",
      "cross entropy loss, iteration 402: = 0.054652\n",
      "cross entropy loss, iteration 403: = 0.054600\n",
      "cross entropy loss, iteration 404: = 0.054548\n",
      "cross entropy loss, iteration 405: = 0.054497\n",
      "cross entropy loss, iteration 406: = 0.054446\n",
      "cross entropy loss, iteration 407: = 0.054395\n",
      "cross entropy loss, iteration 408: = 0.054344\n",
      "cross entropy loss, iteration 409: = 0.054294\n",
      "cross entropy loss, iteration 410: = 0.054243\n",
      "cross entropy loss, iteration 411: = 0.054193\n",
      "cross entropy loss, iteration 412: = 0.054143\n",
      "cross entropy loss, iteration 413: = 0.054093\n",
      "cross entropy loss, iteration 414: = 0.054043\n",
      "cross entropy loss, iteration 415: = 0.053993\n",
      "cross entropy loss, iteration 416: = 0.053944\n",
      "cross entropy loss, iteration 417: = 0.053894\n",
      "cross entropy loss, iteration 418: = 0.053845\n",
      "cross entropy loss, iteration 419: = 0.053796\n",
      "cross entropy loss, iteration 420: = 0.053747\n",
      "cross entropy loss, iteration 421: = 0.053698\n",
      "cross entropy loss, iteration 422: = 0.053649\n",
      "cross entropy loss, iteration 423: = 0.053601\n",
      "cross entropy loss, iteration 424: = 0.053552\n",
      "cross entropy loss, iteration 425: = 0.053504\n",
      "cross entropy loss, iteration 426: = 0.053456\n",
      "cross entropy loss, iteration 427: = 0.053408\n",
      "cross entropy loss, iteration 428: = 0.053360\n",
      "cross entropy loss, iteration 429: = 0.053313\n",
      "cross entropy loss, iteration 430: = 0.053265\n",
      "cross entropy loss, iteration 431: = 0.053218\n",
      "cross entropy loss, iteration 432: = 0.053171\n",
      "cross entropy loss, iteration 433: = 0.053124\n",
      "cross entropy loss, iteration 434: = 0.053077\n",
      "cross entropy loss, iteration 435: = 0.053030\n",
      "cross entropy loss, iteration 436: = 0.052983\n",
      "cross entropy loss, iteration 437: = 0.052937\n",
      "cross entropy loss, iteration 438: = 0.052890\n",
      "cross entropy loss, iteration 439: = 0.052844\n",
      "cross entropy loss, iteration 440: = 0.052798\n",
      "cross entropy loss, iteration 441: = 0.052752\n",
      "cross entropy loss, iteration 442: = 0.052706\n",
      "cross entropy loss, iteration 443: = 0.052660\n",
      "cross entropy loss, iteration 444: = 0.052615\n",
      "cross entropy loss, iteration 445: = 0.052569\n",
      "cross entropy loss, iteration 446: = 0.052524\n",
      "cross entropy loss, iteration 447: = 0.052479\n",
      "cross entropy loss, iteration 448: = 0.052433\n",
      "cross entropy loss, iteration 449: = 0.052389\n",
      "cross entropy loss, iteration 450: = 0.052344\n",
      "cross entropy loss, iteration 451: = 0.052299\n",
      "cross entropy loss, iteration 452: = 0.052254\n",
      "cross entropy loss, iteration 453: = 0.052210\n",
      "cross entropy loss, iteration 454: = 0.052166\n",
      "cross entropy loss, iteration 455: = 0.052121\n",
      "cross entropy loss, iteration 456: = 0.052077\n",
      "cross entropy loss, iteration 457: = 0.052033\n",
      "cross entropy loss, iteration 458: = 0.051990\n",
      "cross entropy loss, iteration 459: = 0.051946\n",
      "cross entropy loss, iteration 460: = 0.051902\n",
      "cross entropy loss, iteration 461: = 0.051859\n",
      "cross entropy loss, iteration 462: = 0.051815\n",
      "cross entropy loss, iteration 463: = 0.051772\n",
      "cross entropy loss, iteration 464: = 0.051729\n",
      "cross entropy loss, iteration 465: = 0.051686\n",
      "cross entropy loss, iteration 466: = 0.051643\n",
      "cross entropy loss, iteration 467: = 0.051600\n",
      "cross entropy loss, iteration 468: = 0.051558\n",
      "cross entropy loss, iteration 469: = 0.051515\n",
      "cross entropy loss, iteration 470: = 0.051473\n",
      "cross entropy loss, iteration 471: = 0.051431\n",
      "cross entropy loss, iteration 472: = 0.051388\n",
      "cross entropy loss, iteration 473: = 0.051346\n",
      "cross entropy loss, iteration 474: = 0.051304\n",
      "cross entropy loss, iteration 475: = 0.051263\n",
      "cross entropy loss, iteration 476: = 0.051221\n",
      "cross entropy loss, iteration 477: = 0.051179\n",
      "cross entropy loss, iteration 478: = 0.051138\n",
      "cross entropy loss, iteration 479: = 0.051096\n",
      "cross entropy loss, iteration 480: = 0.051055\n",
      "cross entropy loss, iteration 481: = 0.051014\n",
      "cross entropy loss, iteration 482: = 0.050973\n",
      "cross entropy loss, iteration 483: = 0.050932\n",
      "cross entropy loss, iteration 484: = 0.050891\n",
      "cross entropy loss, iteration 485: = 0.050850\n",
      "cross entropy loss, iteration 486: = 0.050810\n",
      "cross entropy loss, iteration 487: = 0.050769\n",
      "cross entropy loss, iteration 488: = 0.050729\n",
      "cross entropy loss, iteration 489: = 0.050688\n",
      "cross entropy loss, iteration 490: = 0.050648\n",
      "cross entropy loss, iteration 491: = 0.050608\n",
      "cross entropy loss, iteration 492: = 0.050568\n",
      "cross entropy loss, iteration 493: = 0.050528\n",
      "cross entropy loss, iteration 494: = 0.050488\n",
      "cross entropy loss, iteration 495: = 0.050449\n",
      "cross entropy loss, iteration 496: = 0.050409\n",
      "cross entropy loss, iteration 497: = 0.050370\n",
      "cross entropy loss, iteration 498: = 0.050330\n",
      "cross entropy loss, iteration 499: = 0.050291\n",
      "cross entropy loss, iteration 500: = 0.050252\n",
      "cross entropy loss, iteration 501: = 0.050213\n",
      "cross entropy loss, iteration 502: = 0.050174\n",
      "cross entropy loss, iteration 503: = 0.050135\n",
      "cross entropy loss, iteration 504: = 0.050096\n",
      "cross entropy loss, iteration 505: = 0.050057\n",
      "cross entropy loss, iteration 506: = 0.050019\n",
      "cross entropy loss, iteration 507: = 0.049980\n",
      "cross entropy loss, iteration 508: = 0.049942\n",
      "cross entropy loss, iteration 509: = 0.049904\n",
      "cross entropy loss, iteration 510: = 0.049865\n",
      "cross entropy loss, iteration 511: = 0.049827\n",
      "cross entropy loss, iteration 512: = 0.049789\n",
      "cross entropy loss, iteration 513: = 0.049751\n",
      "cross entropy loss, iteration 514: = 0.049713\n",
      "cross entropy loss, iteration 515: = 0.049676\n",
      "cross entropy loss, iteration 516: = 0.049638\n",
      "cross entropy loss, iteration 517: = 0.049601\n",
      "cross entropy loss, iteration 518: = 0.049563\n",
      "cross entropy loss, iteration 519: = 0.049526\n",
      "cross entropy loss, iteration 520: = 0.049489\n",
      "cross entropy loss, iteration 521: = 0.049451\n",
      "cross entropy loss, iteration 522: = 0.049414\n",
      "cross entropy loss, iteration 523: = 0.049377\n",
      "cross entropy loss, iteration 524: = 0.049340\n",
      "cross entropy loss, iteration 525: = 0.049304\n",
      "cross entropy loss, iteration 526: = 0.049267\n",
      "cross entropy loss, iteration 527: = 0.049230\n",
      "cross entropy loss, iteration 528: = 0.049194\n",
      "cross entropy loss, iteration 529: = 0.049157\n",
      "cross entropy loss, iteration 530: = 0.049121\n",
      "cross entropy loss, iteration 531: = 0.049085\n",
      "cross entropy loss, iteration 532: = 0.049048\n",
      "cross entropy loss, iteration 533: = 0.049012\n",
      "cross entropy loss, iteration 534: = 0.048976\n",
      "cross entropy loss, iteration 535: = 0.048940\n",
      "cross entropy loss, iteration 536: = 0.048904\n",
      "cross entropy loss, iteration 537: = 0.048869\n",
      "cross entropy loss, iteration 538: = 0.048833\n",
      "cross entropy loss, iteration 539: = 0.048797\n",
      "cross entropy loss, iteration 540: = 0.048762\n",
      "cross entropy loss, iteration 541: = 0.048726\n",
      "cross entropy loss, iteration 542: = 0.048691\n",
      "cross entropy loss, iteration 543: = 0.048656\n",
      "cross entropy loss, iteration 544: = 0.048621\n",
      "cross entropy loss, iteration 545: = 0.048586\n",
      "cross entropy loss, iteration 546: = 0.048551\n",
      "cross entropy loss, iteration 547: = 0.048516\n",
      "cross entropy loss, iteration 548: = 0.048481\n",
      "cross entropy loss, iteration 549: = 0.048446\n",
      "cross entropy loss, iteration 550: = 0.048411\n",
      "cross entropy loss, iteration 551: = 0.048377\n",
      "cross entropy loss, iteration 552: = 0.048342\n",
      "cross entropy loss, iteration 553: = 0.048308\n",
      "cross entropy loss, iteration 554: = 0.048273\n",
      "cross entropy loss, iteration 555: = 0.048239\n",
      "cross entropy loss, iteration 556: = 0.048205\n",
      "cross entropy loss, iteration 557: = 0.048171\n",
      "cross entropy loss, iteration 558: = 0.048137\n",
      "cross entropy loss, iteration 559: = 0.048103\n",
      "cross entropy loss, iteration 560: = 0.048069\n",
      "cross entropy loss, iteration 561: = 0.048035\n",
      "cross entropy loss, iteration 562: = 0.048001\n",
      "cross entropy loss, iteration 563: = 0.047968\n",
      "cross entropy loss, iteration 564: = 0.047934\n",
      "cross entropy loss, iteration 565: = 0.047901\n",
      "cross entropy loss, iteration 566: = 0.047867\n",
      "cross entropy loss, iteration 567: = 0.047834\n",
      "cross entropy loss, iteration 568: = 0.047800\n",
      "cross entropy loss, iteration 569: = 0.047767\n",
      "cross entropy loss, iteration 570: = 0.047734\n",
      "cross entropy loss, iteration 571: = 0.047701\n",
      "cross entropy loss, iteration 572: = 0.047668\n",
      "cross entropy loss, iteration 573: = 0.047635\n",
      "cross entropy loss, iteration 574: = 0.047602\n",
      "cross entropy loss, iteration 575: = 0.047570\n",
      "cross entropy loss, iteration 576: = 0.047537\n",
      "cross entropy loss, iteration 577: = 0.047504\n",
      "cross entropy loss, iteration 578: = 0.047472\n",
      "cross entropy loss, iteration 579: = 0.047439\n",
      "cross entropy loss, iteration 580: = 0.047407\n",
      "cross entropy loss, iteration 581: = 0.047374\n",
      "cross entropy loss, iteration 582: = 0.047342\n",
      "cross entropy loss, iteration 583: = 0.047310\n",
      "cross entropy loss, iteration 584: = 0.047278\n",
      "cross entropy loss, iteration 585: = 0.047246\n",
      "cross entropy loss, iteration 586: = 0.047214\n",
      "cross entropy loss, iteration 587: = 0.047182\n",
      "cross entropy loss, iteration 588: = 0.047150\n",
      "cross entropy loss, iteration 589: = 0.047118\n",
      "cross entropy loss, iteration 590: = 0.047087\n",
      "cross entropy loss, iteration 591: = 0.047055\n",
      "cross entropy loss, iteration 592: = 0.047023\n",
      "cross entropy loss, iteration 593: = 0.046992\n",
      "cross entropy loss, iteration 594: = 0.046960\n",
      "cross entropy loss, iteration 595: = 0.046929\n",
      "cross entropy loss, iteration 596: = 0.046898\n",
      "cross entropy loss, iteration 597: = 0.046866\n",
      "cross entropy loss, iteration 598: = 0.046835\n",
      "cross entropy loss, iteration 599: = 0.046804\n",
      "cross entropy loss, iteration 600: = 0.046773\n",
      "cross entropy loss, iteration 601: = 0.046742\n",
      "cross entropy loss, iteration 602: = 0.046711\n",
      "cross entropy loss, iteration 603: = 0.046681\n",
      "cross entropy loss, iteration 604: = 0.046650\n",
      "cross entropy loss, iteration 605: = 0.046619\n",
      "cross entropy loss, iteration 606: = 0.046588\n",
      "cross entropy loss, iteration 607: = 0.046558\n",
      "cross entropy loss, iteration 608: = 0.046527\n",
      "cross entropy loss, iteration 609: = 0.046497\n",
      "cross entropy loss, iteration 610: = 0.046466\n",
      "cross entropy loss, iteration 611: = 0.046436\n",
      "cross entropy loss, iteration 612: = 0.046406\n",
      "cross entropy loss, iteration 613: = 0.046376\n",
      "cross entropy loss, iteration 614: = 0.046346\n",
      "cross entropy loss, iteration 615: = 0.046315\n",
      "cross entropy loss, iteration 616: = 0.046285\n",
      "cross entropy loss, iteration 617: = 0.046256\n",
      "cross entropy loss, iteration 618: = 0.046226\n",
      "cross entropy loss, iteration 619: = 0.046196\n",
      "cross entropy loss, iteration 620: = 0.046166\n",
      "cross entropy loss, iteration 621: = 0.046136\n",
      "cross entropy loss, iteration 622: = 0.046107\n",
      "cross entropy loss, iteration 623: = 0.046077\n",
      "cross entropy loss, iteration 624: = 0.046048\n",
      "cross entropy loss, iteration 625: = 0.046018\n",
      "cross entropy loss, iteration 626: = 0.045989\n",
      "cross entropy loss, iteration 627: = 0.045959\n",
      "cross entropy loss, iteration 628: = 0.045930\n",
      "cross entropy loss, iteration 629: = 0.045901\n",
      "cross entropy loss, iteration 630: = 0.045872\n",
      "cross entropy loss, iteration 631: = 0.045843\n",
      "cross entropy loss, iteration 632: = 0.045814\n",
      "cross entropy loss, iteration 633: = 0.045785\n",
      "cross entropy loss, iteration 634: = 0.045756\n",
      "cross entropy loss, iteration 635: = 0.045727\n",
      "cross entropy loss, iteration 636: = 0.045698\n",
      "cross entropy loss, iteration 637: = 0.045669\n",
      "cross entropy loss, iteration 638: = 0.045641\n",
      "cross entropy loss, iteration 639: = 0.045612\n",
      "cross entropy loss, iteration 640: = 0.045583\n",
      "cross entropy loss, iteration 641: = 0.045555\n",
      "cross entropy loss, iteration 642: = 0.045526\n",
      "cross entropy loss, iteration 643: = 0.045498\n",
      "cross entropy loss, iteration 644: = 0.045470\n",
      "cross entropy loss, iteration 645: = 0.045441\n",
      "cross entropy loss, iteration 646: = 0.045413\n",
      "cross entropy loss, iteration 647: = 0.045385\n",
      "cross entropy loss, iteration 648: = 0.045357\n",
      "cross entropy loss, iteration 649: = 0.045329\n",
      "cross entropy loss, iteration 650: = 0.045301\n",
      "cross entropy loss, iteration 651: = 0.045273\n",
      "cross entropy loss, iteration 652: = 0.045245\n",
      "cross entropy loss, iteration 653: = 0.045217\n",
      "cross entropy loss, iteration 654: = 0.045189\n",
      "cross entropy loss, iteration 655: = 0.045161\n",
      "cross entropy loss, iteration 656: = 0.045134\n",
      "cross entropy loss, iteration 657: = 0.045106\n",
      "cross entropy loss, iteration 658: = 0.045079\n",
      "cross entropy loss, iteration 659: = 0.045051\n",
      "cross entropy loss, iteration 660: = 0.045024\n",
      "cross entropy loss, iteration 661: = 0.044996\n",
      "cross entropy loss, iteration 662: = 0.044969\n",
      "cross entropy loss, iteration 663: = 0.044941\n",
      "cross entropy loss, iteration 664: = 0.044914\n",
      "cross entropy loss, iteration 665: = 0.044887\n",
      "cross entropy loss, iteration 666: = 0.044860\n",
      "cross entropy loss, iteration 667: = 0.044833\n",
      "cross entropy loss, iteration 668: = 0.044806\n",
      "cross entropy loss, iteration 669: = 0.044779\n",
      "cross entropy loss, iteration 670: = 0.044752\n",
      "cross entropy loss, iteration 671: = 0.044725\n",
      "cross entropy loss, iteration 672: = 0.044698\n",
      "cross entropy loss, iteration 673: = 0.044671\n",
      "cross entropy loss, iteration 674: = 0.044644\n",
      "cross entropy loss, iteration 675: = 0.044618\n",
      "cross entropy loss, iteration 676: = 0.044591\n",
      "cross entropy loss, iteration 677: = 0.044564\n",
      "cross entropy loss, iteration 678: = 0.044538\n",
      "cross entropy loss, iteration 679: = 0.044511\n",
      "cross entropy loss, iteration 680: = 0.044485\n",
      "cross entropy loss, iteration 681: = 0.044458\n",
      "cross entropy loss, iteration 682: = 0.044432\n",
      "cross entropy loss, iteration 683: = 0.044406\n",
      "cross entropy loss, iteration 684: = 0.044380\n",
      "cross entropy loss, iteration 685: = 0.044353\n",
      "cross entropy loss, iteration 686: = 0.044327\n",
      "cross entropy loss, iteration 687: = 0.044301\n",
      "cross entropy loss, iteration 688: = 0.044275\n",
      "cross entropy loss, iteration 689: = 0.044249\n",
      "cross entropy loss, iteration 690: = 0.044223\n",
      "cross entropy loss, iteration 691: = 0.044197\n",
      "cross entropy loss, iteration 692: = 0.044171\n",
      "cross entropy loss, iteration 693: = 0.044145\n",
      "cross entropy loss, iteration 694: = 0.044120\n",
      "cross entropy loss, iteration 695: = 0.044094\n",
      "cross entropy loss, iteration 696: = 0.044068\n",
      "cross entropy loss, iteration 697: = 0.044043\n",
      "cross entropy loss, iteration 698: = 0.044017\n",
      "cross entropy loss, iteration 699: = 0.043992\n",
      "cross entropy loss, iteration 700: = 0.043966\n",
      "cross entropy loss, iteration 701: = 0.043941\n",
      "cross entropy loss, iteration 702: = 0.043915\n",
      "cross entropy loss, iteration 703: = 0.043890\n",
      "cross entropy loss, iteration 704: = 0.043865\n",
      "cross entropy loss, iteration 705: = 0.043839\n",
      "cross entropy loss, iteration 706: = 0.043814\n",
      "cross entropy loss, iteration 707: = 0.043789\n",
      "cross entropy loss, iteration 708: = 0.043764\n",
      "cross entropy loss, iteration 709: = 0.043739\n",
      "cross entropy loss, iteration 710: = 0.043714\n",
      "cross entropy loss, iteration 711: = 0.043689\n",
      "cross entropy loss, iteration 712: = 0.043664\n",
      "cross entropy loss, iteration 713: = 0.043639\n",
      "cross entropy loss, iteration 714: = 0.043614\n",
      "cross entropy loss, iteration 715: = 0.043589\n",
      "cross entropy loss, iteration 716: = 0.043564\n",
      "cross entropy loss, iteration 717: = 0.043540\n",
      "cross entropy loss, iteration 718: = 0.043515\n",
      "cross entropy loss, iteration 719: = 0.043490\n",
      "cross entropy loss, iteration 720: = 0.043466\n",
      "cross entropy loss, iteration 721: = 0.043441\n",
      "cross entropy loss, iteration 722: = 0.043417\n",
      "cross entropy loss, iteration 723: = 0.043392\n",
      "cross entropy loss, iteration 724: = 0.043368\n",
      "cross entropy loss, iteration 725: = 0.043343\n",
      "cross entropy loss, iteration 726: = 0.043319\n",
      "cross entropy loss, iteration 727: = 0.043295\n",
      "cross entropy loss, iteration 728: = 0.043271\n",
      "cross entropy loss, iteration 729: = 0.043246\n",
      "cross entropy loss, iteration 730: = 0.043222\n",
      "cross entropy loss, iteration 731: = 0.043198\n",
      "cross entropy loss, iteration 732: = 0.043174\n",
      "cross entropy loss, iteration 733: = 0.043150\n",
      "cross entropy loss, iteration 734: = 0.043126\n",
      "cross entropy loss, iteration 735: = 0.043102\n",
      "cross entropy loss, iteration 736: = 0.043078\n",
      "cross entropy loss, iteration 737: = 0.043054\n",
      "cross entropy loss, iteration 738: = 0.043030\n",
      "cross entropy loss, iteration 739: = 0.043007\n",
      "cross entropy loss, iteration 740: = 0.042983\n",
      "cross entropy loss, iteration 741: = 0.042959\n",
      "cross entropy loss, iteration 742: = 0.042935\n",
      "cross entropy loss, iteration 743: = 0.042912\n",
      "cross entropy loss, iteration 744: = 0.042888\n",
      "cross entropy loss, iteration 745: = 0.042865\n",
      "cross entropy loss, iteration 746: = 0.042841\n",
      "cross entropy loss, iteration 747: = 0.042818\n",
      "cross entropy loss, iteration 748: = 0.042794\n",
      "cross entropy loss, iteration 749: = 0.042771\n",
      "cross entropy loss, iteration 750: = 0.042747\n",
      "cross entropy loss, iteration 751: = 0.042724\n",
      "cross entropy loss, iteration 752: = 0.042701\n",
      "cross entropy loss, iteration 753: = 0.042678\n",
      "cross entropy loss, iteration 754: = 0.042654\n",
      "cross entropy loss, iteration 755: = 0.042631\n",
      "cross entropy loss, iteration 756: = 0.042608\n",
      "cross entropy loss, iteration 757: = 0.042585\n",
      "cross entropy loss, iteration 758: = 0.042562\n",
      "cross entropy loss, iteration 759: = 0.042539\n",
      "cross entropy loss, iteration 760: = 0.042516\n",
      "cross entropy loss, iteration 761: = 0.042493\n",
      "cross entropy loss, iteration 762: = 0.042470\n",
      "cross entropy loss, iteration 763: = 0.042447\n",
      "cross entropy loss, iteration 764: = 0.042425\n",
      "cross entropy loss, iteration 765: = 0.042402\n",
      "cross entropy loss, iteration 766: = 0.042379\n",
      "cross entropy loss, iteration 767: = 0.042356\n",
      "cross entropy loss, iteration 768: = 0.042334\n",
      "cross entropy loss, iteration 769: = 0.042311\n",
      "cross entropy loss, iteration 770: = 0.042289\n",
      "cross entropy loss, iteration 771: = 0.042266\n",
      "cross entropy loss, iteration 772: = 0.042244\n",
      "cross entropy loss, iteration 773: = 0.042221\n",
      "cross entropy loss, iteration 774: = 0.042199\n",
      "cross entropy loss, iteration 775: = 0.042176\n",
      "cross entropy loss, iteration 776: = 0.042154\n",
      "cross entropy loss, iteration 777: = 0.042132\n",
      "cross entropy loss, iteration 778: = 0.042109\n",
      "cross entropy loss, iteration 779: = 0.042087\n",
      "cross entropy loss, iteration 780: = 0.042065\n",
      "cross entropy loss, iteration 781: = 0.042043\n",
      "cross entropy loss, iteration 782: = 0.042020\n",
      "cross entropy loss, iteration 783: = 0.041998\n",
      "cross entropy loss, iteration 784: = 0.041976\n",
      "cross entropy loss, iteration 785: = 0.041954\n",
      "cross entropy loss, iteration 786: = 0.041932\n",
      "cross entropy loss, iteration 787: = 0.041910\n",
      "cross entropy loss, iteration 788: = 0.041888\n",
      "cross entropy loss, iteration 789: = 0.041866\n",
      "cross entropy loss, iteration 790: = 0.041845\n",
      "cross entropy loss, iteration 791: = 0.041823\n",
      "cross entropy loss, iteration 792: = 0.041801\n",
      "cross entropy loss, iteration 793: = 0.041779\n",
      "cross entropy loss, iteration 794: = 0.041757\n",
      "cross entropy loss, iteration 795: = 0.041736\n",
      "cross entropy loss, iteration 796: = 0.041714\n",
      "cross entropy loss, iteration 797: = 0.041693\n",
      "cross entropy loss, iteration 798: = 0.041671\n",
      "cross entropy loss, iteration 799: = 0.041649\n",
      "cross entropy loss, iteration 800: = 0.041628\n",
      "cross entropy loss, iteration 801: = 0.041606\n",
      "cross entropy loss, iteration 802: = 0.041585\n",
      "cross entropy loss, iteration 803: = 0.041564\n",
      "cross entropy loss, iteration 804: = 0.041542\n",
      "cross entropy loss, iteration 805: = 0.041521\n",
      "cross entropy loss, iteration 806: = 0.041500\n",
      "cross entropy loss, iteration 807: = 0.041478\n",
      "cross entropy loss, iteration 808: = 0.041457\n",
      "cross entropy loss, iteration 809: = 0.041436\n",
      "cross entropy loss, iteration 810: = 0.041415\n",
      "cross entropy loss, iteration 811: = 0.041393\n",
      "cross entropy loss, iteration 812: = 0.041372\n",
      "cross entropy loss, iteration 813: = 0.041351\n",
      "cross entropy loss, iteration 814: = 0.041330\n",
      "cross entropy loss, iteration 815: = 0.041309\n",
      "cross entropy loss, iteration 816: = 0.041288\n",
      "cross entropy loss, iteration 817: = 0.041267\n",
      "cross entropy loss, iteration 818: = 0.041246\n",
      "cross entropy loss, iteration 819: = 0.041225\n",
      "cross entropy loss, iteration 820: = 0.041205\n",
      "cross entropy loss, iteration 821: = 0.041184\n",
      "cross entropy loss, iteration 822: = 0.041163\n",
      "cross entropy loss, iteration 823: = 0.041142\n",
      "cross entropy loss, iteration 824: = 0.041122\n",
      "cross entropy loss, iteration 825: = 0.041101\n",
      "cross entropy loss, iteration 826: = 0.041080\n",
      "cross entropy loss, iteration 827: = 0.041060\n",
      "cross entropy loss, iteration 828: = 0.041039\n",
      "cross entropy loss, iteration 829: = 0.041018\n",
      "cross entropy loss, iteration 830: = 0.040998\n",
      "cross entropy loss, iteration 831: = 0.040977\n",
      "cross entropy loss, iteration 832: = 0.040957\n",
      "cross entropy loss, iteration 833: = 0.040936\n",
      "cross entropy loss, iteration 834: = 0.040916\n",
      "cross entropy loss, iteration 835: = 0.040896\n",
      "cross entropy loss, iteration 836: = 0.040875\n",
      "cross entropy loss, iteration 837: = 0.040855\n",
      "cross entropy loss, iteration 838: = 0.040835\n",
      "cross entropy loss, iteration 839: = 0.040814\n",
      "cross entropy loss, iteration 840: = 0.040794\n",
      "cross entropy loss, iteration 841: = 0.040774\n",
      "cross entropy loss, iteration 842: = 0.040754\n",
      "cross entropy loss, iteration 843: = 0.040734\n",
      "cross entropy loss, iteration 844: = 0.040714\n",
      "cross entropy loss, iteration 845: = 0.040693\n",
      "cross entropy loss, iteration 846: = 0.040673\n",
      "cross entropy loss, iteration 847: = 0.040653\n",
      "cross entropy loss, iteration 848: = 0.040633\n",
      "cross entropy loss, iteration 849: = 0.040613\n",
      "cross entropy loss, iteration 850: = 0.040594\n",
      "cross entropy loss, iteration 851: = 0.040574\n",
      "cross entropy loss, iteration 852: = 0.040554\n",
      "cross entropy loss, iteration 853: = 0.040534\n",
      "cross entropy loss, iteration 854: = 0.040514\n",
      "cross entropy loss, iteration 855: = 0.040494\n",
      "cross entropy loss, iteration 856: = 0.040475\n",
      "cross entropy loss, iteration 857: = 0.040455\n",
      "cross entropy loss, iteration 858: = 0.040435\n",
      "cross entropy loss, iteration 859: = 0.040416\n",
      "cross entropy loss, iteration 860: = 0.040396\n",
      "cross entropy loss, iteration 861: = 0.040376\n",
      "cross entropy loss, iteration 862: = 0.040357\n",
      "cross entropy loss, iteration 863: = 0.040337\n",
      "cross entropy loss, iteration 864: = 0.040318\n",
      "cross entropy loss, iteration 865: = 0.040298\n",
      "cross entropy loss, iteration 866: = 0.040279\n",
      "cross entropy loss, iteration 867: = 0.040259\n",
      "cross entropy loss, iteration 868: = 0.040240\n",
      "cross entropy loss, iteration 869: = 0.040221\n",
      "cross entropy loss, iteration 870: = 0.040201\n",
      "cross entropy loss, iteration 871: = 0.040182\n",
      "cross entropy loss, iteration 872: = 0.040163\n",
      "cross entropy loss, iteration 873: = 0.040143\n",
      "cross entropy loss, iteration 874: = 0.040124\n",
      "cross entropy loss, iteration 875: = 0.040105\n",
      "cross entropy loss, iteration 876: = 0.040086\n",
      "cross entropy loss, iteration 877: = 0.040066\n",
      "cross entropy loss, iteration 878: = 0.040047\n",
      "cross entropy loss, iteration 879: = 0.040028\n",
      "cross entropy loss, iteration 880: = 0.040009\n",
      "cross entropy loss, iteration 881: = 0.039990\n",
      "cross entropy loss, iteration 882: = 0.039971\n",
      "cross entropy loss, iteration 883: = 0.039952\n",
      "cross entropy loss, iteration 884: = 0.039933\n",
      "cross entropy loss, iteration 885: = 0.039914\n",
      "cross entropy loss, iteration 886: = 0.039895\n",
      "cross entropy loss, iteration 887: = 0.039876\n",
      "cross entropy loss, iteration 888: = 0.039858\n",
      "cross entropy loss, iteration 889: = 0.039839\n",
      "cross entropy loss, iteration 890: = 0.039820\n",
      "cross entropy loss, iteration 891: = 0.039801\n",
      "cross entropy loss, iteration 892: = 0.039782\n",
      "cross entropy loss, iteration 893: = 0.039764\n",
      "cross entropy loss, iteration 894: = 0.039745\n",
      "cross entropy loss, iteration 895: = 0.039726\n",
      "cross entropy loss, iteration 896: = 0.039708\n",
      "cross entropy loss, iteration 897: = 0.039689\n",
      "cross entropy loss, iteration 898: = 0.039670\n",
      "cross entropy loss, iteration 899: = 0.039652\n",
      "cross entropy loss, iteration 900: = 0.039633\n",
      "cross entropy loss, iteration 901: = 0.039615\n",
      "cross entropy loss, iteration 902: = 0.039596\n",
      "cross entropy loss, iteration 903: = 0.039578\n",
      "cross entropy loss, iteration 904: = 0.039559\n",
      "cross entropy loss, iteration 905: = 0.039541\n",
      "cross entropy loss, iteration 906: = 0.039523\n",
      "cross entropy loss, iteration 907: = 0.039504\n",
      "cross entropy loss, iteration 908: = 0.039486\n",
      "cross entropy loss, iteration 909: = 0.039468\n",
      "cross entropy loss, iteration 910: = 0.039449\n",
      "cross entropy loss, iteration 911: = 0.039431\n",
      "cross entropy loss, iteration 912: = 0.039413\n",
      "cross entropy loss, iteration 913: = 0.039395\n",
      "cross entropy loss, iteration 914: = 0.039377\n",
      "cross entropy loss, iteration 915: = 0.039358\n",
      "cross entropy loss, iteration 916: = 0.039340\n",
      "cross entropy loss, iteration 917: = 0.039322\n",
      "cross entropy loss, iteration 918: = 0.039304\n",
      "cross entropy loss, iteration 919: = 0.039286\n",
      "cross entropy loss, iteration 920: = 0.039268\n",
      "cross entropy loss, iteration 921: = 0.039250\n",
      "cross entropy loss, iteration 922: = 0.039232\n",
      "cross entropy loss, iteration 923: = 0.039214\n",
      "cross entropy loss, iteration 924: = 0.039196\n",
      "cross entropy loss, iteration 925: = 0.039178\n",
      "cross entropy loss, iteration 926: = 0.039160\n",
      "cross entropy loss, iteration 927: = 0.039142\n",
      "cross entropy loss, iteration 928: = 0.039125\n",
      "cross entropy loss, iteration 929: = 0.039107\n",
      "cross entropy loss, iteration 930: = 0.039089\n",
      "cross entropy loss, iteration 931: = 0.039071\n",
      "cross entropy loss, iteration 932: = 0.039053\n",
      "cross entropy loss, iteration 933: = 0.039036\n",
      "cross entropy loss, iteration 934: = 0.039018\n",
      "cross entropy loss, iteration 935: = 0.039000\n",
      "cross entropy loss, iteration 936: = 0.038983\n",
      "cross entropy loss, iteration 937: = 0.038965\n",
      "cross entropy loss, iteration 938: = 0.038947\n",
      "cross entropy loss, iteration 939: = 0.038930\n",
      "cross entropy loss, iteration 940: = 0.038912\n",
      "cross entropy loss, iteration 941: = 0.038895\n",
      "cross entropy loss, iteration 942: = 0.038877\n",
      "cross entropy loss, iteration 943: = 0.038860\n",
      "cross entropy loss, iteration 944: = 0.038842\n",
      "cross entropy loss, iteration 945: = 0.038825\n",
      "cross entropy loss, iteration 946: = 0.038808\n",
      "cross entropy loss, iteration 947: = 0.038790\n",
      "cross entropy loss, iteration 948: = 0.038773\n",
      "cross entropy loss, iteration 949: = 0.038755\n",
      "cross entropy loss, iteration 950: = 0.038738\n",
      "cross entropy loss, iteration 951: = 0.038721\n",
      "cross entropy loss, iteration 952: = 0.038704\n",
      "cross entropy loss, iteration 953: = 0.038686\n",
      "cross entropy loss, iteration 954: = 0.038669\n",
      "cross entropy loss, iteration 955: = 0.038652\n",
      "cross entropy loss, iteration 956: = 0.038635\n",
      "cross entropy loss, iteration 957: = 0.038618\n",
      "cross entropy loss, iteration 958: = 0.038600\n",
      "cross entropy loss, iteration 959: = 0.038583\n",
      "cross entropy loss, iteration 960: = 0.038566\n",
      "cross entropy loss, iteration 961: = 0.038549\n",
      "cross entropy loss, iteration 962: = 0.038532\n",
      "cross entropy loss, iteration 963: = 0.038515\n",
      "cross entropy loss, iteration 964: = 0.038498\n",
      "cross entropy loss, iteration 965: = 0.038481\n",
      "cross entropy loss, iteration 966: = 0.038464\n",
      "cross entropy loss, iteration 967: = 0.038447\n",
      "cross entropy loss, iteration 968: = 0.038430\n",
      "cross entropy loss, iteration 969: = 0.038413\n",
      "cross entropy loss, iteration 970: = 0.038396\n",
      "cross entropy loss, iteration 971: = 0.038380\n",
      "cross entropy loss, iteration 972: = 0.038363\n",
      "cross entropy loss, iteration 973: = 0.038346\n",
      "cross entropy loss, iteration 974: = 0.038329\n",
      "cross entropy loss, iteration 975: = 0.038312\n",
      "cross entropy loss, iteration 976: = 0.038296\n",
      "cross entropy loss, iteration 977: = 0.038279\n",
      "cross entropy loss, iteration 978: = 0.038262\n",
      "cross entropy loss, iteration 979: = 0.038246\n",
      "cross entropy loss, iteration 980: = 0.038229\n",
      "cross entropy loss, iteration 981: = 0.038212\n",
      "cross entropy loss, iteration 982: = 0.038196\n",
      "cross entropy loss, iteration 983: = 0.038179\n",
      "cross entropy loss, iteration 984: = 0.038163\n",
      "cross entropy loss, iteration 985: = 0.038146\n",
      "cross entropy loss, iteration 986: = 0.038129\n",
      "cross entropy loss, iteration 987: = 0.038113\n",
      "cross entropy loss, iteration 988: = 0.038096\n",
      "cross entropy loss, iteration 989: = 0.038080\n",
      "cross entropy loss, iteration 990: = 0.038064\n",
      "cross entropy loss, iteration 991: = 0.038047\n",
      "cross entropy loss, iteration 992: = 0.038031\n",
      "cross entropy loss, iteration 993: = 0.038014\n",
      "cross entropy loss, iteration 994: = 0.037998\n",
      "cross entropy loss, iteration 995: = 0.037982\n",
      "cross entropy loss, iteration 996: = 0.037965\n",
      "cross entropy loss, iteration 997: = 0.037949\n",
      "cross entropy loss, iteration 998: = 0.037933\n",
      "cross entropy loss, iteration 999: = 0.037916\n"
     ]
    }
   ],
   "source": [
    "# iterate learning rule a number of times\n",
    "\n",
    "num_steps = 1000\n",
    "\n",
    "train_error = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    sess.run(train_op,feed_dict={Y:output_train_onehot,R:response_train})\n",
    "    \n",
    "    print \"cross entropy loss, iteration %d: = %f\" % (step,sess.run(cross_entropy_loss,feed_dict={Y:output_train_onehot,R:response_train}))\n",
    "    \n",
    "    incorrect = tf.not_equal(tf.argmax(Y_prob,1),tf.argmax(Y,1))\n",
    "    percent_incorrect = tf.reduce_mean(tf.cast(incorrect,tf.float32))\n",
    "    train_error.append(sess.run(percent_incorrect,feed_dict={Y:output_train_onehot,R:response_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train error = 5.90%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14d0a74d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_error)\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"test error\")\n",
    "\n",
    "print \"Final train error = {0:0.2f}%\".format(train_error[-1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stim:  probability\n",
      "0:  0.000000\n",
      "45:  0.000000\n",
      "90:  0.000000\n",
      "135:  0.000000\n",
      "180:  1.000000\n",
      "225:  0.000000\n",
      "270:  0.000000\n",
      "315:  0.000000\n",
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "correct = tf.equal(tf.argmax(Y_prob,1),tf.argmax(Y,1))\n",
    "\n",
    "probs =  sess.run(Y_prob,feed_dict={Y:output_test_onehot,R:response_test})\n",
    "\n",
    "#print probs\n",
    "print \"stim:  probability\"\n",
    "for i,stim in enumerate(np.unique(output_train)):\n",
    "    print \"%d:  %f\" % (stim,probs[0][i])\n",
    "    \n",
    "print sess.run(correct,feed_dict={Y:output_test_onehot,R:response_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a network with no \"hidden\" units.  Let's try a network with one hidden layer and see what happens.  I'm using new variable names for similar objects to avoid name collision in the same notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the variables with randomize initial values\n",
    "\n",
    "num_hidden = 30\n",
    "\n",
    "W_input = tf.Variable(0.01*np.random.normal(size=[num_cells,num_hidden]),dtype=tf.float32)\n",
    "b_input = tf.Variable(0.01*np.random.normal(num_hidden),dtype=tf.float32)\n",
    "\n",
    "W_hidden = tf.Variable(0.01*np.random.normal(size=[num_hidden,num_classes]),dtype=tf.float32)\n",
    "b_hidden = tf.Variable(0.01*np.random.normal(num_classes),dtype=tf.float32)\n",
    "\n",
    "# The \"responses\" from which we'll decode will be \"placeholders\"\n",
    "R_hidden = tf.placeholder(tf.float32,shape=(None,num_cells))  # None means that this dimension can have an arbitrary length\n",
    "Y_hidden = tf.placeholder(tf.float32,shape=(None,num_classes)) # \"one-hot\" encoding of stimulus condition\n",
    "\n",
    "# define the ops for the outputs\n",
    "\n",
    "# we'll use a relu for the output of the hidden layer\n",
    "hidden_layer_output = tf.nn.relu(tf.matmul(R_hidden,W_input) + b_input)\n",
    "\n",
    "# and softmax for the estimate of the probabilities\n",
    "Y_prob_hidden = tf.nn.softmax(tf.matmul(hidden_layer_output,W_hidden) + b_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beta = 0.05\n",
    "\n",
    "# adding L2 normalization to the weights for this model\n",
    "cross_entropy_loss_hidden = -tf.reduce_mean(Y_hidden*tf.log(Y_prob_hidden) \n",
    "                                            + beta*tf.nn.l2_loss(W_input) \n",
    "                                            + beta*tf.nn.l2_loss(W_hidden) \n",
    "                                            + beta*tf.nn.l2_loss(b_input) \n",
    "                                            + beta*tf.nn.l2_loss(b_hidden))\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_op_hidden = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss_hidden)\n",
    "\n",
    "init_op_hidden = tf.initialize_all_variables()\n",
    "\n",
    "sess_hidden = tf.Session()\n",
    "\n",
    "sess_hidden.run(init_op_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross entropy loss, iteration 0: = 0.248864\n",
      "cross entropy loss, iteration 1: = 0.248749\n",
      "cross entropy loss, iteration 2: = 0.248637\n",
      "cross entropy loss, iteration 3: = 0.248525\n",
      "cross entropy loss, iteration 4: = 0.248413\n",
      "cross entropy loss, iteration 5: = 0.248301\n",
      "cross entropy loss, iteration 6: = 0.248186\n",
      "cross entropy loss, iteration 7: = 0.248080\n",
      "cross entropy loss, iteration 8: = 0.247964\n",
      "cross entropy loss, iteration 9: = 0.247852\n",
      "cross entropy loss, iteration 10: = 0.247742\n",
      "cross entropy loss, iteration 11: = 0.247626\n",
      "cross entropy loss, iteration 12: = 0.247520\n",
      "cross entropy loss, iteration 13: = 0.247405\n",
      "cross entropy loss, iteration 14: = 0.247294\n",
      "cross entropy loss, iteration 15: = 0.247182\n",
      "cross entropy loss, iteration 16: = 0.247068\n",
      "cross entropy loss, iteration 17: = 0.246962\n",
      "cross entropy loss, iteration 18: = 0.246846\n",
      "cross entropy loss, iteration 19: = 0.246735\n",
      "cross entropy loss, iteration 20: = 0.246623\n",
      "cross entropy loss, iteration 21: = 0.246506\n",
      "cross entropy loss, iteration 22: = 0.246395\n",
      "cross entropy loss, iteration 23: = 0.246277\n",
      "cross entropy loss, iteration 24: = 0.246164\n",
      "cross entropy loss, iteration 25: = 0.246051\n",
      "cross entropy loss, iteration 26: = 0.245937\n",
      "cross entropy loss, iteration 27: = 0.245819\n",
      "cross entropy loss, iteration 28: = 0.245704\n",
      "cross entropy loss, iteration 29: = 0.245584\n",
      "cross entropy loss, iteration 30: = 0.245468\n",
      "cross entropy loss, iteration 31: = 0.245352\n",
      "cross entropy loss, iteration 32: = 0.245230\n",
      "cross entropy loss, iteration 33: = 0.245112\n",
      "cross entropy loss, iteration 34: = 0.244990\n",
      "cross entropy loss, iteration 35: = 0.244870\n",
      "cross entropy loss, iteration 36: = 0.244746\n",
      "cross entropy loss, iteration 37: = 0.244626\n",
      "cross entropy loss, iteration 38: = 0.244501\n",
      "cross entropy loss, iteration 39: = 0.244379\n",
      "cross entropy loss, iteration 40: = 0.244252\n",
      "cross entropy loss, iteration 41: = 0.244129\n",
      "cross entropy loss, iteration 42: = 0.244001\n",
      "cross entropy loss, iteration 43: = 0.243875\n",
      "cross entropy loss, iteration 44: = 0.243745\n",
      "cross entropy loss, iteration 45: = 0.243618\n",
      "cross entropy loss, iteration 46: = 0.243486\n",
      "cross entropy loss, iteration 47: = 0.243353\n",
      "cross entropy loss, iteration 48: = 0.243222\n",
      "cross entropy loss, iteration 49: = 0.243085\n",
      "cross entropy loss, iteration 50: = 0.242952\n",
      "cross entropy loss, iteration 51: = 0.242814\n",
      "cross entropy loss, iteration 52: = 0.242674\n",
      "cross entropy loss, iteration 53: = 0.242535\n",
      "cross entropy loss, iteration 54: = 0.242397\n",
      "cross entropy loss, iteration 55: = 0.242255\n",
      "cross entropy loss, iteration 56: = 0.242113\n",
      "cross entropy loss, iteration 57: = 0.241971\n",
      "cross entropy loss, iteration 58: = 0.241824\n",
      "cross entropy loss, iteration 59: = 0.241674\n",
      "cross entropy loss, iteration 60: = 0.241524\n",
      "cross entropy loss, iteration 61: = 0.241374\n",
      "cross entropy loss, iteration 62: = 0.241226\n",
      "cross entropy loss, iteration 63: = 0.241072\n",
      "cross entropy loss, iteration 64: = 0.240915\n",
      "cross entropy loss, iteration 65: = 0.240756\n",
      "cross entropy loss, iteration 66: = 0.240598\n",
      "cross entropy loss, iteration 67: = 0.240441\n",
      "cross entropy loss, iteration 68: = 0.240274\n",
      "cross entropy loss, iteration 69: = 0.240114\n",
      "cross entropy loss, iteration 70: = 0.239950\n",
      "cross entropy loss, iteration 71: = 0.239778\n",
      "cross entropy loss, iteration 72: = 0.239614\n",
      "cross entropy loss, iteration 73: = 0.239445\n",
      "cross entropy loss, iteration 74: = 0.239275\n",
      "cross entropy loss, iteration 75: = 0.239101\n",
      "cross entropy loss, iteration 76: = 0.238929\n",
      "cross entropy loss, iteration 77: = 0.238748\n",
      "cross entropy loss, iteration 78: = 0.238574\n",
      "cross entropy loss, iteration 79: = 0.238396\n",
      "cross entropy loss, iteration 80: = 0.238211\n",
      "cross entropy loss, iteration 81: = 0.238030\n",
      "cross entropy loss, iteration 82: = 0.237850\n",
      "cross entropy loss, iteration 83: = 0.237659\n",
      "cross entropy loss, iteration 84: = 0.237472\n",
      "cross entropy loss, iteration 85: = 0.237281\n",
      "cross entropy loss, iteration 86: = 0.237091\n",
      "cross entropy loss, iteration 87: = 0.236894\n",
      "cross entropy loss, iteration 88: = 0.236700\n",
      "cross entropy loss, iteration 89: = 0.236502\n",
      "cross entropy loss, iteration 90: = 0.236305\n",
      "cross entropy loss, iteration 91: = 0.236099\n",
      "cross entropy loss, iteration 92: = 0.235897\n",
      "cross entropy loss, iteration 93: = 0.235691\n",
      "cross entropy loss, iteration 94: = 0.235487\n",
      "cross entropy loss, iteration 95: = 0.235274\n",
      "cross entropy loss, iteration 96: = 0.235060\n",
      "cross entropy loss, iteration 97: = 0.234850\n",
      "cross entropy loss, iteration 98: = 0.234635\n",
      "cross entropy loss, iteration 99: = 0.234420\n",
      "cross entropy loss, iteration 100: = 0.234198\n",
      "cross entropy loss, iteration 101: = 0.233974\n",
      "cross entropy loss, iteration 102: = 0.233753\n",
      "cross entropy loss, iteration 103: = 0.233528\n",
      "cross entropy loss, iteration 104: = 0.233299\n",
      "cross entropy loss, iteration 105: = 0.233072\n",
      "cross entropy loss, iteration 106: = 0.232839\n",
      "cross entropy loss, iteration 107: = 0.232602\n",
      "cross entropy loss, iteration 108: = 0.232371\n",
      "cross entropy loss, iteration 109: = 0.232132\n",
      "cross entropy loss, iteration 110: = 0.231891\n",
      "cross entropy loss, iteration 111: = 0.231650\n",
      "cross entropy loss, iteration 112: = 0.231406\n",
      "cross entropy loss, iteration 113: = 0.231164\n",
      "cross entropy loss, iteration 114: = 0.230916\n",
      "cross entropy loss, iteration 115: = 0.230665\n",
      "cross entropy loss, iteration 116: = 0.230412\n",
      "cross entropy loss, iteration 117: = 0.230160\n",
      "cross entropy loss, iteration 118: = 0.229903\n",
      "cross entropy loss, iteration 119: = 0.229645\n",
      "cross entropy loss, iteration 120: = 0.229391\n",
      "cross entropy loss, iteration 121: = 0.229130\n",
      "cross entropy loss, iteration 122: = 0.228867\n",
      "cross entropy loss, iteration 123: = 0.228603\n",
      "cross entropy loss, iteration 124: = 0.228336\n",
      "cross entropy loss, iteration 125: = 0.228068\n",
      "cross entropy loss, iteration 126: = 0.227798\n",
      "cross entropy loss, iteration 127: = 0.227526\n",
      "cross entropy loss, iteration 128: = 0.227253\n",
      "cross entropy loss, iteration 129: = 0.226978\n",
      "cross entropy loss, iteration 130: = 0.226701\n",
      "cross entropy loss, iteration 131: = 0.226422\n",
      "cross entropy loss, iteration 132: = 0.226142\n",
      "cross entropy loss, iteration 133: = 0.225861\n",
      "cross entropy loss, iteration 134: = 0.225572\n",
      "cross entropy loss, iteration 135: = 0.225289\n",
      "cross entropy loss, iteration 136: = 0.225002\n",
      "cross entropy loss, iteration 137: = 0.224715\n",
      "cross entropy loss, iteration 138: = 0.224428\n",
      "cross entropy loss, iteration 139: = 0.224137\n",
      "cross entropy loss, iteration 140: = 0.223846\n",
      "cross entropy loss, iteration 141: = 0.223549\n",
      "cross entropy loss, iteration 142: = 0.223254\n",
      "cross entropy loss, iteration 143: = 0.222958\n",
      "cross entropy loss, iteration 144: = 0.222661\n",
      "cross entropy loss, iteration 145: = 0.222358\n",
      "cross entropy loss, iteration 146: = 0.222059\n",
      "cross entropy loss, iteration 147: = 0.221759\n",
      "cross entropy loss, iteration 148: = 0.221458\n",
      "cross entropy loss, iteration 149: = 0.221149\n",
      "cross entropy loss, iteration 150: = 0.220845\n",
      "cross entropy loss, iteration 151: = 0.220540\n",
      "cross entropy loss, iteration 152: = 0.220228\n",
      "cross entropy loss, iteration 153: = 0.219923\n",
      "cross entropy loss, iteration 154: = 0.219610\n",
      "cross entropy loss, iteration 155: = 0.219302\n",
      "cross entropy loss, iteration 156: = 0.218991\n",
      "cross entropy loss, iteration 157: = 0.218675\n",
      "cross entropy loss, iteration 158: = 0.218366\n",
      "cross entropy loss, iteration 159: = 0.218049\n",
      "cross entropy loss, iteration 160: = 0.217735\n",
      "cross entropy loss, iteration 161: = 0.217417\n",
      "cross entropy loss, iteration 162: = 0.217105\n",
      "cross entropy loss, iteration 163: = 0.216785\n",
      "cross entropy loss, iteration 164: = 0.216470\n",
      "cross entropy loss, iteration 165: = 0.216150\n",
      "cross entropy loss, iteration 166: = 0.215836\n",
      "cross entropy loss, iteration 167: = 0.215514\n",
      "cross entropy loss, iteration 168: = 0.215197\n",
      "cross entropy loss, iteration 169: = 0.214875\n",
      "cross entropy loss, iteration 170: = 0.214553\n",
      "cross entropy loss, iteration 171: = 0.214235\n",
      "cross entropy loss, iteration 172: = 0.213911\n",
      "cross entropy loss, iteration 173: = 0.213594\n",
      "cross entropy loss, iteration 174: = 0.213270\n",
      "cross entropy loss, iteration 175: = 0.212947\n",
      "cross entropy loss, iteration 176: = 0.212628\n",
      "cross entropy loss, iteration 177: = 0.212304\n",
      "cross entropy loss, iteration 178: = 0.211979\n",
      "cross entropy loss, iteration 179: = 0.211662\n",
      "cross entropy loss, iteration 180: = 0.211338\n",
      "cross entropy loss, iteration 181: = 0.211013\n",
      "cross entropy loss, iteration 182: = 0.210690\n",
      "cross entropy loss, iteration 183: = 0.210370\n",
      "cross entropy loss, iteration 184: = 0.210047\n",
      "cross entropy loss, iteration 185: = 0.209723\n",
      "cross entropy loss, iteration 186: = 0.209399\n",
      "cross entropy loss, iteration 187: = 0.209075\n",
      "cross entropy loss, iteration 188: = 0.208757\n",
      "cross entropy loss, iteration 189: = 0.208435\n",
      "cross entropy loss, iteration 190: = 0.208112\n",
      "cross entropy loss, iteration 191: = 0.207788\n",
      "cross entropy loss, iteration 192: = 0.207467\n",
      "cross entropy loss, iteration 193: = 0.207144\n",
      "cross entropy loss, iteration 194: = 0.206829\n",
      "cross entropy loss, iteration 195: = 0.206507\n",
      "cross entropy loss, iteration 196: = 0.206187\n",
      "cross entropy loss, iteration 197: = 0.205866\n",
      "cross entropy loss, iteration 198: = 0.205547\n",
      "cross entropy loss, iteration 199: = 0.205227\n",
      "cross entropy loss, iteration 200: = 0.204908\n",
      "cross entropy loss, iteration 201: = 0.204589\n",
      "cross entropy loss, iteration 202: = 0.204272\n",
      "cross entropy loss, iteration 203: = 0.203955\n",
      "cross entropy loss, iteration 204: = 0.203637\n",
      "cross entropy loss, iteration 205: = 0.203321\n",
      "cross entropy loss, iteration 206: = 0.203004\n",
      "cross entropy loss, iteration 207: = 0.202689\n",
      "cross entropy loss, iteration 208: = 0.202374\n",
      "cross entropy loss, iteration 209: = 0.202060\n",
      "cross entropy loss, iteration 210: = 0.201746\n",
      "cross entropy loss, iteration 211: = 0.201433\n",
      "cross entropy loss, iteration 212: = 0.201118\n",
      "cross entropy loss, iteration 213: = 0.200807\n",
      "cross entropy loss, iteration 214: = 0.200495\n",
      "cross entropy loss, iteration 215: = 0.200185\n",
      "cross entropy loss, iteration 216: = 0.199875\n",
      "cross entropy loss, iteration 217: = 0.199566\n",
      "cross entropy loss, iteration 218: = 0.199258\n",
      "cross entropy loss, iteration 219: = 0.198948\n",
      "cross entropy loss, iteration 220: = 0.198641\n",
      "cross entropy loss, iteration 221: = 0.198334\n",
      "cross entropy loss, iteration 222: = 0.198023\n",
      "cross entropy loss, iteration 223: = 0.197718\n",
      "cross entropy loss, iteration 224: = 0.197412\n",
      "cross entropy loss, iteration 225: = 0.197108\n",
      "cross entropy loss, iteration 226: = 0.196805\n",
      "cross entropy loss, iteration 227: = 0.196501\n",
      "cross entropy loss, iteration 228: = 0.196193\n",
      "cross entropy loss, iteration 229: = 0.195893\n",
      "cross entropy loss, iteration 230: = 0.195591\n",
      "cross entropy loss, iteration 231: = 0.195290\n",
      "cross entropy loss, iteration 232: = 0.194990\n",
      "cross entropy loss, iteration 233: = 0.194691\n",
      "cross entropy loss, iteration 234: = 0.194387\n",
      "cross entropy loss, iteration 235: = 0.194088\n",
      "cross entropy loss, iteration 236: = 0.193791\n",
      "cross entropy loss, iteration 237: = 0.193494\n",
      "cross entropy loss, iteration 238: = 0.193192\n",
      "cross entropy loss, iteration 239: = 0.192895\n",
      "cross entropy loss, iteration 240: = 0.192600\n",
      "cross entropy loss, iteration 241: = 0.192304\n",
      "cross entropy loss, iteration 242: = 0.192005\n",
      "cross entropy loss, iteration 243: = 0.191710\n",
      "cross entropy loss, iteration 244: = 0.191417\n",
      "cross entropy loss, iteration 245: = 0.191123\n",
      "cross entropy loss, iteration 246: = 0.190824\n",
      "cross entropy loss, iteration 247: = 0.190532\n",
      "cross entropy loss, iteration 248: = 0.190240\n",
      "cross entropy loss, iteration 249: = 0.189944\n",
      "cross entropy loss, iteration 250: = 0.189652\n",
      "cross entropy loss, iteration 251: = 0.189362\n",
      "cross entropy loss, iteration 252: = 0.189066\n",
      "cross entropy loss, iteration 253: = 0.188776\n",
      "cross entropy loss, iteration 254: = 0.188487\n",
      "cross entropy loss, iteration 255: = 0.188192\n",
      "cross entropy loss, iteration 256: = 0.187904\n",
      "cross entropy loss, iteration 257: = 0.187610\n",
      "cross entropy loss, iteration 258: = 0.187322\n",
      "cross entropy loss, iteration 259: = 0.187034\n",
      "cross entropy loss, iteration 260: = 0.186742\n",
      "cross entropy loss, iteration 261: = 0.186455\n",
      "cross entropy loss, iteration 262: = 0.186168\n",
      "cross entropy loss, iteration 263: = 0.185876\n",
      "cross entropy loss, iteration 264: = 0.185590\n",
      "cross entropy loss, iteration 265: = 0.185300\n",
      "cross entropy loss, iteration 266: = 0.185013\n",
      "cross entropy loss, iteration 267: = 0.184723\n",
      "cross entropy loss, iteration 268: = 0.184437\n",
      "cross entropy loss, iteration 269: = 0.184146\n",
      "cross entropy loss, iteration 270: = 0.183861\n",
      "cross entropy loss, iteration 271: = 0.183575\n",
      "cross entropy loss, iteration 272: = 0.183285\n",
      "cross entropy loss, iteration 273: = 0.183000\n",
      "cross entropy loss, iteration 274: = 0.182710\n",
      "cross entropy loss, iteration 275: = 0.182425\n",
      "cross entropy loss, iteration 276: = 0.182135\n",
      "cross entropy loss, iteration 277: = 0.181851\n",
      "cross entropy loss, iteration 278: = 0.181561\n",
      "cross entropy loss, iteration 279: = 0.181277\n",
      "cross entropy loss, iteration 280: = 0.180988\n",
      "cross entropy loss, iteration 281: = 0.180699\n",
      "cross entropy loss, iteration 282: = 0.180415\n",
      "cross entropy loss, iteration 283: = 0.180126\n",
      "cross entropy loss, iteration 284: = 0.179842\n",
      "cross entropy loss, iteration 285: = 0.179552\n",
      "cross entropy loss, iteration 286: = 0.179268\n",
      "cross entropy loss, iteration 287: = 0.178979\n",
      "cross entropy loss, iteration 288: = 0.178691\n",
      "cross entropy loss, iteration 289: = 0.178407\n",
      "cross entropy loss, iteration 290: = 0.178117\n",
      "cross entropy loss, iteration 291: = 0.177833\n",
      "cross entropy loss, iteration 292: = 0.177544\n",
      "cross entropy loss, iteration 293: = 0.177256\n",
      "cross entropy loss, iteration 294: = 0.176970\n",
      "cross entropy loss, iteration 295: = 0.176681\n",
      "cross entropy loss, iteration 296: = 0.176392\n",
      "cross entropy loss, iteration 297: = 0.176106\n",
      "cross entropy loss, iteration 298: = 0.175817\n",
      "cross entropy loss, iteration 299: = 0.175528\n",
      "cross entropy loss, iteration 300: = 0.175242\n",
      "cross entropy loss, iteration 301: = 0.174953\n",
      "cross entropy loss, iteration 302: = 0.174664\n",
      "cross entropy loss, iteration 303: = 0.174377\n",
      "cross entropy loss, iteration 304: = 0.174088\n",
      "cross entropy loss, iteration 305: = 0.173797\n",
      "cross entropy loss, iteration 306: = 0.173508\n",
      "cross entropy loss, iteration 307: = 0.173221\n",
      "cross entropy loss, iteration 308: = 0.172932\n",
      "cross entropy loss, iteration 309: = 0.172640\n",
      "cross entropy loss, iteration 310: = 0.172350\n",
      "cross entropy loss, iteration 311: = 0.172063\n",
      "cross entropy loss, iteration 312: = 0.171773\n",
      "cross entropy loss, iteration 313: = 0.171481\n",
      "cross entropy loss, iteration 314: = 0.171191\n",
      "cross entropy loss, iteration 315: = 0.170899\n",
      "cross entropy loss, iteration 316: = 0.170611\n",
      "cross entropy loss, iteration 317: = 0.170320\n",
      "cross entropy loss, iteration 318: = 0.170027\n",
      "cross entropy loss, iteration 319: = 0.169735\n",
      "cross entropy loss, iteration 320: = 0.169443\n",
      "cross entropy loss, iteration 321: = 0.169150\n",
      "cross entropy loss, iteration 322: = 0.168862\n",
      "cross entropy loss, iteration 323: = 0.168570\n",
      "cross entropy loss, iteration 324: = 0.168277\n",
      "cross entropy loss, iteration 325: = 0.167983\n",
      "cross entropy loss, iteration 326: = 0.167689\n",
      "cross entropy loss, iteration 327: = 0.167395\n",
      "cross entropy loss, iteration 328: = 0.167102\n",
      "cross entropy loss, iteration 329: = 0.166807\n",
      "cross entropy loss, iteration 330: = 0.166513\n",
      "cross entropy loss, iteration 331: = 0.166218\n",
      "cross entropy loss, iteration 332: = 0.165928\n",
      "cross entropy loss, iteration 333: = 0.165633\n",
      "cross entropy loss, iteration 334: = 0.165338\n",
      "cross entropy loss, iteration 335: = 0.165043\n",
      "cross entropy loss, iteration 336: = 0.164748\n",
      "cross entropy loss, iteration 337: = 0.164452\n",
      "cross entropy loss, iteration 338: = 0.164156\n",
      "cross entropy loss, iteration 339: = 0.163860\n",
      "cross entropy loss, iteration 340: = 0.163564\n",
      "cross entropy loss, iteration 341: = 0.163267\n",
      "cross entropy loss, iteration 342: = 0.162970\n",
      "cross entropy loss, iteration 343: = 0.162673\n",
      "cross entropy loss, iteration 344: = 0.162376\n",
      "cross entropy loss, iteration 345: = 0.162074\n",
      "cross entropy loss, iteration 346: = 0.161776\n",
      "cross entropy loss, iteration 347: = 0.161478\n",
      "cross entropy loss, iteration 348: = 0.161178\n",
      "cross entropy loss, iteration 349: = 0.160880\n",
      "cross entropy loss, iteration 350: = 0.160581\n",
      "cross entropy loss, iteration 351: = 0.160283\n",
      "cross entropy loss, iteration 352: = 0.159984\n",
      "cross entropy loss, iteration 353: = 0.159683\n",
      "cross entropy loss, iteration 354: = 0.159384\n",
      "cross entropy loss, iteration 355: = 0.159080\n",
      "cross entropy loss, iteration 356: = 0.158779\n",
      "cross entropy loss, iteration 357: = 0.158478\n",
      "cross entropy loss, iteration 358: = 0.158179\n",
      "cross entropy loss, iteration 359: = 0.157877\n",
      "cross entropy loss, iteration 360: = 0.157576\n",
      "cross entropy loss, iteration 361: = 0.157271\n",
      "cross entropy loss, iteration 362: = 0.156969\n",
      "cross entropy loss, iteration 363: = 0.156668\n",
      "cross entropy loss, iteration 364: = 0.156365\n",
      "cross entropy loss, iteration 365: = 0.156060\n",
      "cross entropy loss, iteration 366: = 0.155757\n",
      "cross entropy loss, iteration 367: = 0.155456\n",
      "cross entropy loss, iteration 368: = 0.155153\n",
      "cross entropy loss, iteration 369: = 0.154846\n",
      "cross entropy loss, iteration 370: = 0.154543\n",
      "cross entropy loss, iteration 371: = 0.154242\n",
      "cross entropy loss, iteration 372: = 0.153934\n",
      "cross entropy loss, iteration 373: = 0.153630\n",
      "cross entropy loss, iteration 374: = 0.153328\n",
      "cross entropy loss, iteration 375: = 0.153025\n",
      "cross entropy loss, iteration 376: = 0.152716\n",
      "cross entropy loss, iteration 377: = 0.152414\n",
      "cross entropy loss, iteration 378: = 0.152105\n",
      "cross entropy loss, iteration 379: = 0.151801\n",
      "cross entropy loss, iteration 380: = 0.151497\n",
      "cross entropy loss, iteration 381: = 0.151189\n",
      "cross entropy loss, iteration 382: = 0.150885\n",
      "cross entropy loss, iteration 383: = 0.150580\n",
      "cross entropy loss, iteration 384: = 0.150271\n",
      "cross entropy loss, iteration 385: = 0.149967\n",
      "cross entropy loss, iteration 386: = 0.149659\n",
      "cross entropy loss, iteration 387: = 0.149354\n",
      "cross entropy loss, iteration 388: = 0.149046\n",
      "cross entropy loss, iteration 389: = 0.148741\n",
      "cross entropy loss, iteration 390: = 0.148432\n",
      "cross entropy loss, iteration 391: = 0.148127\n",
      "cross entropy loss, iteration 392: = 0.147821\n",
      "cross entropy loss, iteration 393: = 0.147512\n",
      "cross entropy loss, iteration 394: = 0.147207\n",
      "cross entropy loss, iteration 395: = 0.146897\n",
      "cross entropy loss, iteration 396: = 0.146592\n",
      "cross entropy loss, iteration 397: = 0.146283\n",
      "cross entropy loss, iteration 398: = 0.145974\n",
      "cross entropy loss, iteration 399: = 0.145669\n",
      "cross entropy loss, iteration 400: = 0.145359\n",
      "cross entropy loss, iteration 401: = 0.145054\n",
      "cross entropy loss, iteration 402: = 0.144743\n",
      "cross entropy loss, iteration 403: = 0.144437\n",
      "cross entropy loss, iteration 404: = 0.144127\n",
      "cross entropy loss, iteration 405: = 0.143818\n",
      "cross entropy loss, iteration 406: = 0.143512\n",
      "cross entropy loss, iteration 407: = 0.143201\n",
      "cross entropy loss, iteration 408: = 0.142895\n",
      "cross entropy loss, iteration 409: = 0.142587\n",
      "cross entropy loss, iteration 410: = 0.142277\n",
      "cross entropy loss, iteration 411: = 0.141970\n",
      "cross entropy loss, iteration 412: = 0.141662\n",
      "cross entropy loss, iteration 413: = 0.141353\n",
      "cross entropy loss, iteration 414: = 0.141045\n",
      "cross entropy loss, iteration 415: = 0.140737\n",
      "cross entropy loss, iteration 416: = 0.140426\n",
      "cross entropy loss, iteration 417: = 0.140118\n",
      "cross entropy loss, iteration 418: = 0.139813\n",
      "cross entropy loss, iteration 419: = 0.139503\n",
      "cross entropy loss, iteration 420: = 0.139196\n",
      "cross entropy loss, iteration 421: = 0.138889\n",
      "cross entropy loss, iteration 422: = 0.138582\n",
      "cross entropy loss, iteration 423: = 0.138273\n",
      "cross entropy loss, iteration 424: = 0.137966\n",
      "cross entropy loss, iteration 425: = 0.137660\n",
      "cross entropy loss, iteration 426: = 0.137352\n",
      "cross entropy loss, iteration 427: = 0.137046\n",
      "cross entropy loss, iteration 428: = 0.136737\n",
      "cross entropy loss, iteration 429: = 0.136431\n",
      "cross entropy loss, iteration 430: = 0.136124\n",
      "cross entropy loss, iteration 431: = 0.135820\n",
      "cross entropy loss, iteration 432: = 0.135514\n",
      "cross entropy loss, iteration 433: = 0.135207\n",
      "cross entropy loss, iteration 434: = 0.134900\n",
      "cross entropy loss, iteration 435: = 0.134596\n",
      "cross entropy loss, iteration 436: = 0.134290\n",
      "cross entropy loss, iteration 437: = 0.133987\n",
      "cross entropy loss, iteration 438: = 0.133681\n",
      "cross entropy loss, iteration 439: = 0.133376\n",
      "cross entropy loss, iteration 440: = 0.133072\n",
      "cross entropy loss, iteration 441: = 0.132767\n",
      "cross entropy loss, iteration 442: = 0.132463\n",
      "cross entropy loss, iteration 443: = 0.132158\n",
      "cross entropy loss, iteration 444: = 0.131854\n",
      "cross entropy loss, iteration 445: = 0.131550\n",
      "cross entropy loss, iteration 446: = 0.131247\n",
      "cross entropy loss, iteration 447: = 0.130946\n",
      "cross entropy loss, iteration 448: = 0.130643\n",
      "cross entropy loss, iteration 449: = 0.130341\n",
      "cross entropy loss, iteration 450: = 0.130039\n",
      "cross entropy loss, iteration 451: = 0.129737\n",
      "cross entropy loss, iteration 452: = 0.129436\n",
      "cross entropy loss, iteration 453: = 0.129135\n",
      "cross entropy loss, iteration 454: = 0.128834\n",
      "cross entropy loss, iteration 455: = 0.128534\n",
      "cross entropy loss, iteration 456: = 0.128234\n",
      "cross entropy loss, iteration 457: = 0.127932\n",
      "cross entropy loss, iteration 458: = 0.127633\n",
      "cross entropy loss, iteration 459: = 0.127333\n",
      "cross entropy loss, iteration 460: = 0.127035\n",
      "cross entropy loss, iteration 461: = 0.126737\n",
      "cross entropy loss, iteration 462: = 0.126439\n",
      "cross entropy loss, iteration 463: = 0.126141\n",
      "cross entropy loss, iteration 464: = 0.125841\n",
      "cross entropy loss, iteration 465: = 0.125545\n",
      "cross entropy loss, iteration 466: = 0.125247\n",
      "cross entropy loss, iteration 467: = 0.124949\n",
      "cross entropy loss, iteration 468: = 0.124652\n",
      "cross entropy loss, iteration 469: = 0.124355\n",
      "cross entropy loss, iteration 470: = 0.124059\n",
      "cross entropy loss, iteration 471: = 0.123764\n",
      "cross entropy loss, iteration 472: = 0.123468\n",
      "cross entropy loss, iteration 473: = 0.123174\n",
      "cross entropy loss, iteration 474: = 0.122878\n",
      "cross entropy loss, iteration 475: = 0.122582\n",
      "cross entropy loss, iteration 476: = 0.122289\n",
      "cross entropy loss, iteration 477: = 0.121996\n",
      "cross entropy loss, iteration 478: = 0.121702\n",
      "cross entropy loss, iteration 479: = 0.121408\n",
      "cross entropy loss, iteration 480: = 0.121115\n",
      "cross entropy loss, iteration 481: = 0.120823\n",
      "cross entropy loss, iteration 482: = 0.120533\n",
      "cross entropy loss, iteration 483: = 0.120241\n",
      "cross entropy loss, iteration 484: = 0.119949\n",
      "cross entropy loss, iteration 485: = 0.119657\n",
      "cross entropy loss, iteration 486: = 0.119369\n",
      "cross entropy loss, iteration 487: = 0.119078\n",
      "cross entropy loss, iteration 488: = 0.118788\n",
      "cross entropy loss, iteration 489: = 0.118498\n",
      "cross entropy loss, iteration 490: = 0.118211\n",
      "cross entropy loss, iteration 491: = 0.117920\n",
      "cross entropy loss, iteration 492: = 0.117632\n",
      "cross entropy loss, iteration 493: = 0.117346\n",
      "cross entropy loss, iteration 494: = 0.117058\n",
      "cross entropy loss, iteration 495: = 0.116771\n",
      "cross entropy loss, iteration 496: = 0.116484\n",
      "cross entropy loss, iteration 497: = 0.116198\n",
      "cross entropy loss, iteration 498: = 0.115911\n",
      "cross entropy loss, iteration 499: = 0.115625\n",
      "cross entropy loss, iteration 500: = 0.115339\n",
      "cross entropy loss, iteration 501: = 0.115054\n",
      "cross entropy loss, iteration 502: = 0.114770\n",
      "cross entropy loss, iteration 503: = 0.114485\n",
      "cross entropy loss, iteration 504: = 0.114199\n",
      "cross entropy loss, iteration 505: = 0.113917\n",
      "cross entropy loss, iteration 506: = 0.113633\n",
      "cross entropy loss, iteration 507: = 0.113349\n",
      "cross entropy loss, iteration 508: = 0.113067\n",
      "cross entropy loss, iteration 509: = 0.112783\n",
      "cross entropy loss, iteration 510: = 0.112503\n",
      "cross entropy loss, iteration 511: = 0.112220\n",
      "cross entropy loss, iteration 512: = 0.111939\n",
      "cross entropy loss, iteration 513: = 0.111658\n",
      "cross entropy loss, iteration 514: = 0.111375\n",
      "cross entropy loss, iteration 515: = 0.111096\n",
      "cross entropy loss, iteration 516: = 0.110814\n",
      "cross entropy loss, iteration 517: = 0.110536\n",
      "cross entropy loss, iteration 518: = 0.110255\n",
      "cross entropy loss, iteration 519: = 0.109977\n",
      "cross entropy loss, iteration 520: = 0.109697\n",
      "cross entropy loss, iteration 521: = 0.109419\n",
      "cross entropy loss, iteration 522: = 0.109140\n",
      "cross entropy loss, iteration 523: = 0.108863\n",
      "cross entropy loss, iteration 524: = 0.108584\n",
      "cross entropy loss, iteration 525: = 0.108308\n",
      "cross entropy loss, iteration 526: = 0.108030\n",
      "cross entropy loss, iteration 527: = 0.107754\n",
      "cross entropy loss, iteration 528: = 0.107477\n",
      "cross entropy loss, iteration 529: = 0.107202\n",
      "cross entropy loss, iteration 530: = 0.106925\n",
      "cross entropy loss, iteration 531: = 0.106649\n",
      "cross entropy loss, iteration 532: = 0.106373\n",
      "cross entropy loss, iteration 533: = 0.106100\n",
      "cross entropy loss, iteration 534: = 0.105824\n",
      "cross entropy loss, iteration 535: = 0.105551\n",
      "cross entropy loss, iteration 536: = 0.105277\n",
      "cross entropy loss, iteration 537: = 0.105003\n",
      "cross entropy loss, iteration 538: = 0.104731\n",
      "cross entropy loss, iteration 539: = 0.104457\n",
      "cross entropy loss, iteration 540: = 0.104185\n",
      "cross entropy loss, iteration 541: = 0.103913\n",
      "cross entropy loss, iteration 542: = 0.103641\n",
      "cross entropy loss, iteration 543: = 0.103371\n",
      "cross entropy loss, iteration 544: = 0.103098\n",
      "cross entropy loss, iteration 545: = 0.102828\n",
      "cross entropy loss, iteration 546: = 0.102558\n",
      "cross entropy loss, iteration 547: = 0.102287\n",
      "cross entropy loss, iteration 548: = 0.102017\n",
      "cross entropy loss, iteration 549: = 0.101747\n",
      "cross entropy loss, iteration 550: = 0.101477\n",
      "cross entropy loss, iteration 551: = 0.101208\n",
      "cross entropy loss, iteration 552: = 0.100939\n",
      "cross entropy loss, iteration 553: = 0.100670\n",
      "cross entropy loss, iteration 554: = 0.100402\n",
      "cross entropy loss, iteration 555: = 0.100134\n",
      "cross entropy loss, iteration 556: = 0.099867\n",
      "cross entropy loss, iteration 557: = 0.099599\n",
      "cross entropy loss, iteration 558: = 0.099333\n",
      "cross entropy loss, iteration 559: = 0.099064\n",
      "cross entropy loss, iteration 560: = 0.098798\n",
      "cross entropy loss, iteration 561: = 0.098533\n",
      "cross entropy loss, iteration 562: = 0.098266\n",
      "cross entropy loss, iteration 563: = 0.098001\n",
      "cross entropy loss, iteration 564: = 0.097735\n",
      "cross entropy loss, iteration 565: = 0.097471\n",
      "cross entropy loss, iteration 566: = 0.097204\n",
      "cross entropy loss, iteration 567: = 0.096941\n",
      "cross entropy loss, iteration 568: = 0.096676\n",
      "cross entropy loss, iteration 569: = 0.096413\n",
      "cross entropy loss, iteration 570: = 0.096148\n",
      "cross entropy loss, iteration 571: = 0.095885\n",
      "cross entropy loss, iteration 572: = 0.095621\n",
      "cross entropy loss, iteration 573: = 0.095358\n",
      "cross entropy loss, iteration 574: = 0.095096\n",
      "cross entropy loss, iteration 575: = 0.094832\n",
      "cross entropy loss, iteration 576: = 0.094571\n",
      "cross entropy loss, iteration 577: = 0.094309\n",
      "cross entropy loss, iteration 578: = 0.094047\n",
      "cross entropy loss, iteration 579: = 0.093786\n",
      "cross entropy loss, iteration 580: = 0.093524\n",
      "cross entropy loss, iteration 581: = 0.093262\n",
      "cross entropy loss, iteration 582: = 0.093003\n",
      "cross entropy loss, iteration 583: = 0.092742\n",
      "cross entropy loss, iteration 584: = 0.092482\n",
      "cross entropy loss, iteration 585: = 0.092222\n",
      "cross entropy loss, iteration 586: = 0.091962\n",
      "cross entropy loss, iteration 587: = 0.091702\n",
      "cross entropy loss, iteration 588: = 0.091444\n",
      "cross entropy loss, iteration 589: = 0.091185\n",
      "cross entropy loss, iteration 590: = 0.090926\n",
      "cross entropy loss, iteration 591: = 0.090668\n",
      "cross entropy loss, iteration 592: = 0.090410\n",
      "cross entropy loss, iteration 593: = 0.090153\n",
      "cross entropy loss, iteration 594: = 0.089895\n",
      "cross entropy loss, iteration 595: = 0.089637\n",
      "cross entropy loss, iteration 596: = 0.089379\n",
      "cross entropy loss, iteration 597: = 0.089123\n",
      "cross entropy loss, iteration 598: = 0.088867\n",
      "cross entropy loss, iteration 599: = 0.088611\n",
      "cross entropy loss, iteration 600: = 0.088355\n",
      "cross entropy loss, iteration 601: = 0.088099\n",
      "cross entropy loss, iteration 602: = 0.087844\n",
      "cross entropy loss, iteration 603: = 0.087588\n",
      "cross entropy loss, iteration 604: = 0.087333\n",
      "cross entropy loss, iteration 605: = 0.087079\n",
      "cross entropy loss, iteration 606: = 0.086824\n",
      "cross entropy loss, iteration 607: = 0.086570\n",
      "cross entropy loss, iteration 608: = 0.086315\n",
      "cross entropy loss, iteration 609: = 0.086061\n",
      "cross entropy loss, iteration 610: = 0.085808\n",
      "cross entropy loss, iteration 611: = 0.085554\n",
      "cross entropy loss, iteration 612: = 0.085301\n",
      "cross entropy loss, iteration 613: = 0.085048\n",
      "cross entropy loss, iteration 614: = 0.084796\n",
      "cross entropy loss, iteration 615: = 0.084544\n",
      "cross entropy loss, iteration 616: = 0.084291\n",
      "cross entropy loss, iteration 617: = 0.084040\n",
      "cross entropy loss, iteration 618: = 0.083788\n",
      "cross entropy loss, iteration 619: = 0.083537\n",
      "cross entropy loss, iteration 620: = 0.083286\n",
      "cross entropy loss, iteration 621: = 0.083035\n",
      "cross entropy loss, iteration 622: = 0.082785\n",
      "cross entropy loss, iteration 623: = 0.082535\n",
      "cross entropy loss, iteration 624: = 0.082285\n",
      "cross entropy loss, iteration 625: = 0.082034\n",
      "cross entropy loss, iteration 626: = 0.081784\n",
      "cross entropy loss, iteration 627: = 0.081535\n",
      "cross entropy loss, iteration 628: = 0.081287\n",
      "cross entropy loss, iteration 629: = 0.081038\n",
      "cross entropy loss, iteration 630: = 0.080789\n",
      "cross entropy loss, iteration 631: = 0.080540\n",
      "cross entropy loss, iteration 632: = 0.080292\n",
      "cross entropy loss, iteration 633: = 0.080044\n",
      "cross entropy loss, iteration 634: = 0.079796\n",
      "cross entropy loss, iteration 635: = 0.079548\n",
      "cross entropy loss, iteration 636: = 0.079301\n",
      "cross entropy loss, iteration 637: = 0.079055\n",
      "cross entropy loss, iteration 638: = 0.078808\n",
      "cross entropy loss, iteration 639: = 0.078561\n",
      "cross entropy loss, iteration 640: = 0.078314\n",
      "cross entropy loss, iteration 641: = 0.078068\n",
      "cross entropy loss, iteration 642: = 0.077821\n",
      "cross entropy loss, iteration 643: = 0.077576\n",
      "cross entropy loss, iteration 644: = 0.077331\n",
      "cross entropy loss, iteration 645: = 0.077084\n",
      "cross entropy loss, iteration 646: = 0.076839\n",
      "cross entropy loss, iteration 647: = 0.076594\n",
      "cross entropy loss, iteration 648: = 0.076349\n",
      "cross entropy loss, iteration 649: = 0.076105\n",
      "cross entropy loss, iteration 650: = 0.075861\n",
      "cross entropy loss, iteration 651: = 0.075616\n",
      "cross entropy loss, iteration 652: = 0.075373\n",
      "cross entropy loss, iteration 653: = 0.075129\n",
      "cross entropy loss, iteration 654: = 0.074886\n",
      "cross entropy loss, iteration 655: = 0.074642\n",
      "cross entropy loss, iteration 656: = 0.074399\n",
      "cross entropy loss, iteration 657: = 0.074155\n",
      "cross entropy loss, iteration 658: = 0.073913\n",
      "cross entropy loss, iteration 659: = 0.073670\n",
      "cross entropy loss, iteration 660: = 0.073429\n",
      "cross entropy loss, iteration 661: = 0.073186\n",
      "cross entropy loss, iteration 662: = 0.072945\n",
      "cross entropy loss, iteration 663: = 0.072703\n",
      "cross entropy loss, iteration 664: = 0.072462\n",
      "cross entropy loss, iteration 665: = 0.072220\n",
      "cross entropy loss, iteration 666: = 0.071980\n",
      "cross entropy loss, iteration 667: = 0.071739\n",
      "cross entropy loss, iteration 668: = 0.071498\n",
      "cross entropy loss, iteration 669: = 0.071259\n",
      "cross entropy loss, iteration 670: = 0.071018\n",
      "cross entropy loss, iteration 671: = 0.070778\n",
      "cross entropy loss, iteration 672: = 0.070538\n",
      "cross entropy loss, iteration 673: = 0.070299\n",
      "cross entropy loss, iteration 674: = 0.070060\n",
      "cross entropy loss, iteration 675: = 0.069820\n",
      "cross entropy loss, iteration 676: = 0.069582\n",
      "cross entropy loss, iteration 677: = 0.069343\n",
      "cross entropy loss, iteration 678: = 0.069105\n",
      "cross entropy loss, iteration 679: = 0.068866\n",
      "cross entropy loss, iteration 680: = 0.068629\n",
      "cross entropy loss, iteration 681: = 0.068391\n",
      "cross entropy loss, iteration 682: = 0.068153\n",
      "cross entropy loss, iteration 683: = 0.067915\n",
      "cross entropy loss, iteration 684: = 0.067679\n",
      "cross entropy loss, iteration 685: = 0.067442\n",
      "cross entropy loss, iteration 686: = 0.067205\n",
      "cross entropy loss, iteration 687: = 0.066969\n",
      "cross entropy loss, iteration 688: = 0.066733\n",
      "cross entropy loss, iteration 689: = 0.066497\n",
      "cross entropy loss, iteration 690: = 0.066261\n",
      "cross entropy loss, iteration 691: = 0.066025\n",
      "cross entropy loss, iteration 692: = 0.065790\n",
      "cross entropy loss, iteration 693: = 0.065554\n",
      "cross entropy loss, iteration 694: = 0.065320\n",
      "cross entropy loss, iteration 695: = 0.065085\n",
      "cross entropy loss, iteration 696: = 0.064850\n",
      "cross entropy loss, iteration 697: = 0.064616\n",
      "cross entropy loss, iteration 698: = 0.064381\n",
      "cross entropy loss, iteration 699: = 0.064147\n",
      "cross entropy loss, iteration 700: = 0.063912\n",
      "cross entropy loss, iteration 701: = 0.063679\n",
      "cross entropy loss, iteration 702: = 0.063445\n",
      "cross entropy loss, iteration 703: = 0.063211\n",
      "cross entropy loss, iteration 704: = 0.062978\n",
      "cross entropy loss, iteration 705: = 0.062746\n",
      "cross entropy loss, iteration 706: = 0.062513\n",
      "cross entropy loss, iteration 707: = 0.062280\n",
      "cross entropy loss, iteration 708: = 0.062047\n",
      "cross entropy loss, iteration 709: = 0.061815\n",
      "cross entropy loss, iteration 710: = 0.061583\n",
      "cross entropy loss, iteration 711: = 0.061351\n",
      "cross entropy loss, iteration 712: = 0.061118\n",
      "cross entropy loss, iteration 713: = 0.060886\n",
      "cross entropy loss, iteration 714: = 0.060655\n",
      "cross entropy loss, iteration 715: = 0.060424\n",
      "cross entropy loss, iteration 716: = 0.060193\n",
      "cross entropy loss, iteration 717: = 0.059962\n",
      "cross entropy loss, iteration 718: = 0.059731\n",
      "cross entropy loss, iteration 719: = 0.059501\n",
      "cross entropy loss, iteration 720: = 0.059271\n",
      "cross entropy loss, iteration 721: = 0.059041\n",
      "cross entropy loss, iteration 722: = 0.058811\n",
      "cross entropy loss, iteration 723: = 0.058580\n",
      "cross entropy loss, iteration 724: = 0.058351\n",
      "cross entropy loss, iteration 725: = 0.058121\n",
      "cross entropy loss, iteration 726: = 0.057893\n",
      "cross entropy loss, iteration 727: = 0.057664\n",
      "cross entropy loss, iteration 728: = 0.057435\n",
      "cross entropy loss, iteration 729: = 0.057205\n",
      "cross entropy loss, iteration 730: = 0.056977\n",
      "cross entropy loss, iteration 731: = 0.056749\n",
      "cross entropy loss, iteration 732: = 0.056521\n",
      "cross entropy loss, iteration 733: = 0.056292\n",
      "cross entropy loss, iteration 734: = 0.056064\n",
      "cross entropy loss, iteration 735: = 0.055837\n",
      "cross entropy loss, iteration 736: = 0.055610\n",
      "cross entropy loss, iteration 737: = 0.055382\n",
      "cross entropy loss, iteration 738: = 0.055155\n",
      "cross entropy loss, iteration 739: = 0.054929\n",
      "cross entropy loss, iteration 740: = 0.054701\n",
      "cross entropy loss, iteration 741: = 0.054475\n",
      "cross entropy loss, iteration 742: = 0.054249\n",
      "cross entropy loss, iteration 743: = 0.054022\n",
      "cross entropy loss, iteration 744: = 0.053795\n",
      "cross entropy loss, iteration 745: = 0.053570\n",
      "cross entropy loss, iteration 746: = 0.053344\n",
      "cross entropy loss, iteration 747: = 0.053118\n",
      "cross entropy loss, iteration 748: = 0.052893\n",
      "cross entropy loss, iteration 749: = 0.052667\n",
      "cross entropy loss, iteration 750: = 0.052443\n",
      "cross entropy loss, iteration 751: = 0.052217\n",
      "cross entropy loss, iteration 752: = 0.051992\n",
      "cross entropy loss, iteration 753: = 0.051767\n",
      "cross entropy loss, iteration 754: = 0.051543\n",
      "cross entropy loss, iteration 755: = 0.051319\n",
      "cross entropy loss, iteration 756: = 0.051094\n",
      "cross entropy loss, iteration 757: = 0.050870\n",
      "cross entropy loss, iteration 758: = 0.050646\n",
      "cross entropy loss, iteration 759: = 0.050423\n",
      "cross entropy loss, iteration 760: = 0.050198\n",
      "cross entropy loss, iteration 761: = 0.049975\n",
      "cross entropy loss, iteration 762: = 0.049752\n",
      "cross entropy loss, iteration 763: = 0.049529\n",
      "cross entropy loss, iteration 764: = 0.049305\n",
      "cross entropy loss, iteration 765: = 0.049082\n",
      "cross entropy loss, iteration 766: = 0.048860\n",
      "cross entropy loss, iteration 767: = 0.048637\n",
      "cross entropy loss, iteration 768: = 0.048414\n",
      "cross entropy loss, iteration 769: = 0.048192\n",
      "cross entropy loss, iteration 770: = 0.047969\n",
      "cross entropy loss, iteration 771: = 0.047748\n",
      "cross entropy loss, iteration 772: = 0.047526\n",
      "cross entropy loss, iteration 773: = 0.047304\n",
      "cross entropy loss, iteration 774: = 0.047083\n",
      "cross entropy loss, iteration 775: = 0.046861\n",
      "cross entropy loss, iteration 776: = 0.046640\n",
      "cross entropy loss, iteration 777: = 0.046419\n",
      "cross entropy loss, iteration 778: = 0.046198\n",
      "cross entropy loss, iteration 779: = 0.045977\n",
      "cross entropy loss, iteration 780: = 0.045757\n",
      "cross entropy loss, iteration 781: = 0.045536\n",
      "cross entropy loss, iteration 782: = 0.045315\n",
      "cross entropy loss, iteration 783: = 0.045095\n",
      "cross entropy loss, iteration 784: = 0.044875\n",
      "cross entropy loss, iteration 785: = 0.044654\n",
      "cross entropy loss, iteration 786: = 0.044435\n",
      "cross entropy loss, iteration 787: = 0.044215\n",
      "cross entropy loss, iteration 788: = 0.043996\n",
      "cross entropy loss, iteration 789: = 0.043776\n",
      "cross entropy loss, iteration 790: = 0.043556\n",
      "cross entropy loss, iteration 791: = 0.043337\n",
      "cross entropy loss, iteration 792: = 0.043117\n",
      "cross entropy loss, iteration 793: = 0.042898\n",
      "cross entropy loss, iteration 794: = 0.042679\n",
      "cross entropy loss, iteration 795: = 0.042460\n",
      "cross entropy loss, iteration 796: = 0.042241\n",
      "cross entropy loss, iteration 797: = 0.042023\n",
      "cross entropy loss, iteration 798: = 0.041804\n",
      "cross entropy loss, iteration 799: = 0.041586\n",
      "cross entropy loss, iteration 800: = 0.041368\n",
      "cross entropy loss, iteration 801: = 0.041150\n",
      "cross entropy loss, iteration 802: = 0.040932\n",
      "cross entropy loss, iteration 803: = 0.040714\n",
      "cross entropy loss, iteration 804: = 0.040496\n",
      "cross entropy loss, iteration 805: = 0.040279\n",
      "cross entropy loss, iteration 806: = 0.040062\n",
      "cross entropy loss, iteration 807: = 0.039844\n",
      "cross entropy loss, iteration 808: = 0.039627\n",
      "cross entropy loss, iteration 809: = 0.039410\n",
      "cross entropy loss, iteration 810: = 0.039193\n",
      "cross entropy loss, iteration 811: = 0.038976\n",
      "cross entropy loss, iteration 812: = 0.038759\n",
      "cross entropy loss, iteration 813: = 0.038542\n",
      "cross entropy loss, iteration 814: = 0.038325\n",
      "cross entropy loss, iteration 815: = 0.038109\n",
      "cross entropy loss, iteration 816: = 0.037893\n",
      "cross entropy loss, iteration 817: = 0.037676\n",
      "cross entropy loss, iteration 818: = 0.037460\n",
      "cross entropy loss, iteration 819: = 0.037244\n",
      "cross entropy loss, iteration 820: = 0.037028\n",
      "cross entropy loss, iteration 821: = 0.036812\n",
      "cross entropy loss, iteration 822: = 0.036597\n",
      "cross entropy loss, iteration 823: = 0.036380\n",
      "cross entropy loss, iteration 824: = 0.036165\n",
      "cross entropy loss, iteration 825: = 0.035950\n",
      "cross entropy loss, iteration 826: = 0.035734\n",
      "cross entropy loss, iteration 827: = 0.035519\n",
      "cross entropy loss, iteration 828: = 0.035304\n",
      "cross entropy loss, iteration 829: = 0.035089\n",
      "cross entropy loss, iteration 830: = 0.034874\n",
      "cross entropy loss, iteration 831: = 0.034659\n",
      "cross entropy loss, iteration 832: = 0.034445\n",
      "cross entropy loss, iteration 833: = 0.034230\n",
      "cross entropy loss, iteration 834: = 0.034015\n",
      "cross entropy loss, iteration 835: = 0.033800\n",
      "cross entropy loss, iteration 836: = 0.033587\n",
      "cross entropy loss, iteration 837: = 0.033372\n",
      "cross entropy loss, iteration 838: = 0.033158\n",
      "cross entropy loss, iteration 839: = 0.032944\n",
      "cross entropy loss, iteration 840: = 0.032730\n",
      "cross entropy loss, iteration 841: = 0.032516\n",
      "cross entropy loss, iteration 842: = 0.032302\n",
      "cross entropy loss, iteration 843: = 0.032088\n",
      "cross entropy loss, iteration 844: = 0.031875\n",
      "cross entropy loss, iteration 845: = 0.031662\n",
      "cross entropy loss, iteration 846: = 0.031448\n",
      "cross entropy loss, iteration 847: = 0.031234\n",
      "cross entropy loss, iteration 848: = 0.031022\n",
      "cross entropy loss, iteration 849: = 0.030808\n",
      "cross entropy loss, iteration 850: = 0.030595\n",
      "cross entropy loss, iteration 851: = 0.030382\n",
      "cross entropy loss, iteration 852: = 0.030169\n",
      "cross entropy loss, iteration 853: = 0.029957\n",
      "cross entropy loss, iteration 854: = 0.029744\n",
      "cross entropy loss, iteration 855: = 0.029532\n",
      "cross entropy loss, iteration 856: = 0.029319\n",
      "cross entropy loss, iteration 857: = 0.029107\n",
      "cross entropy loss, iteration 858: = 0.028894\n",
      "cross entropy loss, iteration 859: = 0.028682\n",
      "cross entropy loss, iteration 860: = 0.028470\n",
      "cross entropy loss, iteration 861: = 0.028258\n",
      "cross entropy loss, iteration 862: = 0.028046\n",
      "cross entropy loss, iteration 863: = 0.027834\n",
      "cross entropy loss, iteration 864: = 0.027623\n",
      "cross entropy loss, iteration 865: = 0.027411\n",
      "cross entropy loss, iteration 866: = 0.027199\n",
      "cross entropy loss, iteration 867: = 0.026987\n",
      "cross entropy loss, iteration 868: = 0.026776\n",
      "cross entropy loss, iteration 869: = 0.026564\n",
      "cross entropy loss, iteration 870: = 0.026353\n",
      "cross entropy loss, iteration 871: = 0.026141\n",
      "cross entropy loss, iteration 872: = 0.025930\n",
      "cross entropy loss, iteration 873: = 0.025719\n",
      "cross entropy loss, iteration 874: = 0.025508\n",
      "cross entropy loss, iteration 875: = 0.025297\n",
      "cross entropy loss, iteration 876: = 0.025085\n",
      "cross entropy loss, iteration 877: = 0.024875\n",
      "cross entropy loss, iteration 878: = 0.024664\n",
      "cross entropy loss, iteration 879: = 0.024453\n",
      "cross entropy loss, iteration 880: = 0.024242\n",
      "cross entropy loss, iteration 881: = 0.024032\n",
      "cross entropy loss, iteration 882: = 0.023821\n",
      "cross entropy loss, iteration 883: = 0.023611\n",
      "cross entropy loss, iteration 884: = 0.023400\n",
      "cross entropy loss, iteration 885: = 0.023190\n",
      "cross entropy loss, iteration 886: = 0.022980\n",
      "cross entropy loss, iteration 887: = 0.022770\n",
      "cross entropy loss, iteration 888: = 0.022559\n",
      "cross entropy loss, iteration 889: = 0.022349\n",
      "cross entropy loss, iteration 890: = 0.022139\n",
      "cross entropy loss, iteration 891: = 0.021929\n",
      "cross entropy loss, iteration 892: = 0.021720\n",
      "cross entropy loss, iteration 893: = 0.021510\n",
      "cross entropy loss, iteration 894: = 0.021300\n",
      "cross entropy loss, iteration 895: = 0.021091\n",
      "cross entropy loss, iteration 896: = 0.020881\n",
      "cross entropy loss, iteration 897: = 0.020672\n",
      "cross entropy loss, iteration 898: = 0.020462\n",
      "cross entropy loss, iteration 899: = 0.020253\n",
      "cross entropy loss, iteration 900: = 0.020044\n",
      "cross entropy loss, iteration 901: = 0.019835\n",
      "cross entropy loss, iteration 902: = 0.019626\n",
      "cross entropy loss, iteration 903: = 0.019417\n",
      "cross entropy loss, iteration 904: = 0.019208\n",
      "cross entropy loss, iteration 905: = 0.018999\n",
      "cross entropy loss, iteration 906: = 0.018790\n",
      "cross entropy loss, iteration 907: = 0.018581\n",
      "cross entropy loss, iteration 908: = 0.018372\n",
      "cross entropy loss, iteration 909: = 0.018163\n",
      "cross entropy loss, iteration 910: = 0.017955\n",
      "cross entropy loss, iteration 911: = 0.017746\n",
      "cross entropy loss, iteration 912: = 0.017538\n",
      "cross entropy loss, iteration 913: = 0.017329\n",
      "cross entropy loss, iteration 914: = 0.017121\n",
      "cross entropy loss, iteration 915: = 0.016912\n",
      "cross entropy loss, iteration 916: = 0.016704\n",
      "cross entropy loss, iteration 917: = 0.016496\n",
      "cross entropy loss, iteration 918: = 0.016287\n",
      "cross entropy loss, iteration 919: = 0.016079\n",
      "cross entropy loss, iteration 920: = 0.015871\n",
      "cross entropy loss, iteration 921: = 0.015663\n",
      "cross entropy loss, iteration 922: = 0.015455\n",
      "cross entropy loss, iteration 923: = 0.015247\n",
      "cross entropy loss, iteration 924: = 0.015039\n",
      "cross entropy loss, iteration 925: = 0.014831\n",
      "cross entropy loss, iteration 926: = 0.014624\n",
      "cross entropy loss, iteration 927: = 0.014416\n",
      "cross entropy loss, iteration 928: = 0.014208\n",
      "cross entropy loss, iteration 929: = 0.014000\n",
      "cross entropy loss, iteration 930: = 0.013793\n",
      "cross entropy loss, iteration 931: = 0.013585\n",
      "cross entropy loss, iteration 932: = 0.013378\n",
      "cross entropy loss, iteration 933: = 0.013171\n",
      "cross entropy loss, iteration 934: = 0.012963\n",
      "cross entropy loss, iteration 935: = 0.012756\n",
      "cross entropy loss, iteration 936: = 0.012549\n",
      "cross entropy loss, iteration 937: = 0.012341\n",
      "cross entropy loss, iteration 938: = 0.012134\n",
      "cross entropy loss, iteration 939: = 0.011927\n",
      "cross entropy loss, iteration 940: = 0.011720\n",
      "cross entropy loss, iteration 941: = 0.011514\n",
      "cross entropy loss, iteration 942: = 0.011306\n",
      "cross entropy loss, iteration 943: = 0.011099\n",
      "cross entropy loss, iteration 944: = 0.010893\n",
      "cross entropy loss, iteration 945: = 0.010686\n",
      "cross entropy loss, iteration 946: = 0.010479\n",
      "cross entropy loss, iteration 947: = 0.010272\n",
      "cross entropy loss, iteration 948: = 0.010065\n",
      "cross entropy loss, iteration 949: = 0.009859\n",
      "cross entropy loss, iteration 950: = 0.009652\n",
      "cross entropy loss, iteration 951: = 0.009445\n",
      "cross entropy loss, iteration 952: = 0.009239\n",
      "cross entropy loss, iteration 953: = 0.009032\n",
      "cross entropy loss, iteration 954: = 0.008825\n",
      "cross entropy loss, iteration 955: = 0.008619\n",
      "cross entropy loss, iteration 956: = 0.008412\n",
      "cross entropy loss, iteration 957: = 0.008206\n",
      "cross entropy loss, iteration 958: = 0.007999\n",
      "cross entropy loss, iteration 959: = 0.007793\n",
      "cross entropy loss, iteration 960: = 0.007587\n",
      "cross entropy loss, iteration 961: = 0.007380\n",
      "cross entropy loss, iteration 962: = 0.007174\n",
      "cross entropy loss, iteration 963: = 0.006968\n",
      "cross entropy loss, iteration 964: = 0.006761\n",
      "cross entropy loss, iteration 965: = 0.006555\n",
      "cross entropy loss, iteration 966: = 0.006348\n",
      "cross entropy loss, iteration 967: = 0.006142\n",
      "cross entropy loss, iteration 968: = 0.005936\n",
      "cross entropy loss, iteration 969: = 0.005730\n",
      "cross entropy loss, iteration 970: = 0.005524\n",
      "cross entropy loss, iteration 971: = 0.005317\n",
      "cross entropy loss, iteration 972: = 0.005111\n",
      "cross entropy loss, iteration 973: = 0.004905\n",
      "cross entropy loss, iteration 974: = 0.004699\n",
      "cross entropy loss, iteration 975: = 0.004493\n",
      "cross entropy loss, iteration 976: = 0.004287\n",
      "cross entropy loss, iteration 977: = 0.004080\n",
      "cross entropy loss, iteration 978: = 0.003874\n",
      "cross entropy loss, iteration 979: = 0.003669\n",
      "cross entropy loss, iteration 980: = 0.003462\n",
      "cross entropy loss, iteration 981: = 0.003257\n",
      "cross entropy loss, iteration 982: = 0.003050\n",
      "cross entropy loss, iteration 983: = 0.002844\n",
      "cross entropy loss, iteration 984: = 0.002638\n",
      "cross entropy loss, iteration 985: = 0.002432\n",
      "cross entropy loss, iteration 986: = 0.002226\n",
      "cross entropy loss, iteration 987: = 0.002021\n",
      "cross entropy loss, iteration 988: = 0.001815\n",
      "cross entropy loss, iteration 989: = 0.001609\n",
      "cross entropy loss, iteration 990: = 0.001403\n",
      "cross entropy loss, iteration 991: = 0.001198\n",
      "cross entropy loss, iteration 992: = 0.000992\n",
      "cross entropy loss, iteration 993: = 0.000786\n",
      "cross entropy loss, iteration 994: = 0.000580\n",
      "cross entropy loss, iteration 995: = 0.000374\n",
      "cross entropy loss, iteration 996: = 0.000169\n",
      "cross entropy loss, iteration 997: = -0.000037\n",
      "cross entropy loss, iteration 998: = -0.000243\n",
      "cross entropy loss, iteration 999: = -0.000448\n"
     ]
    }
   ],
   "source": [
    "# iterate learning rule a number of times\n",
    "\n",
    "num_steps = 1000\n",
    "\n",
    "train_error_hidden = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    sess_hidden.run(train_op_hidden,feed_dict={Y_hidden:output_train_onehot,R_hidden:response_train})\n",
    "    \n",
    "    print \"cross entropy loss, iteration %d: = %f\" % (step,sess_hidden.run(cross_entropy_loss_hidden,feed_dict={Y_hidden:output_train_onehot,R_hidden:response_train}))\n",
    "    \n",
    "    incorrect = tf.not_equal(tf.argmax(Y_prob_hidden,1),tf.argmax(Y_hidden,1))\n",
    "    percent_incorrect = tf.reduce_mean(tf.cast(incorrect,tf.float32))\n",
    "    train_error_hidden.append(sess_hidden.run(percent_incorrect,feed_dict={Y_hidden:output_train_onehot,R_hidden:response_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train error = 19.62%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x14e31b8d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.plot(train_error_hidden)\n",
    "ax.set_xlabel(\"iteration\")\n",
    "ax.set_ylabel(\"test error\")\n",
    "\n",
    "print \"Final train error = {0:0.2f}%\".format(train_error_hidden[-1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
